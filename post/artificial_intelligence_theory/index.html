<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.5.0">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Artur de Luca">

  
  
  
    
  
  <meta name="description" content="How close can we get to a goal that can’t be precisely described? This idea may seem odd at first, but many of the concepts we daily think of cannot be defined accurately. Being like so, how can one ensure to have actually achieved such a goal?
Consider an individual looking to get healthier. At first thought, his conception of wellness is likely an intangible notion rather than on a thorough combination of bodily measurements, such as blood pressure or respiratory capacity.">

  
  <link rel="alternate" hreflang="en-us" href="https://artur-deluca.github.io/post/artificial_intelligence_theory/">

  


  
  
  
  <meta name="theme-color" content="#2962ff">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.0/css/all.css" integrity="sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.css" integrity="sha512-M2wvCLH6DSRazYeZRIm1JnYyh22purTM+FDB5CsyxtQJYeKq83arPe5wgbNmcFXGqiSH2XR8dT/fJISVA1r/zQ==" crossorigin="anonymous">
    

    

  

  
  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap">
  

  
  
  
  <link rel="stylesheet" href="/css/academic.min.3f344b41a854a4e86bdc347bbce3b34a.css">

  
    
    
    
    
      
    
    
    
    <link rel="stylesheet" href="/css/academic.8f40df2333e4f9e980fb3e30d1449d43.css">
  

  
  
  

  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon-32.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="https://artur-deluca.github.io/post/artificial_intelligence_theory/">

  
  
  
  
    
    
  
  
  <meta property="twitter:card" content="summary">
  
  <meta property="twitter:site" content="@backdeluca">
  <meta property="twitter:creator" content="@backdeluca">
  
  <meta property="og:site_name" content="arturdeluca">
  <meta property="og:url" content="https://artur-deluca.github.io/post/artificial_intelligence_theory/">
  <meta property="og:title" content="Exploring vagueness: the underlying theses in artificial intelligence | arturdeluca">
  <meta property="og:description" content="How close can we get to a goal that can’t be precisely described? This idea may seem odd at first, but many of the concepts we daily think of cannot be defined accurately. Being like so, how can one ensure to have actually achieved such a goal?
Consider an individual looking to get healthier. At first thought, his conception of wellness is likely an intangible notion rather than on a thorough combination of bodily measurements, such as blood pressure or respiratory capacity."><meta property="og:image" content="https://artur-deluca.github.io/img/icon-192.png">
  <meta property="twitter:image" content="https://artur-deluca.github.io/img/icon-192.png"><meta property="og:locale" content="en-us">
  
    
      <meta property="article:published_time" content="2020-04-12T00:00:00-05:00">
    
    <meta property="article:modified_time" content="2020-04-12T00:00:00-05:00">
  

  


    






  





  





  





<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://artur-deluca.github.io/post/artificial_intelligence_theory/"
  },
  "headline": "Exploring vagueness: the underlying theses in artificial intelligence",
  
  "datePublished": "2020-04-12T00:00:00-05:00",
  "dateModified": "2020-04-12T00:00:00-05:00",
  
  "author": {
    "@type": "Person",
    "name": "Artur de Luca"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "arturdeluca",
    "logo": {
      "@type": "ImageObject",
      "url": "https://artur-deluca.github.io/img/icon-512.png"
    }
  },
  "description": "How close can we get to a goal that can’t be precisely described? This idea may seem odd at first, but many of the concepts we daily think of cannot be defined accurately. Being like so, how can one ensure to have actually achieved such a goal?\nConsider an individual looking to get healthier. At first thought, his conception of wellness is likely an intangible notion rather than on a thorough combination of bodily measurements, such as blood pressure or respiratory capacity."
}
</script>

  

  


  


  <link rel="shortcut icon" type="image/icon" href="./img/favicon.ico"/>


  <title>Exploring vagueness: the underlying theses in artificial intelligence | arturdeluca</title>

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  
<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0 compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="/">arturdeluca</a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
        <span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">

      
      
      <ul class="navbar-nav ml-auto">
        

        

        
        
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link  active" href="/"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/files/cv.pdf"><span>CV</span></a>
        </li>

        
        

      

        

        
        <li class="nav-item">
          <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        

        

      </ul>

    </div>
  </div>
</nav>


  <article class="article">

  












  

  
  
  
<div class="article-container pt-3">
  <h1>Exploring vagueness: the underlying theses in artificial intelligence</h1>

  

  
    



<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
    
    Apr 12, 2020
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    18 min read
  </span>
  

  
  
  

  
  

  
    

  

</div>

    














  
</div>



  <div class="article-container">

    <div class="article-style">
      <p>How close can we get to a goal that can’t be precisely described? This idea may seem odd at first, but many of the concepts we daily think of cannot be defined accurately. Being like so, how can one ensure to have actually achieved such a goal?</p>
<p>Consider an individual looking to get healthier. At first thought, his conception of wellness is likely an intangible notion rather than on a thorough combination of bodily measurements, such as blood pressure or respiratory capacity. Even if so, he may come to realize that health cannot be completely outlined by these, as they overlook other aspects of health, such as mental sanity.</p>
<p>As ample a concept seems, the harder it is come up with a comprehensive definition for it, let alone ways to measure it. However, this all may come across as far too finicky, since we rarely ponder over such problems in our daily lives. In practice, one way we avoid these dilemmas is to project our goal onto a concrete achievement or feature that could be indicative of this same objective. In our example, perhaps getting healthier may come as a consequence of eating vegetables every day, getting 8 hours of sleep or even running that half-marathon at the end of the year.</p>
<p>In the scientific community, however, precise definitions matter. Furthermore, if we venture ourselves in this task collectively, how can we ensure that our notions and expectations converge? In these vague circumstances, reaching common ground is not only pragmatic but crucial. With a layer of optimism, the hope is to perpetually refine this agreed objective under the scientific method, even though we may not be yet qualified to properly define this.</p>
<p>There’s seems to be a taxonomical gray area within the field of artificial intelligence. This and some associated terms have lost their original meaning due to the rapid wear and tear in light of the breakthroughs in deep learning over the past decade, and the inflated expectations that have followed. Professor Michael Jordan even contests that these recent advancements are not considered part of artificial intelligence anymore. Jordan claims that current machine learning and deep learning are more focused on an engineering aspect as of reaching the original goal of AI.</p>
<p>Artificial intelligence as we know has evolved from isolated research streams, including the early blueprints where current neural network models originated from (<a href="#References">Buchanan (2005)</a>). The term Artificial Intelligence (AI) was famously coined by John McCarthy for the Dartmouth convention, a two-month workshop dedicated to discussing crucial questions on this then-emerging-yet-to-be-defined field (<a href="#References">McCarthy et. al (1955)</a>). The name was chosen by McCarthy as a way to accommodate and conciliate these research angles around the theme at the time.</p>
<p>According to McCarthy, Artificial Intelligence is the science and engineering of making intelligent machines, especially intelligent computer programs. By investigating and precisely describing the features of intelligence, McCarthy believed that these could be then replicated by machines. This perspective, however, leads us to a tricky debate: what constitutes intelligence?</p>
<h2 id="intelligence">Intelligence</h2>
<p>Intelligence is such a far-reaching concept that its definition remains unsettled. In 1921, in an attempt to technically outline it, the Journal of Educational Psychology asked fourteen experts on the field to present their definitions of intelligence. As expected, there were fourteen distinct definitions. As one theorist on intelligence, R. Sternberg, points out: “there seem to be almost as many definitions of intelligence as there were experts asked to define it” (<a href="#References">Legg and Hunter (2006)</a>).</p>
<blockquote>
<p><em>Having learned or ability to learn to adjust oneself to the environment</em> <br></p>
<p align="right">(S. S. Colvin)</p>
<p><em>The capacity to learn or to profit by experience</em> <br></p>
<p align="right">(W. F. Dearborn)</p>
</blockquote>
<p style="font-size:0.8em;" align="center">Some definitions extracted from the Journal of Educational Psychology (<a href="#References">Pfeifer and Scheier (1999)</a>)</p>
<p>Intelligence is a descriptive term, hence it assigns a certain set of characteristics to a group or individual. So being, it is fairly unlikely to accommodate multiple angles into a single definition. To some, intelligence is in the collective behavior demonstrated by schools of fishes or colonies of ants; to others in particular aspects of human intelligence, such as the ability to speak, or even on how some intelligent features could be artificially recreated.</p>
<p>Nevertheless, this ambivalence didn&rsquo;t restrain researchers from claiming progress in AI. For decades, the game of chess was considered the pinnacle of the human intellect. Building a machine that could defeat world-class contenders was in the scope of engineers and scientists as to prove that intelligent machines could be built. In 1997, IBM organized a historic rematch between the world chess champion Garry Kasparov and the supercomputer named Deep Blue over a six-game match. In the previous year, Kasparov defeated Deep Blue, winning four out of six games. However, in the following encounter, the system had an unprecedented victory, landing two wins and three draws. This was a symbolic event, showcasing several advances that enabled a system to beat the greatest chess player of all time.</p>
<img src="./figures/deep_blue_kasparov.png" width="80%"/>
<p style="font-size:0.8em;" align="center">Kasparov against Deep Blue, 1997</p>
<p>However, a large part of the academic community remained skeptical, portraying the victory as the result of a dumb system with amazing processing speed in contrast with Kasparov’s greater intellect. This was partially true. The system developed by IBM had several chess strategies stored, built upon consulting chess masters multiple times. This allowed the system to cut corners when searching for the right move. Yet, this search was only made possible by the Deep Blue’s remarkable capacity to explore up to 200 million positions per second (<a href="#References">Press (2018)</a>).</p>
<p>As many critics claim that such a system was unintelligent, they wrongfully make of this concept a dichotomy: either you have intelligence or not. But intelligence is more of a spectrum than a predicate. This becomes more noticeable when we introduce a reference: are rats intelligent? Of course not as much as humans, but definitely more than ants. Despite using it qualitatively, when we say a person is intelligent, we normally mean that this individual has an above-average level of intelligence.</p>
<p>As we can observe, there’s little hope of an agreement over an all-inclusive terminology. Nevertheless, almost all definitions of intelligence seem to share a common factor: to comply with the ruling conditions and respond to changes. These two processes can be perceived as the ability to adapt, either to a ruling or emerging pattern, while preserving a certain structure, what <a href="#References">Ashby (1960)</a> refers to as maintaining homeostasis.</p>
<p>Many of the modern techniques we use today within artificial intelligence, and in particular machine learning, make use of algorithms that modify parameters and adapt to the desired task. But even decades before the Dartmouth convention, some fields already attempted to understand and emulate intelligence using adaptable systems. One field, in particular, named connectionism consisted of representing mental phenomena throughout emerging patterns in the brain. This is the cornerstone in which modern neural networks are based on.</p>
<h2 id="theories-of-mind-and-artificial-intelligence">Theories of mind and artificial intelligence</h2>
<p>The research streams around artificial intelligence generally involved much more than attempting to engineer intelligent systems. These encompassed not only computer science but cognitive psychology, neuroscience, and philosophy.</p>
<p>For centuries, thinkers were interested in understanding the relationship between mind and body. Humans are remarkably aware of the manifestations in our minds. With constructs such as beliefs, desires, actions, emotions and so forth, we are capable of not describing our mental states, but to attribute these to other individuals. Do these mental states have physical manifestations as well? This question is famously known as the “Mind-body problem” and the theories around this topic are gathered in the field of philosophy of mind.</p>
<p>Despite modern evidence suggesting the connection between brain and mind, this question remained fairly open up until the last centuries. With modern scientific advancements, this problem began to involve much more than philosophy. From these interactions, the field cognitive science emerged, focused on investigating the mind and its processes.</p>
<p>Cognitive science had a crucial role in the development of AI, from which two major branches unfolded: connectionism and symbolicism. Despite not being mutually exclusive, these two approaches practically formed a feud in artificial intelligence. Nevertheless, they seem to be rooted in the same theory of functionalism.</p>
<p>In philosophy of mind, one of the predominant theories of the last century titled identity of thought interpreted each mental state as a manifestation of a specific event in the brain. Hence, these biological activities were seen as the connection between the mind and the physical domain. This view implied that either a mental state is unique, or, for this to be experienced by others, these must have an identical organic structure.</p>
<p>As this view seems unrealistic, the functionalist theory offers an alternative: instead of tying each mental state to a biological counterpart, it proposed identifying these by their corresponding functions (<a href="#References">Piccini (2004)</a>). If a mental state such as fear could be defined as producing concern for harm and the desire to either fight or flight an enemy. It does not matter if this is achieved through biological elements, as long as it produces the desired effect. (<a href="#References">Carneades.org (2016)</a>)</p>
<p>What implications of functionalism brought to AI? This perspective entails the following: intelligent behavior does not need to be carried out in the exact structure as we have in our brains. As long as the same results are attained, we can achieve the level of intelligence desired using a surrogate model. Hence, in theory, intelligence can be achieved at the algorithmic level using machines.</p>
<h3 id="symbolicism">Symbolicism</h3>
<p>Although functionalism implies that intelligence may be attained apart from biological structures, a clear proxy to this task is understanding how we humans think. Thought can be seen understood in many forms, fluctuating in a biological to philosophical scale. In the most abstract state, we see thought as the interaction concepts. And according to the language of thought hypothesis, these relationships follow a syntax, just like language.</p>
<p>In symbolicism, this syntax is described according to the laws of thought, a set of axioms that dates back to Aristotle, but is modernly incorporated into Boolean logic. In this paradigm, information is represented by a collection of human-readable symbols that possess properties and relationships. Supported by the computational models around the 1950s, symbolicism, also known as classical AI or GOFAI (Good Old-Fashioned Artificial Intelligence), ruled artificial intelligence for more than 40 years. According to the physical symbol system hypothesis formulated by <a href="#References">Newell and Simon (1976)</a>, this is a sufficient condition for achieving a “general intelligent system”<sup>1</sup>.</p>
<pre><code>(apple
  (isa fruit),
  (color red); (color green),
  (origin tree)
);
</code></pre><p style="font-size:0.8em;" align="center">Symbolic representation of an apple</a></p>
<p>The concept of symbols can be traced to Plato&rsquo;s theory of forms, where the real objects are mere representations of abstract essences, in the same way that any apple in the real world is the physical manifestation of its idealistic form. Apples have attributes and relationships, some distinct and others unchangeable: apples can be red or green, but all of them are fruits. These characteristics are best expressed by schemas, an organization of knowledge proposed by Kant, which allows us to classify different symbols and make inferences based on their features. For instance, we can logically differentiate beets from apples based on their different schemas: despite possibly having the same color, apples are fruits that grow on trees as beets are vegetables that grow in soil.</p>
<p style="font-size:0.6em;">
<b>1</b> For this and other contributions, the authors have received a Turing award in 1975. However, this hypothesis is widely contested, for several reasons, not to mention the vagueness in the definition of intelligence. The authors even later recognized that it may not hold. For a more thorough investigation, see <a href="#References">Nilsson (2007)</a><br>
</p>
<p>Reasoning is an important feature of intelligence and thus has always been of great interest in the AI community. Symbolicism has shared great success in these tasks. For instance, one well-celebrated type of implementation called Expert System assembles knowledge of a particular subject, making specialized logical inferences. Upon analyzing an incoming patient if fever, a medical expert system could determine that the patient has some sort of infection, and, based on additional symptoms and logical rules, assert what type of disease the patient might be afflicted.</p>
<p>As in medicine and other risky applications, symbolic systems offered a great advantage. Not only they could automate some of the jobs, but since readable symbols were used throughout the whole solving procedure, these could be easily inspected by human experts for a second opinion. But what would happen if our input was not symbolic? This was where things started to break down.</p>
<p>Interacting with data extracted from the real world and not some enclosed model was a great challenge for symbolic systems. Let us consider the task of identifying a position in space using cameras. This task was part of a project named Shakey, a collective endeavor conducted by 13 researchers at Stanford, aimed to combine several modules in artificial intelligence into a single robot. The input consisted of a large collection of pixels for a particular instant in time. Based on certain mapped patterns across an enclosed space, Shakey ought to recognize its location within the environment.</p>
<img src="./figures/shakey.svg" width="100%"/>
<p style="font-size:0.8em;" align="center">Example of the process flow from input to the high-level representation. (<a href="#References">Stanford (1972))</a></p>
<p>Once the camera captures a region of interest, it conducts a series of preprocessing states to extract the relevant features (in this case the lines of the image). Then, this intermediate representation is compared to each pattern in the system’s internal collection that maps a particular landmark, like a corner of a room as the figure demonstrates (<a href="#References">Rosen and Nilsson (1967)</a>).</p>
<p>Although having a good performance in controlled environments, such as in Shakey, this technique is unsuited for practical applications. Real-world settings are not only prone to noisy recordings but, many of these require immediate feedback. In contrast, the techniques used to transform the high dimensional input into symbolic representations were not only computationally demanding but difficult to parallelize, as showcased by Shakey&rsquo;s “excruciating slow operation” (<a href="#References">Copeland (2020)</a>). What’s more, these inference systems were not robust enough against noisy data. To make matters worse, the deal-breaker for symbolic AI was the inability to learn based on observations. During the late past century, learning became a central piece of intelligence, as a self-correcting mechanism. And without any human intervention, these systems could not improve their actions.</p>
<p>Symbolicism is still widely used today. So much so, that in search problems, where multiple configurations must be evaluated (e.g. in chess), the symbolic paradigm offers a clear advantage over sub-symbolic representations. On the other hand, in more recent applications, such as natural language processing and computer vision, distributed representations make connectionism much more suited for parallel computation.</p>
<p>Perhaps the motive of dispute between connectionists and symbolists is rooted in placing points of view as opposite sides of a spectrum. In symbolic AI, we have rich, symbolic concepts that are naturally coherent to humans. Alternatively, connectionism makes use of small fragments of information, that when combined compose an intelligible concept. Symbolicism typically works with binary data and hard, logical constraints as connectionism deals with continuous numbers and soft, probabilistic constraints.</p>
<p>However, as we hover over details, these differences become blurrier. Not only there are systems on both sides that don&rsquo;t comply with all these definitions. But at the implementation level, symbols and schemas can also be seen as distributed representations, constituted of streams of binary values. At the same time, the probabilistic operations within connectionism are a mere product of pseudo-random number generators, a product of logical, deterministic devices. Even the first conception of a connectionism model on a computer, the McCulloch-Pitts neuron, was initially used to solve propositional logic.</p>
<p>Despite this imprecise differences, the connectionism paradigm received a lot of pushback during the second half of the twentieth century. But as a consequence of all symbolic shortcomings, the paradigm of sub-symbolic artificial intelligence swooped into popularity, bolstered the wave of recent successes in connectionism.</p>
<h3 id="connectionism">Connectionism</h3>
<p>Connectionist models are built on interconnected units, called neurons, which can be assembled in various arrangements, composing the so-called neural networks. Depending on the incoming stimuli, these neurons dispatch values or not, thus creating a representation based on the overall firing pattern.</p>
<img src="./figures/connectionism.svg" width="80%"/>
<p style="font-size:0.8em;" align="center">Toy illustration of a distributed representation of apples, beets and their corresponding features, showcased by the connections between each node</p>
<p>For instance, the representation of apples could indicate the activation of the neurons <code>tree</code> (grows on trees), <code>green</code> or <code>red</code> (color) and <code>fruit</code>. This pattern of discharges is analytically achieved by a set of parameters uniting the nodes, as well as non-linear operations called activation functions, responsible for firing each unit.</p>
<p>Unlike symbolic AI, this paradigm approaches intelligence from a biological aspect. Prototypical models are often linked to organic counterparts, which, at least to a certain extent, served as inspiration for contemporary architectures. Connectionist modeling was a research stream in computer science only claimed during the 1960s, by Frank Rosenblatt, in association with the cognitive science theory that carries the same name (<a href="#References">Copeland (2020)</a>).</p>
<p>The early influences of connectionism and date back to the 1850s, with Herbert Spencer’s Principles of Psychology. In this, Spencer alludes to how intelligence can be seen as the successive association of psychological states, which are driven by the connections between such states. In the following decades, William James, the father of American psychology, outlined the mechanisms that indicated how these mechanisms could associate, even for a distributed representation of mental states, an idea which would be later formalized by Donald Hebb in the 1950s.</p>
<p>Despite today’s historical remarks, the works of Spencer and James were seen as mere speculation at the time. Considered the father of the field, Edward Thorndike, a student of James, was the first to proclaim himself as a “connectionist”. Thorndike, expanded the hypothesis of James to the biological sphere, suggesting that learning was somehow encoded in the connections between neurons. He also suggested these associations could take place upon external reinforcements. This idea was based on the preceding school of behaviorism, more specifically by the work of Pavlov. In this, he demonstrates the processes of excitation and inhibition, outlining the formation of conditioned reflexes, an adaptation of the brain upon a certain pattern, in the eyes of Thorndike (<a href="#References">Walker (1992)</a>).</p>
<img src="./figures/pavlov.svg" width="100%"/>
<p style="font-size:0.8em;" align="center">Illustration of the classical conditioning experiment conducted by Pavlov in 1897.</p>
<p>Researchers in computer science later took inspiration from these ideas to develop computational neural network models and their adaptation mechanisms. However, connectionism in computer science has then strayed from its homonym. This paradigm can now be more seen as a functionalist perspective as it neglects the biological correspondence in search of a generalized view of intelligence.</p>
<p>In terms of accomplishment, connectionism in computer science has witnessed many successes in the last decade. Reviving the contested feat of Deep Blue, researchers at DeepMind have developed a system named Alpha Zero, that not only beat its predecessor, but it did so by its own experience, only playing with itself. Since the victory over Kasparov, researchers raised the stakes in search of more challenging feats. In 2015, the DeepMind’s AlphaGo defeated Lee Sedol, considered one of the greatest players at the game of Go, an immensely more complex challenge than chess (<a href="#References">DeepMind (2020))</a>.</p>
<hr>
<p>Despite some push-back against the distancing between the ongoing advancement of neural network and cognitive science, these fields have been more popular than ever. Not only, neural networks are being used to strengthen cognitive hypothesis<sup>2</sup>, but these have sprawled and engaged other well-established disciplines on solving standing challenges.</p>
<p>Due to the viewpoint of intelligence centered around learning, the most sought feature in applications is the ability to generalize, i.e. to learn beyond the given data. In this perspective, the circumstances and reasons why deep learning models can do so remain unsettled, despite great advancements brought forward by the current statistical learning theory stream <sup>3</sup>.</p>
<p style="font-size:0.6em;">
<b>2</b> See <a href="#References">Banino et. al (2018)</a><br>
<b>3</b> See <a href="#References">Poggio et. al (2020)</a><br>
</p>
<p>Our current capabilities are limited, but the question that lingers is how far are we from understanding human intelligence, or even from reaching it computationally? Most likely very far, despite the widespread optimism surrounding deep learning, there are several visible limitations to the current settings. However, a good indication headway is to not only achieve tangible milestones but also to continuously uncover misconceptions. As biological and philosophical novelties, the ongoing engineering experimentation can also help to shed light on these. And as validate our conceptions from multiple angles, the closer we move towards a comprehensive understanding of intelligence.</p>
<h1><a name="References"></a>References</h1>
<ul style="font-size:0.6em;">
    <li>Buchanan, B. G. (2005). A (Very) Brief History of Artificial Intelligence. AI Magazine, 26(4), 53. <a href="https://doi.org/10.1609/aimag.v26i4.1848">doi:10.1609/aimag.v26i4.1848</a></li>
    <li>McCarthy, J., Minsky, M. L., Rochester, N. and Shannon, C.E. (1955). A proposal for the Dartmouth summer research project on Artificial Intelligence. Retrieved April 13, 2020, from <a href="https://web.archive.org/web/20200403024941/http://www-formal.stanford.edu/jmc/history/dartmouth/dartmouth.html">http://www-formal.stanford.edu/jmc/history/dartmouth/dartmouth.html</a></li>
    <li>Legg, S. and Hunter, M. (2006). A Collection of Definitions of Intelligence</li>
    <li>Pfeifer, R., & Scheier, C. (1999). Understanding intelligence. MIT Press.</li>
    <li>Press, G. (2018). The Brute Force Of IBM Deep Blue And Google DeepMind. Forbes Media. Retrieved April 13, 2020, from <a href="https://www.forbes.com/sites/gilpress/2018/02/07/the-brute-force-of-deep-blue-and-deep-learning/">https://www.forbes.com/sites/gilpress/2018/02/07/the-brute-force-of-deep-blue-and-deep-learning/</a></li>
    <li>Ashby, W. R. (1960). Design for a brain: The origin of adaptive behavior. (2d ed). Wiley;</li>
    <li>Sarıhan, I. (2017). Chapter 2 - Philosophical Puzzles Evade Empirical Evidence: Some Thoughts and Clarifications Regarding the Relation Between Brain Sciences and Philosophy of Mind. In J. Leefmann & E. Hildt (Eds.), The Human Sciences after the Decade of the Brain (pp. 14–23). Academic Press. <a href="https://doi.org/10.1016/B978-0-12-804205-2.00002-1">doi:10.1016/B978-0-12-804205-2.00002-1</a></li>
    <li>Piccinini, G. (2004). The First Computational Theory of Mind and Brain: A Close Look at Mcculloch and Pitts’s “Logical Calculus of Ideas Immanent in Nervous Activity.” Synthese, 141(2), 175–215. <a href="https://doi.org/10.1023/B:SYNT.0000043018.52445.3e">doi:10.1023/B:SYNT.0000043018.52445.3e</a></li>
    <li>Carneades.org. (2016). What is Functionalism? (Philosophy of Mind). Retrieved April 13, 2020, from <a href="https://www.youtube.com/watch?v=a5AwaFsp5Os">https://www.youtube.com/watch?v=a5AwaFsp5Os</a></li>
    <li>Newell, Allen; Simon, H. A. (1976), "Computer Science as Empirical Inquiry: Symbols and Search", Communications of the ACM, 19 (3): 113–126, <a href="https://dl.acm.org/doi/10.1145/360018.360022">doi:10.1145/360018.360022</a></li>
    <li>Nilsson, N. J. (2007). The Physical Symbol System Hypothesis: Status and Prospects. In M. Lungarella, F. Iida, J. Bongard, & R. Pfeifer (Eds.), 50 Years of Artificial Intelligence: Essays Dedicated to the 50th Anniversary of Artificial Intelligence (pp. 9–17). Springer. <a href="https://doi.org/10.1007/978-3-540-77296-5_2">doi:10.1007/978-3-540-77296-5_2</a></li>
    <li>Stanford University Libraries (2016). Shakey: Experiments in Robot Planning and Learning (1972). Retrieved April 13, 2020, from <a href="https://www.youtube.com/watch?v=GmU7SimFkpU">https://www.youtube.com/watch?v=GmU7SimFkpU</a></li>
    <li>Rosen, C. A. and Nilsson, N. J. (1967). Application of Intelligent Automata to Reconnaissance. Third Interim Report. Stanford Research Institute.
    <li>Copeland, B.J. (2020). Artificial Intelligence | Definition, Examples, and Applications. Encyclopedia Britannica. Retrieved April 13, 2020, from <a href="https://www.britannica.com/technology/artificial-intelligence">https://www.britannica.com/technology/artificial-intelligence</a></li>
    <li>Walker, S. F. (1992) A brief history of connectionism and its psychological implications. In Clark, A. and Lutz, R. (eds) Connectionism in Context. Berlin: Springer-Verlag. 123-144</li>
    <li>DeepMind (2020). AlphaGo: The story so far. Retrieved April 13, 2020, from <a href="https://deepmind.com/research/case-studies/alphago-the-story-so-far">https://deepmind.com/research/case-studies/alphago-the-story-so-far</a></li>
    <li>Banino, A., Barry, C., Uria, B., Blundell, C., Lillicrap, T., Mirowski, P., Pritzel, A., Chadwick, M. J., Degris, T., Modayil, J., Wayne, G., Soyer, H., Viola, F., Zhang, B., Goroshin, R., Rabinowitz, N., Pascanu, R., Beattie, C., Petersen, S., Kumaran, D. (2018). Vector-based navigation using grid-like representations in artificial agents. Nature, 557(7705), 429–433. <a href="https://doi.org/10.1038/s41586-018-0102-6">doi:10.1038/s41586-018-0102-6</a></li>
    <li>Poggio, T., Liao, Q., & Banburski, A. (2020). Complexity control by gradient descent in deep networks. Nature Communications, 11(1), 1–5. <a href="https://doi.org/10.1038/s41467-020-14663-9">doi:10.1038/s41467-020-14663-9</a></li>
</ul>
<style>
    h1 {
        text-align: left;
    }

    body {
        text-align: justify;
    }

</style>

    </div>

    


    

<div class="article-tags">
  
  <a class="badge badge-light" href=""></a>
  
</div>



    
      








  






  
  
  
    
  
  
  <div class="media author-card">
    
      <img class="portrait mr-3" src="https://s.gravatar.com/avatar/f6c8c4a6a41a1172a087f1635c006d61?s=200')" alt="Avatar">
    

    <div class="media-body">
      <h5 class="card-title"><a href="https://artur-deluca.github.io/">Artur de Luca</a></h5>
      <h6 class="card-subtitle">Graduate Student in Artificial Intelligence and Robotics<br> at La Sapienza</h6>
      
      <ul class="network-icon" aria-hidden="true">
        
          
          
          
            
          
          
          
          
          
            
          
          <li>
            <a href="https://twitter.com/backdeluca" target="_blank" rel="noopener">
              <i class="fab fa-twitter"></i>
            </a>
          </li>
        
          
          
          
            
          
          
          
          
          
            
          
          <li>
            <a href="https://github.com/artur-deluca" target="_blank" rel="noopener">
              <i class="fab fa-github"></i>
            </a>
          </li>
        
          
          
          
            
          
          
          
          
          
            
          
          <li>
            <a href="https://www.linkedin.com/in/arturbackdeluca" target="_blank" rel="noopener">
              <i class="fab fa-linkedin"></i>
            </a>
          </li>
        
          
          
          
          
          
          
          
            
          
          <li>
            <a href="/files/cv.pdf" >
              <i class="ai ai-cv"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>



      
      
      <div class="article-widget">
        <div class="hr-light"></div>
        <h3>Related</h3>
        <ul>
          
          <li><a href="/post/introduction/">Why deep learning became relevant</a></li>
          
          <li><a href="/post/pretraining/">Deep learning advances: unsupervised pretraining</a></li>
          
        </ul>
      </div>
      
    

    

    

    <div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "artur-deluca-github-io" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

  </div>
</article>

      

    
    
    
    <script src="/js/mathjax-config.js"></script>
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/8.0.0/mermaid.min.js" integrity="sha256-0w92bcB21IY5+rGI84MGj52jNfHNbXVeQLrZ0CGdjNY=" crossorigin="anonymous" title="mermaid"></script>
      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/highlight.min.js" integrity="sha256-aYTdUrn6Ow1DDgh5JTc3aDGnnju48y/1c8s1dgkYPQ8=" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/languages/python.min.js"></script>
        
      

      
      
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML-full" integrity="sha256-GhM+5JHb6QUzOQPXSJLEWP7R73CbkisjzK5Eyij4U9w=" crossorigin="anonymous" async></script>
      
    

    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.js" integrity="sha512-lInM/apFSqyy1o6s89K4iQUKg6ppXEgsVxT35HbzUupEVRh2Eu9Wdl4tHj7dZO0s1uvplcYGmt3498TtHq+log==" crossorigin="anonymous"></script>
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script>
      const search_index_filename = "/index.json";
      const i18n = {
        'placeholder': "Search...",
        'results': "results found",
        'no_results': "No results found"
      };
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.130521ecfc6f534c52c158217bbff718.js"></script>

    






  
  <div class="container">
    <footer class="site-footer">
  
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
