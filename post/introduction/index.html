<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.5.0">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Artur de Luca">

  
  
  
    
  
  <meta name="description" content="In the last decade, there&#39;s been a widespread interest in artificial intelligence, particularly machine learning, and even more specifically deep learning. Curiously, this topic has not only resurged within academia but rather it has also been commonly showcased on non-technical mediums.
Why these topics became so popular now? Certainly, recent feats such as AlphaGo or Waymo may raise awareness and hint to potential implications in our social organization, especially in the labor sector.">

  
  <link rel="alternate" hreflang="en-us" href="https://artur-deluca.github.io/post/introduction/">

  


  
  
  
  <meta name="theme-color" content="#2962ff">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.0/css/all.css" integrity="sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.css" integrity="sha512-M2wvCLH6DSRazYeZRIm1JnYyh22purTM+FDB5CsyxtQJYeKq83arPe5wgbNmcFXGqiSH2XR8dT/fJISVA1r/zQ==" crossorigin="anonymous">
    

    

  

  
  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap">
  

  
  
  
  <link rel="stylesheet" href="/css/academic.min.2d819551d436d2320319fca8a8497a72.css">

  
    
    
    
    
      
    
    
    
    <link rel="stylesheet" href="/css/academic.bd654c477505d2f4707dc3fd3a8e905d.css">
  

  
  
  

  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon-32.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="https://artur-deluca.github.io/post/introduction/">

  
  
  
  
    
    
  
  
  <meta property="twitter:card" content="summary">
  
  <meta property="twitter:site" content="@backdeluca">
  <meta property="twitter:creator" content="@backdeluca">
  
  <meta property="og:site_name" content="arturdeluca">
  <meta property="og:url" content="https://artur-deluca.github.io/post/introduction/">
  <meta property="og:title" content="Why deep learning became relevant | arturdeluca">
  <meta property="og:description" content="In the last decade, there&#39;s been a widespread interest in artificial intelligence, particularly machine learning, and even more specifically deep learning. Curiously, this topic has not only resurged within academia but rather it has also been commonly showcased on non-technical mediums.
Why these topics became so popular now? Certainly, recent feats such as AlphaGo or Waymo may raise awareness and hint to potential implications in our social organization, especially in the labor sector."><meta property="og:image" content="https://artur-deluca.github.io/img/icon-192.png">
  <meta property="twitter:image" content="https://artur-deluca.github.io/img/icon-192.png"><meta property="og:locale" content="en-us">
  
    
      <meta property="article:published_time" content="2019-05-09T00:00:00-05:00">
    
    <meta property="article:modified_time" content="2019-05-09T00:00:00-05:00">
  

  


    






  





  





  





<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://artur-deluca.github.io/post/introduction/"
  },
  "headline": "Why deep learning became relevant",
  
  "datePublished": "2019-05-09T00:00:00-05:00",
  "dateModified": "2019-05-09T00:00:00-05:00",
  
  "author": {
    "@type": "Person",
    "name": "Artur de Luca"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "arturdeluca",
    "logo": {
      "@type": "ImageObject",
      "url": "https://artur-deluca.github.io/img/icon-512.png"
    }
  },
  "description": "In the last decade, there's been a widespread interest in artificial intelligence, particularly machine learning, and even more specifically deep learning. Curiously, this topic has not only resurged within academia but rather it has also been commonly showcased on non-technical mediums.\nWhy these topics became so popular now? Certainly, recent feats such as AlphaGo or Waymo may raise awareness and hint to potential implications in our social organization, especially in the labor sector."
}
</script>

  

  


  


  <link rel="shortcut icon" type="image/icon" href="./img/favicon.ico"/>


  <title>Why deep learning became relevant | arturdeluca</title>

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  
<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0 compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="/">arturdeluca</a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
        <span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">

      
      
      <ul class="navbar-nav ml-auto">
        

        

        
        
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link  active" href="/"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/files/cv.pdf"><span>CV</span></a>
        </li>

        
        

      

        

        
        <li class="nav-item">
          <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        

        

      </ul>

    </div>
  </div>
</nav>


  <article class="article">

  












  

  
  
  
<div class="article-container pt-3">
  <h1>Why deep learning became relevant</h1>

  

  
    



<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
    
    May 9, 2019
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    11 min read
  </span>
  

  
  
  

  
  

  
    

  

</div>

    














  
</div>



  <div class="article-container">

    <div class="article-style">
      <p>In the last decade, there's been a widespread interest in artificial intelligence, particularly machine learning, and even more specifically deep learning. Curiously, this topic has not only resurged within academia but rather it has also been commonly showcased on non-technical mediums.</p>
<p>Why these topics became so popular now? Certainly, recent feats such as AlphaGo or Waymo may raise awareness and hint to potential implications in our social organization, especially in the labor sector. Still, but what underlying technical advances have enabled these achievements - and what makes deep learning a paradigm so relevant within the past few years?</p>
<p>In the aforementioned fields, one common task is to reason upon prior knowledge to derive plausible conclusions, a task known as inference. On this matter, strategies within artificial intelligence like expert systems had shared some success by encapsulating an expert's rationale into knowledge bases. These databases were then used by an inference engine, typically using logical rules, to reason out conclusions.</p>
<p>However, as tasks grow in complexity, performance becomes hindered by computational limitations of reaching a decision using an inference engine in a large database. Not just that, but the lack of satisfactory techniques for reasoning under uncertainty was also an important obstacle that hampered the further development of expert systems (<a href="#References">Heathfield (1999)</a>).</p>
<p>A different paradigm, brought forward by machine learning, is to derive the relationships needed for inference using observations. More importantly, these models must be able to generalize this behavior for unseen instances of the given problem. Oftentimes, these relationships within data can be quite convoluted and in recent decades, a class of learning algorithms came to attention by the ability to derive these mappings: local kernel methods.</p>
<p>This family of methods is a fairly recent grouping of techniques that have been developed independently for several years. They are called so due to the transformation performed so-called kernel function onto the input data, and some examples of models are Support Vector Machines, Gaussian processes, among other techniques. A local kernel method can be defined as:</p>
<p>$$f(x) = b + \sum^n_{i=1} \alpha_iK_D\left(x, x_i\right)$$</p>
<p>where $b$ is the bias term, $D$ is the set of observations of cardinality $n$, $α_i$ is scalar chosen by the learning algorithm upon $D$, $x_i$'s are the training input observations and $K_D$ is the kernel function. These methods are more effective than linear models by being able to generate more favorable representations of feature space by using kernels (usually non-linear), achieving fairly positive results in several applications.</p>
<div>
    <img src="./figures/dataset.svg" width="50%" align="left" style="margin-bottom: 1.5rem;"/>
    <img src="./figures/k_dataset.svg" width="50%" align="right" style="margin-bottom: 1.5rem;"/>
    <p style="font-size:0.8em;" align="center">Transformation of features to generate a linearly-separable decision space. On the left the original dataset with two classes. On the right a transformation using PCA with a Gaussian kernel.</p>
</div>
<p>One typical kernel function is the Gaussian kernel:</p>
<p>$$K_\sigma(u, v) = e^{-\frac{\left\Vert u-v \right\Vert^2}{2\sigma^{2}}}$$</p>
<p>where $\sigma^2$ is the equivalent of the variance in a gaussian probability density function. Note that the function is symmetric and as $u$ and $v$ become very distant from each other, the function approaches zero. This means that for the estimation of a query point $u$, the neighboring values of $u$ have much more influence than those further away from it.</p>
<p>As postulated by David Wolpert in his <a href="#References"><em>no free lunch theorem</em></a>, no model performs better than random guess without any kind of assumption. Local kernel methods are no exception: they implicitly or explicitly partition the input space, obtaining good generalization for a new query point by exploiting the training examples in its neighborhood. This comes at the price of the so-called <em>smoothness prior</em>, meaning that these models expect a target function with little variation of results among neighboring observations.</p>
<hr>
<p>We may insert additional priors through the choice of the kernel as well as the feature representation. This is why for these types of models it might be beneficial to generate alternative representations of the input data. Rather than using their raw representation, these modifications could more closely conform to the model smoothness prior, yielding better results. Although not targeted at handcrafting representations, <a href="#References">Bengio et al. (2014)</a> provides a thorough analysis of good characteristics for representations of features.</p>
<hr>
<p>However, for particular types of problems the smoothness assumption may not hold, posing a predicament to this family of methods. As an illustration, consider the task of recognizing a character from a $\texttt{20×20}$ pixel grid. If we consider the number of variations of a simple horizontal translation, we notice that a particular observation of the letter $O$ has much more similarity with the letter $U$ in the exact position and orientation that the same letter $O$ shifted horizontally, for instance.</p>
<img src="./figures/letters.svg" width="80%"/>
<p style="font-size:0.8em;" align="center">Demonstrations of the amount of variation among observations inter and intra-class.</p>
<p>This indicates that the target function may not be as smooth as expected. In this highly non-linear function, local kernel methods require a very large number of training examples to cover all the desired variations within a class. Hence, the number of necessary templates can grow exponentially with the intrinsic dimension of a class manifold (<a href="#References">Bengio and Lecun (2007)</a>).</p>
<img src="./figures/manifold.svg" width="80%"/>
<p style="font-size:0.8em;" align="center">The set of observations associated with the same class forms a manifold or a set of disjoint
manifolds, i.e. regions of lower dimension than the original space of images. When this manifold is smooth, it can be approximated locally by linear patches, tangent to the manifold. However, as the manifold becomes irregular and highly dimensional, patches become smaller and exponentially many patches, as well as data, are required to obtain a good generalization. Source: <a href="#References">Bengio (2009)</a></p>
<p>Note that this does not confront the ability of local kernel methods to approximate functions, rather their efficiency on approximating a function in regards to data. By contrast, neural network models are capable of dealing with intra-class variations more effortlessly. While each kernel function is activated in a small area of the input space, feature detectors of neural networks are capable of detecting certain patterns (edges or curvatures) almost independently from its orientation. The combination of these feature detectors is what makes neural network representations so efficient.</p>
<p>As an illustration, consider the task of representing a number from 0-8. The representation generated by local methods can be seen as a one-hot encoding, signaling that each representation is mutually exclusive, for instance, $\texttt{00000001}$ signals $1$ and $\texttt{00000010}$ signals $2$ but the representation $\texttt{00000011}$ does not convey any valid information. Conversely, a representation of a neural network can be seen as a binary representation, much more efficient: $\texttt{001}$ is equal to $1$, $\texttt{010}$ to 2 and $\texttt{011}$ to $3$.</p>
<p>In terms of classification, local methods produce mutually exclusive partitions onto the feature space which it oftentimes makes it difficult to generalize for unseen instances of the problem, if the target function is not smooth. On the other hand, neural networks generate several partitions which are compositional, presenting a more efficient decision surface that can be more robust to unseen query points.</p>
<img src="./figures/partitions.svg" width="90%"/>
<p style="font-size:0.8em;" align="center">Sketch of a partition of the feature space produced by local methods (<i>left</i>) and neural networks (<i>right</i>). Note that the sub-partitions of the right figure can be combined to infer unseen instances of the problem, represented by the symbol <b>$?$</b>. These sub-partitions can be seen as potential explanatory factors. As an illustration, these can be the color, shape, and texture in the task of detecting particular types of fruits. Conversely, the partitions on the left symbolize specific combinations of these factors that can indicate a certain fruit. Source: <a href="#References">Bengio (2009)</a></p>
<p>The set of tasks illustrated so far are relatively simple in comparison to those that we see today, as in multi-class object detection. In previous decades, these simpler tasks were carried out relying on a shallow neural network architecture, i.e. using only a small number of layers. However, as the complexity of applications increase, this approach tends to fail.</p>
<p>Complex image classification tasks, for instance, pose a challenge to these methods namely because it becomes overwhelmingly difficult to combine a larger number of low-level features (pixels) to determine a contrasting abstract outcome (object). Alternatively, deep learning is able to better accomplish this task (<a href="#References">Bradley (2010, pg. 22)</a>), as experimental outlines in <a href="#References">He et al. (2015)</a> and <a href="(#References)">Krizhevsky et al. (2012)</a> indicate breakthrough performances upon increase in network depth.</p>
<img src="./figures/imagenet.svg" width="80%"/>
<p style="font-size:0.8em;" align="center">Results of the 2012 ImageNet classification competition. Only the first out of 5 top models was a deep neural network, which achieved unprecedented performance. Source: <a href="http://image-net.org/challenges/LSVRC/2012/results.html">ImageNet</a></p>
<p>Hierarchical learning, deep structured learning, or simply deep learning can be defined as <em>a class of machine learning techniques that exploit many layers of non-linear units</em> (<a href="#References">Deng and Yu (2013, pg. 199)</a>). This better capability is attained by the substantial coupling of non-linear operations, deriving complex feature hierarchies from low-level inputs.</p>
<img src="./figures/deep_learning_pg6.png" width="60%"/>
<p style="font-size:0.8em;" align="center">Compositional representation of features throughout layers: from pixels to gradients and edges. <br>Source: <a href="#References">Goodfellow et al. (2016, pg. 6)</a></p>
<p>As illustrated, different representations are synthesized along the layers and as representations become more related to the task in hand, the better the model can perform. However, how many layers are necessary to well approximate a function? In reality, the absolute number of layers is not the important factor, rather how many of these are necessary to effectively represent the target function, denominated as the <em>compact representation</em>. Moreover, if a configuration is smaller than the compact representation, that can generate a negative effect on performance.</p>
<p>It turns out that a configuration with one layer less than the compact representation may need an exponential number of neurons to achieve the same performance. This can be illustrated by the awarded work of <a href="(#References)">Håstad (1986)</a>. By analogy, if we consider neural networks as simple logic circuits (which they are able to emulate), we can observe that a logic architecture limited in-depth presents an exponential number of components in comparison with a deep counterpart. Consider the calculation of the parity function, defined as:</p>
<p>$$f:\{0,1\}^{n}\rightarrow\{0,1\},\thinspace f(x)=\left(\overset{|x]}{\underset{i=1}{\sum}}x_{i}\right) mod\space 2$$</p>
<p>The number of logical components for a depth-limited architecture of 2 layers and $N$ inputs is of an order of $O(2^{N})$. On the other hand, unbounded architectures can produce less complex systems, such as the daisy-chain structure of complexity $O(N\thinspace log\thinspace N)$.</p>
<img src="./figures/logic_circuits.png" width="80%"/>
<p style="font-size:0.8em;" align="center">Distinct architectures to compute the parity function. A Disjunctive normal form structure (left) with a complexity of $2^{N-1}$ and a balanced tree structure (right) with 5 layers and complexity of $O(N\thinspace log\thinspace N)$.</p>
<p><a name="back1"></a>Conversely, what could happen when freely adding more layers? In principle, besides the additional computational cost, this would not imply any counterpoints to approximating the target function, since the spare layers could simply replicate the second-to-last representation output. However, as in the shallow case, the model may carry too many parameters and insufficient training examples. This, in turn, would have statistical drawbacks: instead of approximating the desired mapping, the model starts to memorize the training data, causing overfitting, i.e. poor generalization<sup><a href="#1">1</a></sup>.</p>
<p><a name="back2"></a>Despite the remarked capacity, deep architectures were not widely researched in the past as today. The reasons for this may encompass the former contemptuous view of researchers on this topic, or an insufficient computational power especially compared to contemporary standards. This, however, will not be covered in this series. The challenges of employing such architectures were also imposed by the difficulty in training neural networks with more than two layers, ultimately delivering poor generalization (<a href="#References">Bengio et al. (2007)</a>)<sup><a href="#2">2</a></sup>.</p>
<p>In the next series of posts, I cover some of the improvements and predicaments that have followed the development of deep learning, starting from <a href="../pretraining">unsupervised pretraining</a>.</p>
<p><a name="1"></a></p>
<p style="font-size:0.6em;">
<br>
<b>1</b> This is a generally accepted consequence of very deep models that do not have enough training observations. However, recent research by <a href="https://windowsontheory.org/2019/12/05/deep-double-descent/">Nakkiran et al.</a> points to a completely different direction [<a href="#back1">return</a>].
</p>
<p><a name="2"></a></p>
<p style="font-size:0.6em;">
<b>2</b> <a href="#References">Bengio (2009, pg. 24)</a> appropriately points out that Convolutional Neural Networks with up to seven layers did not suffer as other types of models. This may be due to what the author denominates as a Topographic Structure, suggesting that each neuron in a layer is associated with a small portion of the input. From this, two favorable aspects are hypothesized: the gradient is propagated less diffusely and this hierarchical local connectivity structure may be more suited for image recognition tasks, which is the most common use for such architecture [<a href="#back2">return</a>].
</p>
<h2><a name="References"></a>References</h2>
<ul style="font-size:0.6em;">
    <li>Heathfield, H. (1999). The rise and “fall” of expert systems in medicine. Expert Systems. 183–188.</li>
    <li>Wolpert, D. (1996), The Lack of A Priori Distinctions between Learning Algorithms. Neural Computation, 1341-1390.</li>
    <li>Bengio, Y., Courville, A. and Vincent, P. (2014). Representation Learning: A Review and New Perspectives</li>
    <li>Bengio, Y. and LeCun, Y. (2007). Scaling Learning Algorithms towards AI. 41.</li>
    <li>Bengio, Y. (2009). Learning Deep Architectures for AI. Foundations and Trends in Machine Learning. 1-127.</li>
    <li>Bengio, Y., Lamblin, P., Popovici, D., and Larochelle, H. (2007). Greedy Layer-Wise Training of Deep Networks. In Advances in Neural Information Processing Systems 19, 153-160.</li>
    <li>Bradley, D. M. (2010). Learning In Modular Systems. PhD Thesis, Carnegie Mellon University.</li>
    <li>Olah, C. (2014). Neural Networks, Manifolds, and Topology. Colah's blog</li>
    <li>Deng, L. and Yu, D. (2013). Deep Learning: Methods and Applications.
    Foundations and Trends in Signal Processing, 7-197.</li>
    <li>Krizhevsky, A., Sutskever, I., and Hinton, G. E. (2012). ImageNet Classification with Deep Convolutional Neural Networks. Advances in Neural Information Processing Systems 25, 1097-1105.</li>
    <li>He, K., Zhang, X., Ren, S., and Sun, J. (2015). Deep Residual Learning for Image Recognition. Proceedings of the IEEE conference on computer vision and pattern recognition.</li>
    <li>Goodfellow, I., Bengio, Y., and Courville, A. (2016). Deep Learning. MIT Press.</li>
</ul>
<style>
    h1 {
        text-align: left;
    }

    body {
        text-align: justify;
    }

</style>

    </div>

    


    



    
      








  






  
  
  
    
  
  
  <div class="media author-card">
    
      <img class="portrait mr-3" src="https://s.gravatar.com/avatar/f6c8c4a6a41a1172a087f1635c006d61?s=200')" alt="Avatar">
    

    <div class="media-body">
      <h5 class="card-title"><a href="https://artur-deluca.github.io/">Artur de Luca</a></h5>
      <h6 class="card-subtitle">Graduate Student in Artificial Intelligence and Robotics<br> at La Sapienza</h6>
      
      <ul class="network-icon" aria-hidden="true">
        
          
          
          
            
          
          
          
          
          
            
          
          <li>
            <a href="https://twitter.com/backdeluca" target="_blank" rel="noopener">
              <i class="fab fa-twitter"></i>
            </a>
          </li>
        
          
          
          
            
          
          
          
          
          
            
          
          <li>
            <a href="https://github.com/artur-deluca" target="_blank" rel="noopener">
              <i class="fab fa-github"></i>
            </a>
          </li>
        
          
          
          
            
          
          
          
          
          
            
          
          <li>
            <a href="https://www.linkedin.com/in/arturbackdeluca" target="_blank" rel="noopener">
              <i class="fab fa-linkedin"></i>
            </a>
          </li>
        
          
          
          
          
          
          
          
            
          
          <li>
            <a href="/files/cv.pdf" >
              <i class="ai ai-cv"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>



      
      
    

    

    

    <div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "artur-deluca-github-io" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

  </div>
</article>

      

    
    
    
    <script src="/js/mathjax-config.js"></script>
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/8.0.0/mermaid.min.js" integrity="sha256-0w92bcB21IY5+rGI84MGj52jNfHNbXVeQLrZ0CGdjNY=" crossorigin="anonymous" title="mermaid"></script>
      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/highlight.min.js" integrity="sha256-aYTdUrn6Ow1DDgh5JTc3aDGnnju48y/1c8s1dgkYPQ8=" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/languages/python.min.js"></script>
        
      

      
      
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML-full" integrity="sha256-GhM+5JHb6QUzOQPXSJLEWP7R73CbkisjzK5Eyij4U9w=" crossorigin="anonymous" async></script>
      
    

    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.js" integrity="sha512-lInM/apFSqyy1o6s89K4iQUKg6ppXEgsVxT35HbzUupEVRh2Eu9Wdl4tHj7dZO0s1uvplcYGmt3498TtHq+log==" crossorigin="anonymous"></script>
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script>
      const search_index_filename = "/index.json";
      const i18n = {
        'placeholder': "Search...",
        'results': "results found",
        'no_results': "No results found"
      };
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.130521ecfc6f534c52c158217bbff718.js"></script>

    






  
  <div class="container">
    <footer class="site-footer">
  
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
