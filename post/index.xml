<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | arturdeluca</title>
    <link>https://artur-deluca.github.io/post/</link>
      <atom:link href="https://artur-deluca.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Thu, 09 May 2019 00:00:00 -0500</lastBuildDate>
    <image>
      <url>https://artur-deluca.github.io/img/icon-192.png</url>
      <title>Posts</title>
      <link>https://artur-deluca.github.io/post/</link>
    </image>
    
    <item>
      <title>Deep learning advances: unsupervised pretraining</title>
      <link>https://artur-deluca.github.io/post/pretraining/</link>
      <pubDate>Thu, 09 May 2019 00:00:00 -0500</pubDate>
      <guid>https://artur-deluca.github.io/post/pretraining/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;These series intend to shed some light on the problems and corresponding solutions that have followed the development of deep learning. I don&#39;t intend to make this a comprehensive review of all techniques but to select some of the prominent ones.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In the early development of neural networks, the initialization of parameters was performed by sampling a uniform distribution bounded to a predetermined interval. In this process, the weights and biases produced solutions that would generally lie far from an optimum set of parameters, requiring many training iterations to readjust. Shallow networks did not suffer too much from this due to the smaller number of parameters, hence presenting an easier system to optimize. However, as the depth of networks increased, so did the difficulty to train such models using this initialization procedure.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;#References&#34;&gt;Glorot and Bengio (2010)&lt;/a&gt; promoted a study to understand why random initialization performed so poorly in deep networks. In this investigation, the authors considered one common initialization heuristic, introduced by &lt;a href=&#34;#References&#34;&gt;LeCun et al. (1998)&lt;/a&gt;, that defines the biases at 0 and the weights via sampling according to the following uniform distribution:&lt;/p&gt;
&lt;p&gt;$$W_{ij}\sim U\left[-\frac{1}{\sqrt{n}},\frac{1}{\sqrt{n}}\right]$$&lt;/p&gt;
&lt;p&gt;where $n$ is the number of inputs to the unit. The authors then verified that in deep configurations of 4 to 5 layers, activation values on the last layers got stuck in plateaus situated at the extremes of the activation function, a case otherwise known as saturation.&lt;/p&gt;
&lt;img src=&#34;./figures/saturation.png&#34; width=&#34;80%&#34;/&gt;
&lt;p style=&#34;font-size:0.8em;&#34; align=&#34;center&#34;&gt;Saturation regions on the sigmoid function. In the case observed by &lt;a href=&#34;#References&#34;&gt;Glorot and Bengio (2010)&lt;/a&gt;, the saturation occurred in the 5th and last layer of the network, with activation values converging to zero.&lt;/p&gt;
&lt;p&gt;One hypothesis that explains saturation on sigmoid-equipped deep networks is that the random initialization does not provide useful information to the last layer of the network, that starts to suppress the previous contributions and rely more on its biases, which, in turn, are trained faster than its weights.&lt;/p&gt;
&lt;img src=&#34;./figures/saturation_plot.png&#34; width=&#34;100%&#34;/&gt;
&lt;p style=&#34;font-size:0.8em;&#34; align=&#34;center&#34;&gt;Mean (&lt;i&gt;lines&lt;/i&gt;) and standard deviation (&lt;i&gt;vertical bars&lt;/i&gt;) of sigmoid activation values across layers in a neural network using random initialization. The saturation is detectable in the last layer, where the activation values reach virtually zero. Source: &lt;a href=&#34;#References&#34;&gt;Glorot and Bengio (2010)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Gradually but rapidly, the error gradient tends to push the activations towards zero in an attempt to suppress the influence of the previous layers. Eventually, the saturation may be overcome but the overall result would be of poor generalization.&lt;/p&gt;
&lt;h3 id=&#34;unsupervised-pre-training&#34;&gt;Unsupervised pre-training&lt;/h3&gt;
&lt;p&gt;Layer saturation was one of the biggest technical hurdles that limited the progress of deep learning in the dawn of the millennium. However, in 2006, inspired by a well-established procedure, &lt;a href=&#34;#References&#34;&gt;Hinton et al. (2006)&lt;/a&gt; developed a novel approach to initialize the parameters of a Deep Belief Network — a class of neural networks — that overcame the saturation issue and surpassed state of the art performance in deep architectures. These results not only re-sparked but drastically expanded researchers’ interest in this field.&lt;/p&gt;
&lt;img src=&#34;./figures/deep_belief_net.svg&#34; width=&#34;80%&#34;/&gt;
&lt;p style=&#34;font-size:0.8em;&#34; align=&#34;center&#34;&gt;A Deep Belief Network (DBN) can be seen as a stack of smaller unsupervised learning algorithms named Restricted Boltzmann Machines. This configuration can then be bundled with a classical multi-layer perceptron for supervised learning tasks&lt;/p&gt;
&lt;p&gt;This initialization procedure encompassed an unprecedented process: an unsupervised greedy layer-wise pre-training step&lt;sup&gt;1&lt;/sup&gt;. Prior to the conventional supervised training, each layer is trained with its anterior neighboring layer identically to a Restricted Boltzmann Machine, using an unsupervised learning algorithm named Contrastive Divergence. This process starts with the input and first layer, and progressively advances one layer at a time until it sweeps all layers.&lt;/p&gt;
&lt;p&gt;A Boltzmann Machine is an unsupervised generative algorithm that learns the data representation by associating the patterns identified in the inputs to probabilistic configurations within its parameters. A Restricted Boltzmann Machine is a variation of such a model that reproduces a similar behavior but with significantly fewer connections.&lt;/p&gt;
&lt;p style=&#34;font-size:0.6em;&#34;&gt;
&lt;b&gt;1&lt;/b&gt; Despite the imprecision, unsupervised pre-training is here used interchangeably
&lt;/p&gt;
&lt;img src=&#34;./figures/BM_RBM.svg&#34; width=&#34;80%&#34;/&gt;
&lt;p style=&#34;font-size:0.8em;&#34; align=&#34;center&#34;&gt;A Boltzmann Machine (&lt;i&gt;left&lt;/i&gt;) and a Restricted Boltzmann Machine (&lt;i&gt;right&lt;/i&gt;)&lt;/p&gt;
&lt;p&gt;Many other unsupervised pre-training algorithms were developed concomitantly or immediately after, such as autoencoders (&lt;a href=&#34;#References&#34;&gt;Bengio et al. (2007)&lt;/a&gt;), denoising autoencoders (&lt;a href=&#34;#References&#34;&gt;Vincent et al. (2008)&lt;/a&gt;), contractive autoencoders (&lt;a href=&#34;#References&#34;&gt;Rifai et al. (2011)&lt;/a&gt;), among others.&lt;/p&gt;
&lt;img src=&#34;./figures/pretraining_improvement_comparison.png&#34; width=&#34;100%&#34;/&gt;
&lt;p style=&#34;font-size:0.8em;&#34; align=&#34;center&#34;&gt;Comparison of performance between networks running without pre-training (&lt;i&gt;left&lt;/i&gt;) and with pre-training (&lt;i&gt;right&lt;/i&gt;) &lt;br&gt;Source: &lt;a href=&#34;#References&#34;&gt;Erhan et al. (2010, pg. 636)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Why does this unsupervised learning methods help training deep architectures? Much of the explanation remains uncertain. Nonetheless, &lt;a href=&#34;#References&#34;&gt;Erhan et al. (2010)&lt;/a&gt; provide some clarifications through considerable experimentation. The claims of the authors reside on two possible but not mutually exclusive reasons: &lt;i&gt;optimization&lt;/i&gt; and &lt;i&gt;regularization&lt;/i&gt;.&lt;/p&gt;
&lt;p&gt;Deep neural networks are composed of many parameters whose values are used to compute an approximation of a function. Due to its substantial nonlinear nature, this approximation yields a non-convex function that poses a challenge on searching the best combination of weights and biases.&lt;sup&gt;2&lt;/sup&gt;&lt;/p&gt;
&lt;img src=&#34;./figures/convex_nonconvex.png&#34; width=&#34;100%&#34;/&gt;
&lt;p style=&#34;font-size:0.8em;&#34; align=&#34;center&#34;&gt;A convex (left) and non-convex (right) function. Note that, contrarily to the convex function, the non-convex function possesses multiple local optima. Source: [Zadeh (2016)](#References)&lt;/p&gt;
&lt;p style=&#34;font-size:0.6em;&#34;&gt;
&lt;b&gt;2&lt;/b&gt; Many discussions today in academia evolve around the particular shape of loss landscape in deep neural networks, since many of the local minima appear to have equally good qualities, suggesting that the critical solutions reached during training are actually saddle points. This discussion will be reserved to &lt;a href=&#34;https://github.com/artur-deluca/landscapeviz&#34;&gt;further studies&lt;/a&gt;.
&lt;/p&gt;
&lt;p&gt;Gradient-based methods employed in training eventually converge to their pre-selected basin of attraction, a region of the function space, such that any point in it eventually is iterated into the attractor (roughly speaking, a valley in the loss function). Unsupervised pre-training may work towards optimization by favoring a basin of attraction that might yield a lower training error. Thus, since the gradients are very prompt to abrupt changes, backpropagation is only used at a local search level, from an already favorable starting point &lt;a href=&#34;#References&#34;&gt;(Hinton (2012, lecture 14b))&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;As for &lt;i&gt;regularization&lt;/i&gt;, one may commonly associate it with explicit techniques, such as the L1 or L2 norm:&lt;/p&gt;
&lt;p&gt;$$C=-\frac{1}{n}\sum_{j}\left[y_j\ln a_j^{L}+\left(1-y_j\right)\ln\left(1-a_j^{L}\right)\right]+\frac{\lambda}{2n}\sum_iw_i{{}^2}$$&lt;/p&gt;
&lt;p&gt;By adding the &lt;i&gt;L2 regularization&lt;/i&gt; factor in the cross-entropy cost function, presented in the equation above, one can penalize overly complex models, that would result in poor generalization, i.e. &lt;i&gt;high testing error&lt;/i&gt;. However, the regularization employed by pre-training is implicit. In attempt to model how such technique would work explicitly, &lt;a href=&#34;#References&#34;&gt;Erhan et al. (2009)&lt;/a&gt; defines a regularization term such as:&lt;/p&gt;
&lt;p&gt;$$regularizer=-log\thinspace P\left(\theta\right)$$&lt;/p&gt;
&lt;p&gt;The function $P\left(\theta\right)$ describes the probability that the weights of the neural network are initialized as $\theta$. So, if a configuration shows to be highly improbable, the regularizer term will hurt the cost function strongly. Furthermore, if the probability of landing such set of parameters is high, the regularizer will then reward the cost function. This probability is governed by the initialization methods employed. Considering two instances with and without pre-training, we obtain:&lt;/p&gt;
&lt;p&gt;$$P_{{\text{pre-training}}}(\theta) = \sum_k\frac{1_{\theta \in R_k}\pi_k}{v_k}$$&lt;/p&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;p&gt;$$P_{{\text{no pre-training}}}(\theta) = \sum_k\frac{1_{\theta \in R_k}r_k}{v_k}$$&lt;/p&gt;
&lt;br&gt;
&lt;p&gt;where $R_k$ is the basin of attraction that contains $\theta$, and $1_{\theta \in R_k}$ is the identifier function – unitary for all $\theta$ in $R_k$, otherwise null. Additionally, $\pi_k$ and $r_k$ are the probabilities of landing in the basin of attraction $R_k$, which has a volume $v_k$. Since the basins of attraction are disjunct sets, the probability density function of the set of parameters located in $R_k$ is uniform, calculated by taking the probability of landing in the k-th basin ($\pi_k$ or $r_k$) and dividing by its volume.&lt;/p&gt;
&lt;p&gt;Pre-training the parameters of the network conditions the network initialization to land on regions of better generalization. This is hypothetically achieved by increasing the $\pi_k$&#39;s where the network parameters represent meaningful variations of the input, contributing to predict the output. For this reason, pre-training also reduces the variance upon parameter initialization.&lt;/p&gt;
&lt;img src=&#34;./figures/trajectories.png&#34; width=&#34;70%&#34;/&gt;
&lt;p style=&#34;font-size:0.8em;&#34; align=&#34;center&#34;&gt;2-D visualization of parameters&#39; trajectory of 100 neural networks with and without the unsupervised pre-training step. The color gradient from dark-blue to cyan symbolizes the progression of iterations. &lt;br&gt;Source: &lt;a href=&#34;#References&#34;&gt;Erhan et al. (2010, pg. 541)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The visualization of the parameters’ trajectory may demonstrate the effects of optimization and regularization. As mentioned through the former, it may select a basin of attraction with lower training errors. Conversely, regularization may bound the parameter interval to a range that yields good generalization. Also, it is crucial to notice that both training and testing errors collected in the experiments of &lt;a href=&#34;#References&#34;&gt;Erhan et al. (2010)&lt;/a&gt; support these hypotheses, but do favor the latter.&lt;/p&gt;
&lt;p&gt;Furthermore, once established within the solution space, the parameters do not drastically change during the gradient-based adjustment process. This process is also denominated fine-tuning, as it only modifies the features slightly to get the category boundaries, rather than discovering new relationships (&lt;a href=&#34;#References&#34;&gt;Hinton (2012, lecture 14b)&lt;/a&gt;).&lt;/p&gt;
&lt;img src=&#34;./figures/trained_filters.svg&#34; width=&#34;90%&#34;/&gt;
&lt;p style=&#34;font-size:0.8em;&#34; align=&#34;center&#34;&gt;Visualization of filters of a Deep Belief Network used to recognize digits form the MNIST data-set after the different training processes; from left to right: units from the first, second and third layers, respectively.&lt;br&gt;Source:&lt;a href=&#34;#References&#34;&gt;Erhan et al. (2010, pg. 638-639)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;But how can one conceptually understand the effects of unsupervised learning? Apart from the regularization and optimization hypothesis, the layer-wise pre-training resembles the underlying distribution of the input. Ideally, this representation, by combining the different features and mapping their inner relationships, can unveil, and more importantly, disentangle causal elements that influence the output. If those inputs can be transformed into uncorrelated features, it is possible to solve for a particular parameter disregarding its influence over the others.&lt;/p&gt;
&lt;p&gt;As mentioned in &lt;a href=&#34;#References&#34;&gt;Goodfellow et al. (2016, pg. 541)&lt;/a&gt;, this hypothesis justify approaches in which one first seeks a good representation for $p(x)$ before training with the output. If the output is closely related to factors captured by the input, an initialization that captures the distribution of x is useful for predicting the desired output distribution $p\left(y|x\right)$.&lt;/p&gt;
&lt;p&gt;Furthermore, unsupervised pretraining can be related with the recent work of &lt;a href=&#34;#References&#34;&gt;Schwartz-Ziv and Tishby (2017)&lt;/a&gt; on studying neural networks from an information theory perspective. Essentially, the authors claim that the learning process of a neural network model is based on the maximizing the mutual information between the inputs and the outputs. Mutual information can be defined as:&lt;/p&gt;
&lt;p&gt;$$I(X,Y) = H(X)-H(X|Y)$$&lt;/p&gt;
&lt;p&gt;where $H$ is the entropy of the variable $X$:&lt;/p&gt;
&lt;p&gt;$$H(X) = \mathbb{E}[-\log\thinspace(P(X)]$$&lt;/p&gt;
&lt;p&gt;Entropy essentially measures the amount of information, i.e. the degree of uncertainty one has over a random variable. Moreover, when provided another random variable Y, we can measure the conditional entropy of X given Y:&lt;/p&gt;
&lt;p&gt;$$H(Y|X)=-\sum _{x\in {\mathcal {X}},y\in {\mathcal {Y}}}p(x,y)\log {\frac {p(x,y)}{p(x)}}
$$&lt;/p&gt;
&lt;p&gt;Thus, mutual information is a statistical measurement between two random variables that indicate how much knowing one of these variables reduces uncertainty about the other. This, in turn, may have a connection with training neural networks as one begins the training stage knowing to little about the input and output, hence, having low mutual information stored in the neural network layers.&lt;/p&gt;
&lt;img src=&#34;./figures/tishby_anim.gif&#34; width=&#34;100%&#34;/&gt;
&lt;p style=&#34;font-size:0.8em;&#34; align=&#34;center&#34;&gt;Mutual information measurements on 100-layer neural networks. Layer ordering ranges from green (initial layers) to orange (final layers). Source:&lt;a href=&#34;#References&#34;&gt;Schwartz-Ziv and Tishby (2017)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;However, as the training phase starts, the layers slowing move towards higher mutual information over $X$ and then towards $Y$. What happens in principle is that in the beginning, the network layers learn different representations over the input space, which in turn carry a lot of information over the input, but also some information about the output. As the training phase continues, the network layers, particularly the deeper ones, then discard some of the irrelevant information of $X$ which is not predictive of $Y$.&lt;/p&gt;
&lt;img src=&#34;./figures/tishby.png&#34; width=&#34;100%&#34;/&gt;
&lt;p style=&#34;font-size:0.8em;&#34; align=&#34;center&#34;&gt;Snapshot of mutual information measurements along layers in different trainig epohcs. On the left, the neural network is at epoch zero, on the center at epoch 400, and the on right at epoch 9000. Source:&lt;a href=&#34;#References&#34;&gt;Schwartz-Ziv and Tishby (2017)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The pretraining procedure can be related to the theory, particularly in the first part. By using an unsupervised learning algorithm, we may find a combination of parameters for each layer along with the network that shares higher mutual information with the input variable $X$ and consequently some with the output variable $Y$. As the gradient-based learning process continues, some of this information gets refined, as previously mentioned.&lt;/p&gt;
&lt;p&gt;However, despite the aforementioned advantages, unsupervised pre-training presents noteworthy drawbacks, such as establishing two separate learning stages (unsupervised and supervised). As a consequence, there is a long delay between adjusting hyperparameters on the first stage utilizing feedback from the second. Additionally, although pre-training being considered a valuable regularizer, its strength adjustment is troublesome, requiring a somewhat unclear modification of far too many hyperparameters — contrasting with explicit regularization techniques that can be adjusted by a single one.&lt;/p&gt;
&lt;p&gt;For the reasons mentioned above, unsupervised pre-training is not so popularly used today, as other techniques discovered yielded the same benefits but much more efficiently. These will be explained in the following post, where these strategies will also tackle a different obstacle: vanishing gradients.&lt;/p&gt;
&lt;h1&gt;&lt;a name=&#34;References&#34;&gt;&lt;/a&gt;References&lt;/h1&gt;
&lt;ul style=&#34;font-size:0.6em;&#34;&gt;
    &lt;li&gt;Bengio, Y. (2009). Learning Deep Architectures for AI. Foundations and Trends in Machine Learning. 1-127.&lt;/li&gt;
    &lt;li&gt;Bengio, Y., Lamblin, P., Popovici, D., and Larochelle, H. (2007). Greedy Layer-Wise Training of Deep Networks. In Advances in Neural Information Processing Systems 19, 153-160.&lt;/li&gt;
    &lt;li&gt;Erhan, D., Bengio, Y., Courville, A., Manzagol, P.A., Vincent, P., and Bengio, S. (2010). Why Does Unsupervised Pre-training Help Deep Learning? Journal of Machine Learning Research, 11-36.&lt;/li&gt;
    &lt;li&gt;Erhan, D., Manzagol, P.A., Bengio, Y., Bengio, S., and Vincent, P. (2009). The Difficulty of Training Deep Architectures and the Effect of Unsupervised Pre-Training. Artificial Intelligence and Statistics, 153-160.&lt;/li&gt;
    &lt;li&gt;Glorot, X. and Bengio, Y. (2010). Understanding the diffculty of training deep feedforward neural networks. Proceedings of the Thirteenth International Conference on Artificial Intelligence and
    Statistics , pages 249-256.&lt;/li&gt;
    &lt;li&gt;Goodfellow, I., Bengio, Y., and Courville, A. (2016). Deep Learning. MIT Press.&lt;/li&gt;
    &lt;li&gt;Hinton, G. (2012). Neural Networks for Machine Learning. Coursera Online Course.&lt;/li&gt;
    &lt;li&gt;Hinton, G. E., Osindero, S., and Teh, Y.-W. (2006). A Fast Learning Algorithm for Deep Belief Nets. Neural Computation , 1527-1554.&lt;/li&gt;
    &lt;li&gt;LeCun, Y. A., Bottou, L., Orr, G. B., and Müller, K.-R. (1998). Efficient BackProp. Neural Networks: Tricks of the Trade, 9-48.&lt;/li&gt;
    &lt;li&gt;Rifai, S., Vincent, P., Muller, X., Glorot, X., and Bengio, Y. (2011).
    Contractive Auto-encoders: Explicit Invariance During Feature Extraction. Proceedings of the 28th International Conference 
    on Machine Learning, 833-840.&lt;/li&gt;
    &lt;li&gt;Vincent, P., Larochelle, H., Bengio, Y., and Manzagol, P.-A. (2008).
    Extracting and Composing Robust Features with Denoising Autoencoders. Proceedings of the 25th International Conference, 1096-1103.&lt;/li&gt;
    &lt;li&gt;Zadeh, R. (2016). The hard thing about deep learning. O&#39;Reilly Media.&lt;/li&gt;
    &lt;li&gt;Schwartz-Ziv, R. and Tishby, N. (2017). Opening the black box of Deep Neural Networks via Information. Featured on: Why &amp; When Deep Learning Works: Looking Inside Deep Learning. arXiv:1703.00810v3&lt;/li&gt;
&lt;/ul&gt;
&lt;style&gt;
    h1 {
        text-align: left;
    }

    body {
        text-align: justify;
    }

&lt;/style&gt;
</description>
    </item>
    
    <item>
      <title>Obstacles along deep learning evolution: vanishing gradients</title>
      <link>https://artur-deluca.github.io/post/init/</link>
      <pubDate>Thu, 09 May 2019 00:00:00 -0500</pubDate>
      <guid>https://artur-deluca.github.io/post/init/</guid>
      <description>&lt;style&gt;
body {
text-align: justify}
&lt;/style&gt;
&lt;p&gt;The objective of this series is to illustrate some of the past and on-going challenges within deep learning. In the previous blog post, the obstacle discussed was the &lt;a href=&#34;../saturation&#34;&gt;saturation&lt;/a&gt; within activation functions during training. This is a follow-up, so I recommend you take a brief look before continuing.&lt;/p&gt;
&lt;!--
## The challenges of training deep networks
Despite mentioning pre-training as solution for saturation, such technique is not employed as much today. This is due to fact that the properties that pre-training ensures can be attained by a much more efficient strategy which will be explained further on. 

https://arxiv.org/pdf/1504.06825.pdf
--&gt;
&lt;h3 id=&#34;problem-vanishing-gradients&#34;&gt;Problem: Vanishing gradients&lt;/h3&gt;
&lt;p&gt;As mentioned in the previous post, the activation values start to migrate towards the extremities of the sigmoid function in networks with several layers. But what happens to the gradients? Around the 1980&#39;s researchers at USC and CMU started to work on algorithms using the gradient of the loss function throughout the layers of the network as a way to adjust each weight with a corresponding &amp;ldquo;influence&amp;rdquo; over the error. These gradients were obtained analytically by applying the chain rule from the loss function to the desired weight. This is famously called the backpropagation technique, and has spurred the development of neural networks later on. (Cite hinton 1989)&lt;/p&gt;
&lt;p&gt;However, once again, as the stack of layers increase, obstacles arise to  hamper performance, and this time this is caused by a phenomenon known as the Vanishing/Exploding Gradients. To illustrate it, consider a network with a general cost function and symmetric activation functions $f$ with unit derivative at 0. If we write $a_{i}$ for the activation vector of layer $i$, and $s_{i}$ the argument vector of the activation function at layer $i$, we have $s_{i}=a_{i}W_{i}+b_{i}$ and $a_{i}=f(s_{i})$.&lt;/p&gt;
&lt;p&gt;Consider the following node output and activation:&lt;/p&gt;
&lt;p&gt;Consider the following node output and activation:&lt;/p&gt;
&lt;p&gt;$$s^{i}=W^{i}a^{i-1}+b_{i}$$&lt;/p&gt;
&lt;p&gt;$$a_{i+1}=g\left(W_{i}a_{i-1}+b_{i}\right)$$&lt;/p&gt;
&lt;p&gt;The derivative of the loss function $\mathcal{L}$ in a network with $l$ layers in terms of a weight $W_{n}$ can be written as:&lt;/p&gt;
&lt;p&gt;$$\frac{\partial\mathcal{L}}{\partial W_{n}}=\frac{\partial s_{n}}{\partial W_{n}}\left[\underset{i=n}{\overset{l-1}{\prod}}\frac{\partial a_{i+1}}{\partial s_{i}}\frac{\partial s_{i+1}}{\partial a_{i+1}}\right]\frac{\partial a_{l}}{\partial s_{l}}\frac{\partial\mathcal{L}}{\partial a_{l}}$$&lt;/p&gt;
&lt;p&gt;Defining a linear activation function $g(z_{i})=z_{i}$ and $W_{i}=W$, we obtain:&lt;/p&gt;
&lt;p&gt;$$\require{cancel}\frac{\partial\mathcal{L}}{\partial W_{n}}=\frac{\partial s_{n}}{\partial W_{n}}\left[\underset{i=n}{\overset{l-1}{\prod}}\cancelto{1}{\frac{\partial a_{i+1}}{\partial s_{i}}}\cancelto{W}{\frac{\partial s_{i+1}}{\partial a_{i+1}}}\right]\frac{\partial a_{l}}{\partial s_{l}}\frac{\partial\mathcal{L}}{\partial a_{l}}$$&lt;/p&gt;
&lt;p&gt;Thus:&lt;/p&gt;
&lt;p&gt;$$\frac{\partial\mathcal{L}}{\partial W_{n}}=a_{n-1}W^{n-l}\frac{\partial\mathcal{L}}{\partial a_{l}}$$&lt;/p&gt;
&lt;p&gt;So, in deep configurations, the backpropagation technique may become troublesome as a result of exploding or vanishing gradients. As the cost function is progressively derived in terms of numerically large parameters, these adjustments will likely overshoot, hampering training. Conversely, small contributions propagated through many layers may cause virtually no effect at all:&lt;/p&gt;
&lt;p&gt;Moreover, in deep configurations, the backpropagation technique may become troublesome as a result of exploding or vanishing gradients. As the cost function is progressively derived in terms of numerically large parameters, these adjustments will likely overshoot, hampering training. Conversely, small contributions propagated through many layers may cause virtually no effect at all:&lt;/p&gt;
&lt;p&gt;If the elements in W greater than one, and with a sufficiently large n-l value, $\frac{\partial\mathcal{L}}{\partial W_{n}}$ will tend to infinity, i.e. exploding. Conversely, for values less than one, the derivative tends to zero, i.e. vanishing.&lt;/p&gt;
&lt;p&gt;For the case of early neural network development, a typical initialization framework presented in Erhan et al. (2009) would have parameters randomly sampled from a uniform distribution centered in zero of $[-1/\sqrt{k};1/\sqrt{k}]$ where k is the size of the previous layer of a fully-connected network. For deep architectures, many nodes would likely be initialized very close to zero, leading to the previously mentioned vanishing gradient problem. Bradley (2010, pg. 25) provides an interesting illustration of this, by analyzing the distribution of weights in the initialization of the network. As the number of layers increased, the weights got significantly peaked around zero, thus evidenciating the cause of the issue.&lt;/p&gt;
&lt;h3 id=&#34;activation-functions-and-initialization-methods&#34;&gt;Activation functions and initialization methods&lt;/h3&gt;
&lt;p&gt;In the following course of deep learning development, improvements introduced into networks&amp;rsquo; training and architecture led to the near abandonment of the very one technique that resurged attention onto the field: unsupervised pre-training. Such advances were a combination of modern activation functions and simpler but sophisticated initialization methods. These also helped to mitigate the vanishing gradient problem. But appart from the ackowledged need for better alternatives to pre-training, what is the problem of using the once well-adopted activation function: the sigmoid? By exploring different functions, it became apparent that the activation was one of the main contributors to the saturation issue, demonstrated through research conducted by Glorot and Bengio (2010)&lt;/p&gt;
&lt;img src=&#34;./figures/saturation_plot.png&#34; width=&#34;100%&#34;/&gt;
&lt;p style=&#34;font-size:0.8em;&#34; align=&#34;center&#34;&gt;Mean (&lt;i&gt;lines&lt;/i&gt;) and standard deviation (&lt;i&gt;vertical bars&lt;/i&gt;) of sigmoid activation values across layers in a neural network using random initialization. The saturation is detectable in the last layer, where the activation values reach virtually zero. &lt;br&gt;Source: [Glorot and Bengio (2010)](#References)&lt;/p&gt;
&lt;p&gt;A technique employed way before such improvements although is goes along with them are the old standardizing of inputs and parameters. LeCun et al. (1998) points out that rescaling the input reducing its mean to 0 and variance to 1 may help training. The first prevents undiserable bias towards particular directions, making it less sensible to the distinc features scales while it also prevents undesirable uniform gradient updates if the majority of the input data where of the same sign. The latter helps to even the contributions from all features, balancing the rate at which the weights connected to the input nodes learn. Additionally it also helps to not saturate too early and does not go straight to zero. Such techniques are not only important on the inputs as they are desirable characteristics for each layer within the network. So, preserving the variance throughout layers during training is a desirable property. Additionally from the backpropagation point of view, the variance of the derivative of the cost function by the weights is equally important to be mantained.&lt;/p&gt;
&lt;p&gt;Nevertheless, expanding on &lt;a href=&#34;#References&#34;&gt;Glorot and Bengio (2010&lt;/a&gt; work, &lt;a href=&#34;#References&#34;&gt;Xu et al. (2016)&lt;/a&gt; draws theorical clues to indicate that in its linear regime, in comparison with other activation functions, the sigmoid is more prone to escalating the variance of layers througout training.&lt;/p&gt;
&lt;p&gt;Considering the following notation:&lt;/p&gt;
&lt;p&gt;$$s^i=f\left(s^{i-1}\right)W^i+b^i$$&lt;/p&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;p&gt;$$a^{i+1}=f\left(a^{i}W^{i}+b^{i}\right)$$&lt;/p&gt;
&lt;p&gt;where $f(s)$ and $s$ are the activation function and its argument vector, respectively; $a$ is the activation value, $W\in\mathbb{R}^{{n_i\times n_{i-1}}}$ the weight matrix and $b\in \mathbb{R}^{n_i}$ the bias vector. In a linear regime, the activation function can be modeled as:&lt;/p&gt;
&lt;p&gt;$$f(s)=\alpha s+\beta$$&lt;/p&gt;
&lt;p&gt;Assuming that, similar to the intialization presented in [eq:1], biases are set to 0, the variance at layer i can be defined as:&lt;/p&gt;
&lt;p&gt;$$Var\left[a^{i}\right]=\alpha{{}^2}n_i\sigma_{i-1}^{2}\left(Var\left(a^{i-1}\right)+\beta{{}^2}I_{n_i}\right)$$&lt;/p&gt;
&lt;p&gt;Likewise, the gradient variance, comprised during the backwards pass, is defined as:&lt;/p&gt;
&lt;p&gt;$$Var\left(\frac{\partial cost}{\partial a^{i-1}}\right)=\alpha{{}^2}n_i\sigma_{i-1}^{2}Var\left(\frac{\partial cost}{\partial a^{i}}\right)$$&lt;/p&gt;
&lt;p&gt;According to &lt;a href=&#34;#References&#34;&gt;Glorot and Bengio (2010)&lt;/a&gt;, preserving the variance throuhgout layers and iterations is an indication that the information is flowing without loss. Furthermore, it is evident that as the variance increase, the more the activation values resort to the function&#39;s extremes, resulting in saturation. Thus, ideally:&lt;/p&gt;
&lt;p&gt;$$Var\left(y^{i}\right)=Var\left(y^{i-1}\right)\text{ and }Var\left(\frac{\partial cost}{\partial y^{i}}\right)=Var\left(\frac{\partial cost}{\partial y^{l-1}}\right)$$&lt;/p&gt;
&lt;p&gt;assuming $n_{i}\sigma_{i-1}^{2}\approxeq1$ and $n_{i-1}\sigma_{i-1}^{2}\approxeq 1$. So, to satisfy this condition $\alpha$ and $\beta$ must be:&lt;/p&gt;
&lt;p&gt;$$\alpha=1\text{ and }\beta=0$$&lt;/p&gt;
&lt;p&gt;Considering the Taylor expansions of different activation functions:&lt;/p&gt;
&lt;p&gt;$$sigmoid(x)=\frac{1}{2}+\frac{x}{4}-\frac{x^{3}}{48}+O\left(x^{5}\right)$$&lt;/p&gt;
&lt;p&gt;$$tanh(x)=0+x+\frac{x^{3}}{3}+O\left(x^{5}\right)$$&lt;/p&gt;
&lt;p&gt;This approximation indicates, that in the linear regime, the tanh function satisfies the condition $\alpha=1$ while the sigmoid posesses a constant term that may explain the increase in variance throughout the feedfoward propagation. Additionally, due to the significantly small slope in comparison with the tanh, the sigmoid function requires a weight initialization 16 times greater ($\alpha^{2}$) than the tanh to maintain the gradient variance (&lt;a href=&#34;#References&#34;&gt;Xu et al. (2016)&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;mention Xu et. al and also Tishby variance&lt;/p&gt;
&lt;h1&gt;&lt;a name=&#34;References&#34;&gt;&lt;/a&gt;References&lt;/h1&gt;
&lt;ul style=&#34;font-size:0.8em;&#34;&gt;
    &lt;li&gt;Bradley, D. M. (2010). Learning In Modular Systems. PhD Thesis, Carnegie Mellon University.&lt;/li&gt;
    &lt;li&gt;Erhan, D., Manzagol, P.A., Bengio, Y., Bengio, S., and Vincent, P. (2009). The Difficulty of Training Deep Architectures and the Effect of Unsupervised Pre-Training. Artificial Intelligence and Statistics, 153-160.&lt;/li&gt;
    &lt;li&gt;Glorot, X. and Bengio, Y. (2010). Understanding the diffculty of training deep feedforward neural networks. Proceedings of the Thirteenth International Conference on Artificial Intelligence and
    Statistics , pages 249-256.&lt;/li&gt;
    &lt;li&gt;Goodfellow, I., Bengio, Y., and Courville, A. (2016). Deep Learning. MIT Press.&lt;/li&gt;
    &lt;li&gt;LeCun, Y. A., Bottou, L., Orr, G. B., and Müller, K.-R. (1998). Efficient BackProp. Neural Networks: Tricks of the Trade, 9-48.&lt;/li&gt;
    &lt;li&gt;Shanmugamani, R. (2018). Deep Learning for Computer Vision.&lt;/li&gt;
    &lt;li&gt;Xu, B., Huang, R., and Li, M. (2016). Revise Saturated Activation Functions.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Why deep learning became relevant</title>
      <link>https://artur-deluca.github.io/post/introduction/</link>
      <pubDate>Thu, 09 May 2019 00:00:00 -0500</pubDate>
      <guid>https://artur-deluca.github.io/post/introduction/</guid>
      <description>&lt;p&gt;In the last decade, there&#39;s been a widespread interest in artificial intelligence, particularly machine learning, and even more specifically deep learning. Curiously, this topic has not only resurged within academia but rather it has also been commonly showcased on non-technical mediums.&lt;/p&gt;
&lt;p&gt;Why these topics became so popular now? Certainly, recent feats such as AlphaGo or Waymo may raise awareness and hint to potential implications in our social organization, especially in the labor sector. Still, but what underlying technical advances have enabled these achievements - and what makes deep learning a paradigm so relevant within the past few years?&lt;/p&gt;
&lt;p&gt;In the aforementioned fields, one common task is to reason upon prior knowledge to derive plausible conclusions, a task known as inference. On this matter, strategies within artificial intelligence like expert systems had shared some success by encapsulating an expert&#39;s rationale into knowledge bases. These databases were then used by an inference engine, typically using logical rules, to reason out conclusions.&lt;/p&gt;
&lt;p&gt;However, as tasks grow in complexity, performance becomes hindered by computational limitations of reaching a decision using an inference engine in a large database. Not just that, but the lack of satisfactory techniques for reasoning under uncertainty was also an important obstacle that hampered the further development of expert systems (&lt;a href=&#34;#References&#34;&gt;Heathfield (1999)&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;A different paradigm, brought forward by machine learning, is to derive the relationships needed for inference using observations. More importantly, these models must be able to generalize this behavior for unseen instances of the given problem. Oftentimes, these relationships within data can be quite convoluted and in recent decades, a class of learning algorithms came to attention by the ability to derive these mappings: local kernel methods.&lt;/p&gt;
&lt;p&gt;This family of methods is a fairly recent grouping of techniques that have been developed independently for several years. They are called so due to the transformation performed so-called kernel function onto the input data, and some examples of models are Support Vector Machines, Gaussian processes, among other techniques. A local kernel method can be defined as:&lt;/p&gt;
&lt;p&gt;$$f(x) = b + \sum^n_{i=1} \alpha_iK_D\left(x, x_i\right)$$&lt;/p&gt;
&lt;p&gt;where $b$ is the bias term, $D$ is the set of observations of cardinality $n$, $α_i$ is scalar chosen by the learning algorithm upon $D$, $x_i$&#39;s are the training input observations and $K_D$ is the kernel function. These methods are more effective than linear models by being able to generate more favorable representations of feature space by using kernels (usually non-linear), achieving fairly positive results in several applications.&lt;/p&gt;
&lt;div&gt;
    &lt;img src=&#34;./figures/dataset.svg&#34; width=&#34;50%&#34; align=&#34;left&#34; style=&#34;margin-bottom: 1.5rem;&#34;/&gt;
    &lt;img src=&#34;./figures/k_dataset.svg&#34; width=&#34;50%&#34; align=&#34;right&#34; style=&#34;margin-bottom: 1.5rem;&#34;/&gt;
    &lt;p style=&#34;font-size:0.8em;&#34; align=&#34;center&#34;&gt;Transformation of features to generate a linearly-separable decision space. On the left the original dataset with two classes. On the right a transformation using PCA with a Gaussian kernel.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;One typical kernel function is the Gaussian kernel:&lt;/p&gt;
&lt;p&gt;$$K_\sigma(u, v) = e^{-\frac{\left\Vert u-v \right\Vert^2}{2\sigma^{2}}}$$&lt;/p&gt;
&lt;p&gt;where $\sigma^2$ is the equivalent of the variance in a gaussian probability density function. Note that the function is symmetric and as $u$ and $v$ become very distant from each other, the function approaches zero. This means that for the estimation of a query point $u$, the neighboring values of $u$ have much more influence than those further away from it.&lt;/p&gt;
&lt;p&gt;As postulated by David Wolpert in his &lt;a href=&#34;#References&#34;&gt;&lt;em&gt;no free lunch theorem&lt;/em&gt;&lt;/a&gt;, no model performs better than random guess without any kind of assumption. Local kernel methods are no exception: they implicitly or explicitly partition the input space, obtaining good generalization for a new query point by exploiting the training examples in its neighborhood. This comes at the price of the so-called &lt;em&gt;smoothness prior&lt;/em&gt;, meaning that these models expect a target function with little variation of results among neighboring observations.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;We may insert additional priors through the choice of the kernel as well as the feature representation. This is why for these types of models it might be beneficial to generate alternative representations of the input data. Rather than using their raw representation, these modifications could more closely conform to the model smoothness prior, yielding better results. Although not targeted at handcrafting representations, &lt;a href=&#34;#References&#34;&gt;Bengio et al. (2014)&lt;/a&gt; provides a thorough analysis of good characteristics for representations of features.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;However, for particular types of problems the smoothness assumption may not hold, posing a predicament to this family of methods. As an illustration, consider the task of recognizing a character from a $\texttt{20×20}$ pixel grid. If we consider the number of variations of a simple horizontal translation, we notice that a particular observation of the letter $O$ has much more similarity with the letter $U$ in the exact position and orientation that the same letter $O$ shifted horizontally, for instance.&lt;/p&gt;
&lt;img src=&#34;./figures/letters.svg&#34; width=&#34;80%&#34;/&gt;
&lt;p style=&#34;font-size:0.8em;&#34; align=&#34;center&#34;&gt;Demonstrations of the amount of variation among observations inter and intra-class.&lt;/p&gt;
&lt;p&gt;This indicates that the target function may not be as smooth as expected. In this highly non-linear function, local kernel methods require a very large number of training examples to cover all the desired variations within a class. Hence, the number of necessary templates can grow exponentially with the intrinsic dimension of a class manifold (&lt;a href=&#34;#References&#34;&gt;Bengio and Lecun (2007)&lt;/a&gt;).&lt;/p&gt;
&lt;img src=&#34;./figures/manifold.svg&#34; width=&#34;80%&#34;/&gt;
&lt;p style=&#34;font-size:0.8em;&#34; align=&#34;center&#34;&gt;The set of observations associated with the same class forms a manifold or a set of disjoint
manifolds, i.e. regions of lower dimension than the original space of images. When this manifold is smooth, it can be approximated locally by linear patches, tangent to the manifold. However, as the manifold becomes irregular and highly dimensional, patches become smaller and exponentially many patches, as well as data, are required to obtain a good generalization. Source: &lt;a href=&#34;#References&#34;&gt;Bengio (2009)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Note that this does not confront the ability of local kernel methods to approximate functions, rather their efficiency on approximating a function in regards to data. By contrast, neural network models are capable of dealing with intra-class variations more effortlessly. While each kernel function is activated in a small area of the input space, feature detectors of neural networks are capable of detecting certain patterns (edges or curvatures) almost independently from its orientation. The combination of these feature detectors is what makes neural network representations so efficient.&lt;/p&gt;
&lt;p&gt;As an illustration, consider the task of representing a number from 0-8. The representation generated by local methods can be seen as a one-hot encoding, signaling that each representation is mutually exclusive, for instance, $\texttt{00000001}$ signals $1$ and $\texttt{00000010}$ signals $2$ but the representation $\texttt{00000011}$ does not convey any valid information. Conversely, a representation of a neural network can be seen as a binary representation, much more efficient: $\texttt{001}$ is equal to $1$, $\texttt{010}$ to 2 and $\texttt{011}$ to $3$.&lt;/p&gt;
&lt;p&gt;In terms of classification, local methods produce mutually exclusive partitions onto the feature space which it oftentimes makes it difficult to generalize for unseen instances of the problem, if the target function is not smooth. On the other hand, neural networks generate several partitions which are compositional, presenting a more efficient decision surface that can be more robust to unseen query points.&lt;/p&gt;
&lt;img src=&#34;./figures/partitions.svg&#34; width=&#34;90%&#34;/&gt;
&lt;p style=&#34;font-size:0.8em;&#34; align=&#34;center&#34;&gt;Sketch of a partition of the feature space produced by local methods (&lt;i&gt;left&lt;/i&gt;) and neural networks (&lt;i&gt;right&lt;/i&gt;). Note that the sub-partitions of the right figure can be combined to infer unseen instances of the problem, represented by the symbol &lt;b&gt;$?$&lt;/b&gt;. These sub-partitions can be seen as potential explanatory factors. As an illustration, these can be the color, shape, and texture in the task of detecting particular types of fruits. Conversely, the partitions on the left symbolize specific combinations of these factors that can indicate a certain fruit. Source: &lt;a href=&#34;#References&#34;&gt;Bengio (2009)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The set of tasks illustrated so far are relatively simple in comparison to those that we see today, as in multi-class object detection. In previous decades, these simpler tasks were carried out relying on a shallow neural network architecture, i.e. using only a small number of layers. However, as the complexity of applications increase, this approach tends to fail.&lt;/p&gt;
&lt;p&gt;Complex image classification tasks, for instance, pose a challenge to these methods namely because it becomes overwhelmingly difficult to combine a larger number of low-level features (pixels) to determine a contrasting abstract outcome (object). Alternatively, deep learning is able to better accomplish this task (&lt;a href=&#34;#References&#34;&gt;Bradley (2010, pg. 22)&lt;/a&gt;), as experimental outlines in &lt;a href=&#34;#References&#34;&gt;He et al. (2015)&lt;/a&gt; and &lt;a href=&#34;(#References)&#34;&gt;Krizhevsky et al. (2012)&lt;/a&gt; indicate breakthrough performances upon increase in network depth.&lt;/p&gt;
&lt;img src=&#34;./figures/imagenet.svg&#34; width=&#34;80%&#34;/&gt;
&lt;p style=&#34;font-size:0.8em;&#34; align=&#34;center&#34;&gt;Results of the 2012 ImageNet classification competition. Only the first out of 5 top models was a deep neural network, which achieved unprecedented performance. Source: &lt;a href=&#34;http://image-net.org/challenges/LSVRC/2012/results.html&#34;&gt;ImageNet&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Hierarchical learning, deep structured learning, or simply deep learning can be defined as &lt;em&gt;a class of machine learning techniques that exploit many layers of non-linear units&lt;/em&gt; (&lt;a href=&#34;#References&#34;&gt;Deng and Yu (2013, pg. 199)&lt;/a&gt;). This better capability is attained by the substantial coupling of non-linear operations, deriving complex feature hierarchies from low-level inputs.&lt;/p&gt;
&lt;img src=&#34;./figures/deep_learning_pg6.png&#34; width=&#34;60%&#34;/&gt;
&lt;p style=&#34;font-size:0.8em;&#34; align=&#34;center&#34;&gt;Compositional representation of features throughout layers: from pixels to gradients and edges. &lt;br&gt;Source: &lt;a href=&#34;#References&#34;&gt;Goodfellow et al. (2016, pg. 6)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;As illustrated, different representations are synthesized along the layers and as representations become more related to the task in hand, the better the model can perform. However, how many layers are necessary to well approximate a function? In reality, the absolute number of layers is not the important factor, rather how many of these are necessary to effectively represent the target function, denominated as the &lt;em&gt;compact representation&lt;/em&gt;. Moreover, if a configuration is smaller than the compact representation, that can generate a negative effect on performance.&lt;/p&gt;
&lt;p&gt;It turns out that a configuration with one layer less than the compact representation may need an exponential number of neurons to achieve the same performance. This can be illustrated by the awarded work of &lt;a href=&#34;(#References)&#34;&gt;Håstad (1986)&lt;/a&gt;. By analogy, if we consider neural networks as simple logic circuits (which they are able to emulate), we can observe that a logic architecture limited in-depth presents an exponential number of components in comparison with a deep counterpart. Consider the calculation of the parity function, defined as:&lt;/p&gt;
&lt;p&gt;$$f:\{0,1\}^{n}\rightarrow\{0,1\},\thinspace f(x)=\left(\overset{|x]}{\underset{i=1}{\sum}}x_{i}\right) mod\space 2$$&lt;/p&gt;
&lt;p&gt;The number of logical components for a depth-limited architecture of 2 layers and $N$ inputs is of an order of $O(2^{N})$. On the other hand, unbounded architectures can produce less complex systems, such as the daisy-chain structure of complexity $O(N\thinspace log\thinspace N)$.&lt;/p&gt;
&lt;img src=&#34;./figures/logic_circuits.png&#34; width=&#34;80%&#34;/&gt;
&lt;p style=&#34;font-size:0.8em;&#34; align=&#34;center&#34;&gt;Distinct architectures to compute the parity function. A Disjunctive normal form structure (left) with a complexity of $2^{N-1}$ and a balanced tree structure (right) with 5 layers and complexity of $O(N\thinspace log\thinspace N)$.&lt;/p&gt;
&lt;p&gt;&lt;a name=&#34;back1&#34;&gt;&lt;/a&gt;Conversely, what could happen when freely adding more layers? In principle, besides the additional computational cost, this would not imply any counterpoints to approximating the target function, since the spare layers could simply replicate the second-to-last representation output. However, as in the shallow case, the model may carry too many parameters and insufficient training examples. This, in turn, would have statistical drawbacks: instead of approximating the desired mapping, the model starts to memorize the training data, causing overfitting, i.e. poor generalization&lt;sup&gt;&lt;a href=&#34;#1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a name=&#34;back2&#34;&gt;&lt;/a&gt;Despite the remarked capacity, deep architectures were not widely researched in the past as today. The reasons for this may encompass the former contemptuous view of researchers on this topic, or an insufficient computational power especially compared to contemporary standards. This, however, will not be covered in this series. The challenges of employing such architectures were also imposed by the difficulty in training neural networks with more than two layers, ultimately delivering poor generalization (&lt;a href=&#34;#References&#34;&gt;Bengio et al. (2007)&lt;/a&gt;)&lt;sup&gt;&lt;a href=&#34;#2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;In the next series of posts, I cover some of the improvements and predicaments that have followed the development of deep learning, starting from &lt;a href=&#34;../pretraining&#34;&gt;unsupervised pretraining&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a name=&#34;1&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p style=&#34;font-size:0.6em;&#34;&gt;
&lt;br&gt;
&lt;b&gt;1&lt;/b&gt; This is a generally accepted consequence of very deep models that do not have enough training observations. However, recent research by &lt;a href=&#34;https://windowsontheory.org/2019/12/05/deep-double-descent/&#34;&gt;Nakkiran et al.&lt;/a&gt; points to a completely different direction [&lt;a href=&#34;#back1&#34;&gt;return&lt;/a&gt;].
&lt;/p&gt;
&lt;p&gt;&lt;a name=&#34;2&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p style=&#34;font-size:0.6em;&#34;&gt;
&lt;b&gt;2&lt;/b&gt; &lt;a href=&#34;#References&#34;&gt;Bengio (2009, pg. 24)&lt;/a&gt; appropriately points out that Convolutional Neural Networks with up to seven layers did not suffer as other types of models. This may be due to what the author denominates as a Topographic Structure, suggesting that each neuron in a layer is associated with a small portion of the input. From this, two favorable aspects are hypothesized: the gradient is propagated less diffusely and this hierarchical local connectivity structure may be more suited for image recognition tasks, which is the most common use for such architecture [&lt;a href=&#34;#back2&#34;&gt;return&lt;/a&gt;].
&lt;/p&gt;
&lt;h2&gt;&lt;a name=&#34;References&#34;&gt;&lt;/a&gt;References&lt;/h2&gt;
&lt;ul style=&#34;font-size:0.6em;&#34;&gt;
    &lt;li&gt;Heathfield, H. (1999). The rise and “fall” of expert systems in medicine. Expert Systems. 183–188.&lt;/li&gt;
    &lt;li&gt;Wolpert, D. (1996), The Lack of A Priori Distinctions between Learning Algorithms. Neural Computation, 1341-1390.&lt;/li&gt;
    &lt;li&gt;Bengio, Y., Courville, A. and Vincent, P. (2014). Representation Learning: A Review and New Perspectives&lt;/li&gt;
    &lt;li&gt;Bengio, Y. and LeCun, Y. (2007). Scaling Learning Algorithms towards AI. 41.&lt;/li&gt;
    &lt;li&gt;Bengio, Y. (2009). Learning Deep Architectures for AI. Foundations and Trends in Machine Learning. 1-127.&lt;/li&gt;
    &lt;li&gt;Bengio, Y., Lamblin, P., Popovici, D., and Larochelle, H. (2007). Greedy Layer-Wise Training of Deep Networks. In Advances in Neural Information Processing Systems 19, 153-160.&lt;/li&gt;
    &lt;li&gt;Bradley, D. M. (2010). Learning In Modular Systems. PhD Thesis, Carnegie Mellon University.&lt;/li&gt;
    &lt;li&gt;Olah, C. (2014). Neural Networks, Manifolds, and Topology. Colah&#39;s blog&lt;/li&gt;
    &lt;li&gt;Deng, L. and Yu, D. (2013). Deep Learning: Methods and Applications.
    Foundations and Trends in Signal Processing, 7-197.&lt;/li&gt;
    &lt;li&gt;Krizhevsky, A., Sutskever, I., and Hinton, G. E. (2012). ImageNet Classification with Deep Convolutional Neural Networks. Advances in Neural Information Processing Systems 25, 1097-1105.&lt;/li&gt;
    &lt;li&gt;He, K., Zhang, X., Ren, S., and Sun, J. (2015). Deep Residual Learning for Image Recognition. Proceedings of the IEEE conference on computer vision and pattern recognition.&lt;/li&gt;
    &lt;li&gt;Goodfellow, I., Bengio, Y., and Courville, A. (2016). Deep Learning. MIT Press.&lt;/li&gt;
&lt;/ul&gt;
&lt;style&gt;
    h1 {
        text-align: left;
    }

    body {
        text-align: justify;
    }

&lt;/style&gt;
</description>
    </item>
    
  </channel>
</rss>
