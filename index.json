[{"authors":["admin"],"categories":null,"content":"arturbackdeluca [at] gmail [dot] com\nI am a master's student in Artificial Intelligence and Robotics at the University of Rome. I am currently looking for research programs involving questions of statistical learning and optimization within (but not restricted to) deep learning. Please feel free to reach me via any of the digital mediums here available. I currently live in Rome, but I'm originally from Brazil. My favorite activities are listening to unfamiliar genres of music and running, although usually not at the same time ðŸ˜ƒ.\n ","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"arturbackdeluca [at] gmail [dot] com\nI am a master's student in Artificial Intelligence and Robotics at the University of Rome. I am currently looking for research programs involving questions of statistical learning and optimization within (but not restricted to) deep learning. Please feel free to reach me via any of the digital mediums here available. I currently live in Rome, but I'm originally from Brazil. My favorite activities are listening to unfamiliar genres of music and running, although usually not at the same time ðŸ˜ƒ.","tags":null,"title":"Artur De Luca","type":"authors"},{"authors":null,"categories":null,"content":"In an attempt to understand any sort of phenomenon, when faced with many potential factors, we tend to stick to those indicating of having a closer relationship with the outcome. If you, unfortunately, have a case of the runs, a doctor might first check if you had anything odd to eat, presented any other symptoms or even ask for the dreaded stool test instead of requesting an invasive biopsy right of the bat. With this approach, we may be implicitly exercising the pertinent clichÃ© popularized by Willian of Occam\u0026rsquo; (aka the principle of parsimony)\n \u0026ldquo;Entities should not be multiplied without necessity.\u0026quot;\n \u0026ldquo;When presented with competing hypotheses that make the same predictions, one should select the solution with the fewest assumptions\u0026rdquo;. Going back to the diagnosis, a doctor probably would initially relate the case with a direct root cause, such as the shady meals you've been eating rather than a claim of colon cancer, since there may not be enough evidence to suggest it. (which can be tested much easier https://plato.stanford.edu/entries/simplicity/)\n Consider leaving a Star if this helps you.\n Contemporary philosophers have tended to reinterpret OR as a principle of theory choice: OR implies thatâ€”other things being equalâ€”it is rational to prefer theories which commit us to smaller ontologies. This suggests the following paraphrase of OR:\n(OR1) Other things being equal, if T1 is more ontologically parsimonious than T2 then it is rational to prefer T1 to T2. What does it mean to say that one theory is more ontologically parsimonious than another? The basic notion of ontological parsimony is quite straightforward, and is standardly cashed out in terms of Quine's concept of ontological commitment. A theory, T, is ontologically committed to Fs if and only if T entails that F's exist (Quine 1981, pp. 144â€“4). If two theories, T1 and T2, have the same ontological commitments except that T2 is ontologically committed to Fs and T1 is not, then T1 is more parsimonious than T2. More generally, a sufficient condition for T1 being more parsimonious than T2 is for the ontological commitments of T1 to be a proper subset of those of T2. Note that OR1 is considerably weaker than the informal version of Occam's Razor, OR, with which we started. OR stipulates only that entities should not be multiplied beyond necessity. OR1, by contrast, states that entities should not be multiplied other things being equal, and this is compatible with parsimony being a comparatively weak theoretical virtue.\nSince Occam's Razor ought to be invoked only when several hypotheses explain the same set of facts equally well, in practice its domain will be very limitedâ€¦[C]ases where competing hypotheses explain a phenomenon equally well are comparatively rare (Holsinger 1980, pp. 144â€“5).\nRead: Justifications of Simplicity https://plato.stanford.edu/entries/simplicity/ (https://en.wikipedia.org/wiki/Minimum_description_length)\n  \u0026nbsp;\u0026nbsp;What is a feature  \nIn machine learning and pattern recognition, a **feature** (or an attribute) is an individual measurable property or characteristic of a phenomenon being observed.[1]  So what if instead of diagnosing one patient, we took a step further and focus to find the factors linked to a disease, let's say colon cancer. By constraining the problem to investigate only genetic traits this can even seem easier, since we are dealing with a bounded set of possibilities. However, in this case the number of equally potential factors is such that impedes us humans to understand the big picture. A dataset mapping to colon cancer presents more than 2.000 characteristics, or features [[4]](#References). And many other aplications in data analysis generally present equal or greater amounts of information. 2009: ImageNet - 14 million images with 256x256 pixels (+196k features) each, and more than 20,000 categories [2]\n2010: The Wikipedia Corpus - almost 1.9 billion words from more than 4 million articles [3]\n2011: Cancer detection based on gene expression (e.g.: Colon dataset - 2,000 features) [4]\nImageNet: 256x256x3 = 196608\nWikipedia: 1.9 billion words, not distinct, of course - the Second Edition of the 20-volume Oxford English Dictionary, published in 1989, contains full entries for 171,476 words in current use, and 47,156 obsolete words.\nThe tendency for \u0026ldquo;wider\u0026rdquo; amounts of information has increase over the years, probably related to our increase in storage and processing capabilties. We can observe this trend in a UBC dataset survey. Before the 2000s, very few domains operated with more than 40 features and soon after we have more than XX.\n Source: [5]\n2. The problem 2. The problem\nToo many features, too many problems What problems arise with too many features?\n   require longer training times*\n  jeopardize human interpretability*\n  worsen prediction quality due to sample size effects\n  potentially increase overfitting\n  * Considered self-explainatory, will not be further explained\n3. Worsen prediction quality due to sample size effects Couldn't a predictor simply disregard irrelevant features?\n To answer this, we will have to resort to some statistical learning theory, exploring the ways of estimating functional dependency from a given collection of data.\nStatistical Learning Theory Let $$X \\in \\mathbb R^p$$ be a input random vector and $Y\\in \\mathbb R$ be an output random variable, with joint distribution $P(X,Y)$.\nThe task of learning aims at finding a function $f(X)$ for predicting $Y$ given values of input $X$. This structure requires a loss function $L(Y, f(X))$ for identifying and penalizing errors in prediction. With this structure, we can define a criterion for choosing a suitable $f$ known as the Statistical Risk ($R$).\n$$ \\begin{equation} \\begin{split} \\text{R}(f) \u0026amp; = \\mathbb{E} \\thinspace L(Y, f(X)) \\\n\u0026amp; = \\int L(y, f(x))\\thinspace P(dx,dy) \\\n\u0026amp; = \\mathbb{E}{X}\\mathbb{E}{Y|X} [L(Y, f(X))|X] \\end{split} \\end{equation} $$\nSource: [5]\nThis criterion tells us how well, on average, the predictor $f$ performs with respect to the chosen loss function\nInstead of working with the joint distribution, we can condition the Statistical Risk on $X$\n$$ \\begin{equation} \\begin{split} \\text{R}(f) \u0026amp; = \\int\\int [L(y, f(x))|x]\\thinspace P(dy)\\thinspace P(dx) \\\n\u0026amp; = \\mathbb{E}{X}\\mathbb{E}{Y|X} [L(Y, f(X))|X] \\end{split} \\end{equation} $$\nFinally, what we seek is a $f(X)$ which minimizes the the Risk: $$f(X){opt}= argmin_c \\mathbb{E}{X}\\mathbb{E}_{Y|X}[L(Y,c)|X]$$\nThe optimal solution will be different depending on the metric used\nRegression Mean Squared Error (MSE) $$ \\begin{equation} \\begin{split} f(X){opt} \u0026amp; = argmin_c \\mathbb{E}{X}\\mathbb{E}_{Y|X}[(Y-c)^2|X] \\\n\u0026amp; = \\mathbb{E}(Y|X=x) \\end{split} \\end{equation} $$\nknown as the conditional mean or expectation - the \u0026ldquo;average\u0026rdquo; value over an arbitrarily large number of occurrences\nMean Absolute Error (MAP) $$ \\begin{equation} \\begin{split} f(X){opt} \u0026amp; = argmin_c \\mathbb{E}{X}\\mathbb{E}_{Y|X}[|Y-c|\\thinspace|X] \\\n\u0026amp; = median(Y|X=x) \\end{split} \\end{equation} $$\nor the conditional median of the distribution\nRegression loss functions for SLT MSE: the most preferable option, due to its ease of computation of minimum, since it is differentiable. However it is more sensitive to outliers as a big difference becomes even larger by squaring them.\nMAP: its optimal estimates are more robust than those for the conditional mean. However, MAP has discontinuities in their derivatives, which have hindered their widespread use.\nClassification 0-1 loss $$ \\begin{equation} \\begin{split} f(X){opt} \u0026amp; = argmin_c \\mathbb{E}{X}\\mathbb{E}_{Y|X}[I(f(X),Y)] \\\n\u0026amp; = \\underset{y \\in Y}{\\max}P(y|X) \\end{split} \\end{equation} $$\nwhere ${\\displaystyle I}$ is the indicator function:\n$$I :=\\begin{cases} 0\\text{, if } f(X) = Y\\\n1\\text{, otherwise}\\\n\\end{cases}$$\nThe decider is also known as the Optimal Bayes classifier (revise this).\nIf the joint probability $P(X,Y)$ is known and the classification decision is optimal.\n This doesn't mean that there are no errors, rather than the lowest error achievable, resulted by noise among distributions. If the probability distribution for the problem was known, the classifier wouldn't be affected by adding more of features.\nIf such features carried the slightest contributions, it is shown that the Bayes optimal classifier tends to zero as the number of features approach infinity.\n However, the probability distribution used is an estimation, based on the finite set of samples, causing a peaking phenomenon: the prediction accuracy increases with the number of features, but soon reaches a peak, in which the noise becomes larger than the separability increase caused by the new feature.\nSource: [6]\nThis is often called the peaking phenomenon. Some ideas evolved around it are discussed in the great pattern recognition blog by Ela Pekalska and Bob Duin called 37steps:\n Peaking summarized Peaking paradox Trunkâ€™s example of the peaking phenomenond The curse of dimensionality   4. potentially increase overfitting As the number of features increase, the observations become more sparse within the feature space.\n    Having the observations further apart makes it difficult for the estimator to generalize, increasing its variance, i.e. relying on the specific observations to produce new predictions, causing overfitting.\n   Consequences for classical:\nNon-parametric (Local methods): methods such as the $k$ nearest neighboors, as the examples become increasingly sparse, the approximation of the conditional expectation by taking the average of its nearest neighboors becomes correspondingly worse as the query point distance itself from the known examples. Additionally, in high dimensions, as the datapoints become more spread apart, their distances become more uniform, making it dificult for the algorithm to decide which data points are more relevant to region of interest Source\nParametric methods: In very high dimensional spaces, there is more than one plane that can be fitted to your data, and without proper type of regularization can cause the model to behave very poorly. . Collinearity often results in overfitting, i.e. in a too efficient modelling of learning samples without model generalization ability. [7]\nMore examples on check the answer on stackexchange\nWhat causes these problems?\nThe curse of dimensionality\nThe curse of dimensionality     Increasing the number of factors to be taken into consideration requires an exponential growth of observations [5]\nHowever, having a small number of samples means that many regions of the feature space are never observed\n   References [1] : Bishop, C. (2006). Pattern recognition and machine learning\n[2] : Guyon, I. and Elissee, A. (2003). An Introduction to Variable and Feature Selection\n[3]: Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., C. Berg, A. and Fei-Fei, L. (2015) ImageNet Large Scale Visual Recognition Challenge. IJCV, 2015\n[4]:Kossinets, G. (2010). Processed Wikipedia Edit History. Stanford large network dataset collection\n[5]: Liu, Q., Sung, A. H., Chen, Z., Liu, J., Chen, L., Qiao, M., Deng, Y. (2011). Gene selection and classification for cancer microarray data based on machine learning and similarity measures.\n[6]: Hastie, T., Tibshirani, R., and Friedman, J. H. (2009). The elements of Statistical learning: data mining, inference, and prediction.\n[7]: Trunk, G. V. (1979). A Problem of Dimensionality: A Simple Example\n[8]: Verleysen, M. and FranÃ§ois, D. (2005). The Curse of Dimensionality in Data Mining and Time Series Prediction\n[9]: Ng, A. Y. and Jordan, M. I. (2001). On Discriminative vs. Generative Classifiers: A comparison of logistic regression and naive Bayes\n[10]: Li, J., Cheng, K., Wang, S., Morstatter, F., Trevino, R. P., Tang, J., and Liu, H. (2017). Feature Selection: A Data Perspective\n[11]: Dash, M. and Liu, H. (1997). Feature selection for classification\n[12]: Kohavi, R. (1994). Feature Subset Selection as Search with Probabilistic Estimates\n[13]: Tang, J., Alelyani, S., Liu, H. (2014). Feature selection for classification: A review\n[14]: Amaldi,E., Kann, V. (1998). On the approximability of minimizing nonzero variables or unsatisfied relations in linear systems\n[15]:Chandrashekar, G., Sahin, F. (2014). A survey on feature selection methods\n[16]: Narendra, P. M., Fukunaga, K. (1977). A Branch and Bound Algorithm for Feature Subset Selection\n[17]: Rayward-Smith, V., Osman, I., Reeves, C. D., Smith, G. (1996). Modern Heuristic Search Methods\n[18]: JoviÄ‡, A., BrkiÄ‡, K., and BogunoviÄ‡, N. (2015). A review of feature selection methods with applications\n","date":1559019600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559019600,"objectID":"f4bac1c5d34a403accf34441cc2d577a","permalink":"/work/feature_selection/","publishdate":"2019-05-28T00:00:00-05:00","relpermalink":"/work/feature_selection/","section":"work","summary":"In an attempt to understand any sort of phenomenon, when faced with many potential factors, we tend to stick to those indicating of having a closer relationship with the outcome. If you, unfortunately, have a case of the runs, a doctor might first check if you had anything odd to eat, presented any other symptoms or even ask for the dreaded stool test instead of requesting an invasive biopsy right of the bat.","tags":null,"title":"Dimensionality: The more the better?","type":"work"},{"authors":null,"categories":null,"content":" h1 { text-align: left; } body { text-align: justify; }  The role of a machine learning system is to approximate a mapping between inputs and outputs using data and optimally generalize this behavior for unseen instances of the same problem.\nOn this matter, several distinct strategies within artificial intelligence once had their share of success. In the past these were mainly supported by expert knowledge - either by embedding rigid rules into models or by manually combining input variables and designing more abstract features to mitigate limitations of the settings then used. At the same time, neural network models were able to determine the target function solely based on the input data. The traditional setting of this model relied on a shallow structure, i.e. a small number of layers, and its achievements were subordinate to a simple set of applications.\n Combination of features to generate a linearly-separable decision space. On the left a two-layer network successfully separating the two classes. On the right a similar setting in which the same network fails.\nSource: Olah, C. (2016)\n As the complexity of applications increase the mentioned approaches tend to fail. A task such as image classification poses a challenge to these methods namely because it is overwhelmingly difficult to combine a large number of low-level features (pixels) to determine a contrasting abstract outcome (object). Alternatively, deep learning is able to better accomplish this task (Bradley (2010, pg. 22)).\nHierarchical learning, deep structured learning, or simply deep learning can be defined as a class of machine learning techniques that exploit many layers of non-linear units (Deng and Yu (2013, pg. 199)). This better capability is attained by the substantial coupling of non-linear operations whose parameters readapt, gradually approximating the target function thus deriving complex feature hierarchies from low-level inputs.\nCompositional representation of features throughout layers: from pixels to gradients and edges. Source: [Goodfellow et al. (2016, pg. 6)](#References)\nA neural network can progressively synthesize more structured features along the layers, indicating the number of layers as a crucial factor in model design, and as research outlines in He et al. (2015, pg. 2) and Krizhevsky et al. (2012, pg. 1) acknowledge performance improvement upon network depth increase.\nStill, how many layers are necessary to well approximate a function? It turns out that the absolute number of layers is not the important factor, rather how many of these are necessary to effectively represent the target function, denominated as the compact representation . Conversely, if a configuration is smaller than the compact representation, that can pose a predicament to the performance, as Bengio (2009, pg. 9) describes:\n More precisely, functions that can be compactly represented by a depth $k$ architecture might require an exponential number of computational elements to be represented by a depth $k-1$ architecture. Since the number of computational elements one can afford depends on the number of training examples available to tune or select them, the consequences are not just computational but also statistical: poor generalization may be expected when using an insufficiently deep architecture for representing some functions.\n So, ideally, a function requires a number of layers to be represented in an efficient manner. However, in theory, that does not imply any counterpoints to stacks of layers greater than required by the function, as the spare layers could simply replicate the last compact representation output. However, many challenges arise as the depth of networks increase.\n Consider leaving a \u0026nbspStar \u0026nbsp if this helps you.\n Despite the remarked capacity, deep architectures were not widely researched in the past as today. The reasons for this may encompass the former contemptuous view of researchers on this topic, or an insufficient computational power especially compared to contemporary standards. This, however, will not be covered in this study. The challenges of employing such architectures were also imposed by the difficulty in training neural networks with more than two layers, ultimately delivering poor generalization (Bengio et al. (2007))1.\n\n[1] [Bengio (2009, pg. 24)](#References) appropriately points out that Convolutional Neural Networks with up to seven layers did not suffer as other types of models. This may be due to what the author denominates as a Topographic Structure, suggesting that each neuron in a layer is associated with a small portion of the input. From this, two favorable aspects are hypothesized: the gradient is propagated less diffusely and this hierarchical local connectivity structure may be more suited for image recognition tasks, which is the most common use for such architecture. Problem: Layer saturation In the early development of neural networks, the initialization of parameters was performed by sampling a uniform distribution bounded to a predetermined interval. In this process, the weights and biases produced solutions that would generally lie far from an optimum set of parameters, requiring many training iterations to readjust. Shallow networks did not suffer too much from this due to the smaller number of parameters, hence presenting an easier system to optimize. However, as the depth of networks increased, so did the difficulty to train such models using this initialization procedure.\nGlorot and Bengio (2010) promoted a study to understand why random initialization performed so poorly in deep networks. In this investigation, the authors considered one common initialization heuristic, introduced by LeCun et al. (1998), that defines the biases at 0 and the weights via sampling according to the following distribution:\n$$W_{ij}\\sim U\\left[-\\frac{1}{\\sqrt{n}},\\frac{1}{\\sqrt{n}}\\right]$$\nwhere $n$ is the number of inputs to the unit. The authors then verified that in deep configurations of 4 to 5 layers, activation values on the last layers got stuck in plateaus situated at the extremes of the activation function, case otherwise known as saturation.\nSaturation regions on the sigmoid function. In the case observed by [Glorot and Bengio (2010)](#References), the saturation occurred in the 5th and last layer of the network, with activation values converging to zero.\nOne hypothesis that explains saturation on sigmoid-equipped deep networks is that the random initialization does not provide useful information to the last layer of the network, that starts to rely more on its biases, which, in turn, are trained faster than its weights.\nMean (lines) and standard deviation (vertical bars) of sigmoid activation values across layers in a neural network using random initialization. The saturation is detectable in the last layer, where the activation values reach virtually zero. Source: [Glorot and Bengio (2010)](#References)\nGradually but rapidly, the error gradient tends to push the activations towards zero in an attempt to suppress the influence of the previous layers. Eventually, the saturation may be overcome but the overall result would be of poor generalization.\nSolution: Unsupervised pre-training Layer saturation was the biggest technical hurdle that limited the progress of deep learning in the dawn of the millennium. In 2006, however, Hinton et al. (2006) inspired by a well-established procedure, developed a novel approach to initialize the parameters of a Deep Belief Network â€” a class of neural networks â€” what in time overcame the saturation issue and surpassed performance ever seen using deep architectures. These results not only re-sparked but drastically expanded researchers\u0026rsquo; interest in this field.\nA Deep Belief Network (DBN) can be seen as a stack of smaller unsupervised learning algorithms named Restricted Boltzmann Machines. This configuration can then be bundled with a classical multi-layer perceptron for supervised learning tasks\nThis initialization procedure encompassed an unprecedented process: an unsupervised greedy layer-wise pre-training step.2 Preceding the conventional supervised training, each layer is trained with its anterior neighboring layer identically to a Restricted Boltzmann Machine. This process starts with the input and first layer, and progressively advances one layer at a time until it sweeps all layers.\nA Boltzmann Machine is an unsupervised generative algorithm that learns the data representation by associating the patterns identified in the inputs to probabilistic configurations within its parameters. A Restricted Boltzmann Machine is a variation of such a model that reproduces a similar behavior but with significantly fewer connections.\n[2] Despite the imprecision, unsupervised pre-training is here used interchangeably A Boltzmann Machine (left) and a Restricted Boltzmann Machine (right)\nMany other unsupervised pre-training algorithms were developed concomitantly or immediately after, such as autoencoders (Bengio et al. (2007)), denoising autoencoders (Vincent et al. (2008)), contractive autoencoders (Rifai et al. (2011)), among others.\nComparison of performance between networks running without pre-training (left) and with pre-training (right) Source: [Erhan et al. (2010, pg. 636)](#References)\nWhy does this unsupervised learning methods help training deep architectures? Much of the explanation remains uncertain. Nonetheless, Erhan et al. (2010) provide some clarifications through considerable experimentation. The claims of the authors reside on two possible, but not mutually exclusive reasons: optimization and regularization.\nDeep neural networks are composed of many parameters whose values are used to compute an approximation of a function. Due to its substantial nonlinear nature, this approximation yields a non-convex function that poses a challenge on searching the best combination of weights and biases.3\nA convex (left) and non-convex (right) function. Note that, contrarily to the convex function, the non-convex function possesses multiple local optima. Source: [Zadeh (2016)](#References)\n[3] Many discussions today in academia evolve around the particular shape of loss landscape in deep neural networks, since many of the local minima appear to have equally good qualities, suggesting that the critical solutions reached during training are actually saddle points. This discussion will be reserved for further studies. Gradient-based methods employed in training eventually converge to their pre-selected basin of attraction, a region of the function space, such that any point in it eventually is iterated into the attractor. Unsupervised pre-training may work towards optimization by favoring a basin of attraction that might yield a lower training error. Thus, since the gradients are very prompt to abrupt changes, backpropagation is only used at a local search level, from an already favorable starting point (Hinton (2012, lecture 14b)).\nAs for regularization, one may commonly associate it with explicit techniques, such as the L1 or L2 norm:\n$$C=-\\frac{1}{n}\\sum_{j}\\left[y_j\\ln a_j^{L}+\\left(1-y_j\\right)\\ln\\left(1-a_j^{L}\\right)\\right]+\\frac{\\lambda}{2n}\\sum_iw_i{{}^2}$$\nBy adding the L2 regularization factor in the cross-entropy cost function, presented in the equation above, one can penalize overly complex models, that would result in poor generalization, i.e. high testing error. However, the regularization employed by pre-training is implicit. In attempt to model how such technique would work explicitly, Erhan et al. (2009) defines a regularization term such as:\n$$regularizer=-log,P\\left(\\theta\\right)$$\nThe function $P\\left(\\theta\\right)$ describes the probability that the weights of the neural network are initialized as $\\theta$. So, if a configuration shows to be highly improbable, the regularizer term will hurt the cost function strongly. Furthermore, if the probability of landing such set of parameters is high, the regularizer will then reward the cost function. This probability is governed by the initialization methods employed. Considering two instances with and without pre-training, we obtain:\n$$P_{{\\text{pre-training}}}(\\theta) = \\sum_k\\frac{1_{\\theta \\in R_k}\\pi_k}{v_k}$$\nand\n$$P_{{\\text{no pre-training}}}(\\theta) = \\sum_k\\frac{1_{\\theta \\in R_k}r_k}{v_k}$$\nwhere $R_k$ is the basin of attraction that contains $\\theta$, and $1\\_{\\theta \\in R_k}$ is the identifier function â€“ unitary for all $\\theta$ in $R_k$, otherwise null. Additionally, $\\pi_k$ and $r_k$ are the probabilities of landing in the basin of attraction $R_k$, which has a volume $v_k$. Since the basins of attraction are disjunct sets, the probability density function of the set of parameters located in $R_k$ is uniform, calculated by taking the probability of landing in the k-th basin ($\\pi_k$ or $r_k$) and dividing by its volume. Pre-training the parameters of the network conditions the network initialization to land on regions of better generalization. This is hypothetically achieved by increasing the $\\pi_k$'s where the network parameters represent meaningful variations of the input, contributing to predict the output. For this reason, pre-training also reduces the variance upon parameter initialization.\n2-D visualization of parameters' trajectory of 100 neural networks with and without the unsupervised pre-training step. The color gradient from dark-blue to cyan symbolizes the progression of iterations. Source: [Erhan et al. (2010, pg. 541)](#References)\nThe visualization of the parameters\u0026rsquo; trajectory may demonstrate the effects of optimization and regularization. As mentioned through the former, it may select a basin of attraction with lower training errors. Conversely, regularization may bound the parameter interval to a range that yields good generalization. Also, it is crucial to notice that both training and testing errors collected in the experiments of Erhan et al. (2010) support these hypotheses, but do favor the latter.\nFurthermore, once established within the solution space, the parameters do not drastically change during the gradient-based adjustment process. This process is also denominated fine-tuning, as it only modifies the features slightly to get the category boundaries, rather than discovering new relationships (Hinton (2012, lecture 14b)).\nVisualization of filters of a Deep Belief Network used to recognize digits form the MNIST data-set after the different training processes; from left to right: units from the first, second and third layers, respectively. Source: [Erhan et al. (2010, pg. 638-639)](#References)\nBut how can one conceptually understand the effects of unsupervised learning? Apart from the regularization and optimization hypothesis, the layer-wise pre-training resembles the underlying distribution of the input. Ideally, this representation, by combining the different features and mapping their inner relationships, can unveil, and more importantly, disentangle causal elements that influence the output. If those inputs can be transformed into uncorrelated features, it is possible to solve for a particular parameter disregarding its influence over the others.\nAs mentioned in Goodfellow et al. (2016, pg. 541), this hypothesis justifies approaches in which one first seeks a good representation for $p(x)$ before training with the output. If the output is closely related to factors captured by the input, an initialization that captures the distribution of x is useful for predicting the desired output distribution $p\\left(y|x\\right)$.\nHowever, despite the aforementioned advantages, unsupervised pre-training presents noteworthy drawbacks, such as establishing two separate learning stages (unsupervised and supervised). As a consequence, there is a long delay between adjusting hyperparameters on the first stage utilizing feedback from the second. Additionally, although pre-training being considered a valuable regularizer, its strength adjustment is troublesome, requiring a somewhat unclear modification of far too many hyperparameters â€” contrasting with explicit regularization techniques that can be adjusted by a single one.\nFor the reasons mentioned above, unsupervised pre-training is not so popularly used today, as other techniques discovered yielded the same benefits but much more efficiently. These will be explained in the following post, where these strategies will also tackle a different obstacle: Vanishing gradients.\nReferences Bengio, Y. (2009). Learning Deep Architectures for AI. Foundations and Trends in Machine Learning. 1-127. Bengio, Y., Lamblin, P., Popovici, D., and Larochelle, H. (2007). Greedy Layer-Wise Training of Deep Networks. In Advances in Neural Information Processing Systems 19, 153-160. Bradley, D. M. (2010). Learning In Modular Systems. PhD Thesis, Carnegie Mellon University. Olah, C. (2014). Neural Networks, Manifolds, and Topology. Colah's blog Deng, L. and Yu, D. (2013). Deep Learning: Methods and Applications. Foundations and Trends in Signal Processing, 7-197. Erhan, D., Bengio, Y., Courville, A., Manzagol, P.A., Vincent, P., and Bengio, S. (2010). Why Does Unsupervised Pre-training Help Deep Learning? Journal of Machine Learning Research, 11-36. Erhan, D., Manzagol, P.A., Bengio, Y., Bengio, S., and Vincent, P. (2009). The Difficulty of Training Deep Architectures and the Effect of Unsupervised Pre-Training. Artificial Intelligence and Statistics, 153-160. Glorot, X. and Bengio, Y. (2010). Understanding the diffculty of training deep feedforward neural networks. Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics , pages 249-256. Goodfellow, I., Bengio, Y., and Courville, A. (2016). Deep Learning. MIT Press. He, K., Zhang, X., Ren, S., and Sun, J. (2015). Deep Residual Learning for Image Recognition. Proceedings of the IEEE conference on computer vision and pattern recognition. Hinton, G. (2012). Neural Networks for Machine Learning. Coursera Online Course. Hinton, G. E., Osindero, S., and Teh, Y.-W. (2006). A Fast Learning Algorithm for Deep Belief Nets. Neural Computation , 1527-1554. Krizhevsky, A., Sutskever, I., and Hinton, G. E. (2012). ImageNet Classification with Deep Convolutional Neural Networks. Advances in Neural Information Processing Systems 25, 1097-1105. LeCun, Y. A., Bottou, L., Orr, G. B., and MÃ¼ller, K.-R. (1998). Efficient BackProp. Neural Networks: Tricks of the Trade, 9-48. Rifai, S., Vincent, P., Muller, X., Glorot, X., and Bengio, Y. (2011). Contractive Auto-encoders: Explicit Invariance During Feature Extraction. Proceedings of the 28th International Conference on Machine Learning, 833-840. Vincent, P., Larochelle, H., Bengio, Y., and Manzagol, P.-A. (2008). Extracting and Composing Robust Features with Denoising Autoencoders. Proceedings of the 25th International Conference, 1096-1103. Xu, B., Huang, R., and Li, M. (2016). Revise Saturated Activation Functions. Zadeh, R. (2016). The hard thing about deep learning. O'Reilly Media.  ","date":1557378000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557378000,"objectID":"59db1488beb21c91a498bf30b204663c","permalink":"/work/saturation/","publishdate":"2019-05-09T00:00:00-05:00","relpermalink":"/work/saturation/","section":"work","summary":"h1 { text-align: left; } body { text-align: justify; }  The role of a machine learning system is to approximate a mapping between inputs and outputs using data and optimally generalize this behavior for unseen instances of the same problem.\nOn this matter, several distinct strategies within artificial intelligence once had their share of success. In the past these were mainly supported by expert knowledge - either by embedding rigid rules into models or by manually combining input variables and designing more abstract features to mitigate limitations of the settings then used.","tags":null,"title":"Deep learning challenges: saturation","type":"work"},{"authors":null,"categories":null,"content":" body { text-align: justify}  The objective of this series is to illustrate some of the past and on-going challenges within deep learning. In the previous blog post, the obstacle discussed was the saturation within activation functions during training. This is a follow-up, so I recommend you take a brief look before continuing.\nProblem: Vanishing gradients As mentioned in the previous post, the activation values start to migrate towards the extremities of the sigmoid function in networks with several layers. But what happens to the gradients? Around the 1980's researchers at USC and CMU started to work on algorithms using the gradient of the loss function throughout the layers of the network as a way to adjust each weight with a corresponding \u0026ldquo;influence\u0026rdquo; over the error. These gradients were obtained analytically by applying the chain rule from the loss function to the desired weight. This is famously called the backpropagation technique, and has spurred the development of neural networks later on. (Cite hinton 1989)\nHowever, once again, as the stack of layers increase, obstacles arise to hamper performance, and this time this is caused by a phenomenon known as the Vanishing/Exploding Gradients. To illustrate it, consider a network with a general cost function and symmetric activation functions $f$ with unit derivative at 0. If we write $a_{i}$ for the activation vector of layer $i$, and $s_{i}$ the argument vector of the activation function at layer $i$, we have $s_{i}=a_{i}W_{i}+b_{i}$ and $a_{i}=f(s_{i})$.\nConsider the following node output and activation:\nConsider the following node output and activation:\n$$s^{i}=W^{i}a^{i-1}+b_{i}$$\n$$a_{i+1}=g\\left(W_{i}a_{i-1}+b_{i}\\right)$$\nThe derivative of the loss function $\\mathcal{L}$ in a network with $l$ layers in terms of a weight $W_{n}$ can be written as:\n$$\\frac{\\partial\\mathcal{L}}{\\partial W_{n}}=\\frac{\\partial s_{n}}{\\partial W_{n}}\\left[\\underset{i=n}{\\overset{l-1}{\\prod}}\\frac{\\partial a_{i+1}}{\\partial s_{i}}\\frac{\\partial s_{i+1}}{\\partial a_{i+1}}\\right]\\frac{\\partial a_{l}}{\\partial s_{l}}\\frac{\\partial\\mathcal{L}}{\\partial a_{l}}$$\nDefining a linear activation function $g(z_{i})=z_{i}$ and $W_{i}=W$, we obtain:\n$$\\require{cancel}\\frac{\\partial\\mathcal{L}}{\\partial W_{n}}=\\frac{\\partial s_{n}}{\\partial W_{n}}\\left[\\underset{i=n}{\\overset{l-1}{\\prod}}\\cancelto{1}{\\frac{\\partial a_{i+1}}{\\partial s_{i}}}\\cancelto{W}{\\frac{\\partial s_{i+1}}{\\partial a_{i+1}}}\\right]\\frac{\\partial a_{l}}{\\partial s_{l}}\\frac{\\partial\\mathcal{L}}{\\partial a_{l}}$$\nThus:\n$$\\frac{\\partial\\mathcal{L}}{\\partial W_{n}}=a_{n-1}W^{n-l}\\frac{\\partial\\mathcal{L}}{\\partial a_{l}}$$\nSo, in deep configurations, the backpropagation technique may become troublesome as a result of exploding or vanishing gradients. As the cost function is progressively derived in terms of numerically large parameters, these adjustments will likely overshoot, hampering training. Conversely, small contributions propagated through many layers may cause virtually no effect at all:\nMoreover, in deep configurations, the backpropagation technique may become troublesome as a result of exploding or vanishing gradients. As the cost function is progressively derived in terms of numerically large parameters, these adjustments will likely overshoot, hampering training. Conversely, small contributions propagated through many layers may cause virtually no effect at all:\nIf the elements in W greater than one, and with a sufficiently large n-l value, $\\frac{\\partial\\mathcal{L}}{\\partial W_{n}}$ will tend to infinity, i.e. exploding. Conversely, for values less than one, the derivative tends to zero, i.e. vanishing.\nFor the case of early neural network development, a typical initialization framework presented in Erhan et al. (2009) would have parameters randomly sampled from a uniform distribution centered in zero of $[-1/\\sqrt{k};1/\\sqrt{k}]$ where k is the size of the previous layer of a fully-connected network. For deep architectures, many nodes would likely be initialized very close to zero, leading to the previously mentioned vanishing gradient problem. Bradley (2010, pg. 25) provides an interesting illustration of this, by analyzing the distribution of weights in the initialization of the network. As the number of layers increased, the weights got significantly peaked around zero, thus evidenciating the cause of the issue.\nActivation functions and initialization methods In the following course of deep learning development, improvements introduced into networks\u0026rsquo; training and architecture led to the near abandonment of the very one technique that resurged attention onto the field: unsupervised pre-training. Such advances were a combination of modern activation functions and simpler but sophisticated initialization methods. These also helped to mitigate the vanishing gradient problem. But appart from the ackowledged need for better alternatives to pre-training, what is the problem of using the once well-adopted activation function: the sigmoid? By exploring different functions, it became apparent that the activation was one of the main contributors to the saturation issue, demonstrated through research conducted by Glorot and Bengio (2010)\nMean (lines) and standard deviation (vertical bars) of sigmoid activation values across layers in a neural network using random initialization. The saturation is detectable in the last layer, where the activation values reach virtually zero. Source: [Glorot and Bengio (2010)](#References)\nA technique employed way before such improvements although is goes along with them are the old standardizing of inputs and parameters. LeCun et al. (1998) points out that rescaling the input reducing its mean to 0 and variance to 1 may help training. The first prevents undiserable bias towards particular directions, making it less sensible to the distinc features scales while it also prevents undesirable uniform gradient updates if the majority of the input data where of the same sign. The latter helps to even the contributions from all features, balancing the rate at which the weights connected to the input nodes learn. Additionally it also helps to not saturate too early and does not go straight to zero. Such techniques are not only important on the inputs as they are desirable characteristics for each layer within the network. So, preserving the variance throughout layers during training is a desirable property. Additionally from the backpropagation point of view, the variance of the derivative of the cost function by the weights is equally important to be mantained.\nNevertheless, expanding on Glorot and Bengio (2010 work, Xu et al. (2016) draws theorical clues to indicate that in its linear regime, in comparison with other activation functions, the sigmoid is more prone to escalating the variance of layers througout training.\nConsidering the following notation:\n$$s^i=f\\left(s^{i-1}\\right)W^i+b^i$$\nand\n$$a^{i+1}=f\\left(a^{i}W^{i}+b^{i}\\right)$$\nwhere $f(s)$ and $s$ are the activation function and its argument vector, respectively; $a$ is the activation value, $W\\in\\mathbb{R}^{{n_i\\times n_{i-1}}}$ the weight matrix and $b\\in \\mathbb{R}^{n_i}$ the bias vector. In a linear regime, the activation function can be modeled as:\n$$f(s)=\\alpha s+\\beta$$\nAssuming that, similar to the intialization presented in [eq:1], biases are set to 0, the variance at layer i can be defined as:\n$$Var\\left[a^{i}\\right]=\\alpha{{}^2}n_i\\sigma_{i-1}^{2}\\left(Var\\left(a^{i-1}\\right)+\\beta{{}^2}I_{n_i}\\right)$$\nLikewise, the gradient variance, comprised during the backwards pass, is defined as:\n$$Var\\left(\\frac{\\partial cost}{\\partial a^{i-1}}\\right)=\\alpha{{}^2}n_i\\sigma_{i-1}^{2}Var\\left(\\frac{\\partial cost}{\\partial a^{i}}\\right)$$\nAccording to Glorot and Bengio (2010), preserving the variance throuhgout layers and iterations is an indication that the information is flowing without loss. Furthermore, it is evident that as the variance increase, the more the activation values resort to the function's extremes, resulting in saturation. Thus, ideally:\n$$Var\\left(y^{i}\\right)=Var\\left(y^{i-1}\\right)\\text{ and }Var\\left(\\frac{\\partial cost}{\\partial y^{i}}\\right)=Var\\left(\\frac{\\partial cost}{\\partial y^{l-1}}\\right)$$\nassuming $n_{i}\\sigma_{i-1}^{2}\\approxeq1$ and $n_{i-1}\\sigma_{i-1}^{2}\\approxeq 1$. So, to satisfy this condition $\\alpha$ and $\\beta$ must be:\n$$\\alpha=1\\text{ and }\\beta=0$$\nConsidering the Taylor expansions of different activation functions:\n$$sigmoid(x)=\\frac{1}{2}+\\frac{x}{4}-\\frac{x^{3}}{48}+O\\left(x^{5}\\right)$$\n$$tanh(x)=0+x+\\frac{x^{3}}{3}+O\\left(x^{5}\\right)$$\nThis approximation indicates, that in the linear regime, the tanh function satisfies the condition $\\alpha=1$ while the sigmoid posesses a constant term that may explain the increase in variance throughout the feedfoward propagation. Additionally, due to the significantly small slope in comparison with the tanh, the sigmoid function requires a weight initialization 16 times greater ($\\alpha^{2}$) than the tanh to maintain the gradient variance (Xu et al. (2016)).\nReferences Bradley, D. M. (2010). Learning In Modular Systems. PhD Thesis, Carnegie Mellon University. Erhan, D., Manzagol, P.A., Bengio, Y., Bengio, S., and Vincent, P. (2009). The Difficulty of Training Deep Architectures and the Effect of Unsupervised Pre-Training. Artificial Intelligence and Statistics, 153-160. Glorot, X. and Bengio, Y. (2010). Understanding the diffculty of training deep feedforward neural networks. Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics , pages 249-256. Goodfellow, I., Bengio, Y., and Courville, A. (2016). Deep Learning. MIT Press. LeCun, Y. A., Bottou, L., Orr, G. B., and MÃ¼ller, K.-R. (1998). Efficient BackProp. Neural Networks: Tricks of the Trade, 9-48. Shanmugamani, R. (2018). Deep Learning for Computer Vision. Xu, B., Huang, R., and Li, M. (2016). Revise Saturated Activation Functions.  ","date":1557378000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557378000,"objectID":"8832da7da1695fff4ed9bf77f04cabea","permalink":"/work/vanishing_gradients/","publishdate":"2019-05-09T00:00:00-05:00","relpermalink":"/work/vanishing_gradients/","section":"work","summary":"body { text-align: justify}  The objective of this series is to illustrate some of the past and on-going challenges within deep learning. In the previous blog post, the obstacle discussed was the saturation within activation functions during training. This is a follow-up, so I recommend you take a brief look before continuing.\nProblem: Vanishing gradients As mentioned in the previous post, the activation values start to migrate towards the extremities of the sigmoid function in networks with several layers.","tags":null,"title":"Obstacles along deep learning evolution: vanishing gradients","type":"work"}]