[{"authors":null,"categories":null,"content":"arturbackdeluca [at] gmail [dot] com\nI am a master's student in Artificial Intelligence and Robotics at the University of Rome. I am currently looking for research programs mainly involved with the investigation of training dynamics of neural networks and their theoretical guarantees. Please feel free to reach me via any of the digital mediums here available. I currently live in Rome, but I\u0026rsquo;m originally from Brazil (I\u0026rsquo;m back home due to COVID). My favorite activities are discovering new genres of music and running, usually at the same time 😃.\n","date":-1,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-1,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"arturbackdeluca [at] gmail [dot] com\nI am a master's student in Artificial Intelligence and Robotics at the University of Rome. I am currently looking for research programs mainly involved with the investigation of training dynamics of neural networks and their theoretical guarantees.","tags":null,"title":"","type":"authors"},{"authors":null,"categories":null,"content":"By this point in time, there\u0026rsquo;s no need to emphasize how important deep learning has become. Yet, despite paving the way for some of the boldest and most complicated projects of this decade, much of deep learning\u0026rsquo;s theoretical guarantees remain unexplained.\nAccording to classical statistical learning theory, the structure displayed by deep networks entails poor predictions over unseen instances. However, this is not often the case and, this mismatch between theory and practice is what gives deep learning a sort of magical aura. At the same time, this aura is besieged by researchers, that, in their skepticism, will constantly try to poke holes to tap into what really allows these models to perform the way they do.\nTo proper discuss these issues, we need to formulate our setting. In classification problems, we typically have:\n A candidate model (or hypothesis) $h$ that maps the domain $x$ to the output $y$; A set of $n$ coupled observations $\\mathcal{S_n} = \\{ (x_1;y_1), \u0026hellip;, (x_n; y_n) \\}$, sampled from an unknown probability distribution $\\mathcal{D}$; A target function $f$ that determines the output $y \\in \\{-1;1\\}$ depending on the input $x$; A loss function $\\ell$ that measures the prediction error of $h$ with respect to $f$.  Considering our setting, we would like to find a model that most closely approximates the target function $f$. According to our sampling distribution, this entails minimizing our expected risk: the the expected value of the error with respect to our $\\mathcal{D}$:\n$$R(h) := \\mathbb{E}_{\\mathcal{D}}[\\ell(h, f)] \\equiv \\int_{\\mathcal{X}} \\ell(h(x), f(x))\\,p(x)dx$$\nWe go about generating models through a learning algorithm $\\mathcal{A}$, that given a set of observations outputs a hypothesis. Knowing that we cannot access the sampling probability, our compass in this search is our training set. In other words, our proxy for the expected risk is the empirical risk: $$R_n(h) := \\sum_{i=1}^n \\ell(h(x_i), y_i),$$\nusing the principle of Empirical Risk Minimization (ERM), we are assuming that the solution that minimizes the empirical risk also minimizes the expected risk. We say that a learning algorithm generalizes if the empirical risk approaches the expected risk as data grows to infinity. In other words, the generalization error, $\\Delta R = R(h)-R_n(h)$ goes to zero. On the other hand, when this difference remains significantly large, we say the model overfits.\nBut consider the following, if we could access every conceivable hypothesis, what guarantee could ERM provide of finding a candidate that does not overfit? Think of the hypothesis that memorizes all training points and output their correct classification. Under ERM, we\u0026rsquo;d be finding a good candidate, since it perfectly fits our training data. However, when predicting an unseen observation, this model would be no better them random guessing, thus generalizing poorly.\nTo circumvent this, we limit our search space to a certain class of hypotheses, by only considering models with a certain structure (e.g. linear predictors). Here we essentially apply a set of assumptions of our model based on some prior knowledge of the problem, what we call inductive bias.\nUnder these circumstances, ERM is guaranteed to generalize as the number of data points grows to infinity. But more importantly, if our training data is sampled iid, we can devise bounds that indicate, with a small probability of error $\\delta$, how close we are from the true risk.\n$$\\underset{S\\sim\\mathcal{D}^N}{\\mathbb{P}}\\left(R(h) - R_n(h) \\le O\\left (\\frac{C(\\mathcal{H})}{\\sqrt{n}} \\right)\\right) \u0026gt; 1- \\delta$$\nThese bounds are naturally governed by the number of data points, but also by the complexity $C$ of our hypothesis class. When considering finite sets of hypotheses, the complexity measure can simply be represented by the cardinality of the set. However, under infinte hypothesis classes, more elaborated techniques are needed.\nMany competing complexity measures, and consequently generalization bounds, have been developed over the last 30 years: VC-dimension, Rademacher complexity, covering numbers, and so on. If we think about the effect of complexity on the generalization gap, the more complex a class is, the bigger the generalization gap can be. Even with a large amount of data, if the complexity measure of the model is too high, we have fewer guarantees of avoiding overfitting.\nFor the sake of illustration, let\u0026rsquo;s take the number of parameters of a neural network as a measure of its complexity. The MNIST dataset consists of binary images of handwritten digits in a 16x16 grid and contains more than 55 thousand data points. A fairly simple model, as an off-the-shelf LeNet contains more than 470 thousand parameters only in its first layer, let alone in the rest of the network. Due to its complexity, for this model to generalize with an acceptable error and probability, we would need trillions and trillions of data points. If we took this model and train it over MNIST, our natural inclination would be to think that a great performance in a training set would be the result of an interpolation of the data points, due to the large over parametrization of the model when compared to the dataset.\n Generalization gap decreasing with increase in network capacity. Source: Neyshabur et al. 2018   Still, we have tons of evidence that support generalization of under these settings. So\u0026hellip; what gives?\n Structured Risk Minimization and Regularization  By observing the bounds generated over the complexity of neural networks, we may think that without tremendous amounts of data, we cannot achieve generalization. However, imagine we had a class of hypothesis with a hierarchical structure, i.e. $\\mathcal{H} \\supseteq \\mathcal{H}_1 \u0026hellip; \\supseteq \\mathcal{H}_n$. In this class, each subset $\\mathcal{H_i}$ has a smaller complexity than its supersets $C(\\mathcal{H}) \\ge C(\\mathcal{H}_1) \u0026hellip; \\ge C(\\mathcal{H}_n)$. If we could exploit the hierarchical structure, we can devise a way to learn a function while also minimizing its complexity, thus generating reasonable bounds. Akin to the ERM, this principle is called Structured Risk Minimization (SRM). However, if we even encounter such hierarchical classes, how to make these a controllable variable?\nRegularization is a way to supplement the inductive bias present in our set of hypotheses by restraining the candidates to a particular condition. One typical example is known as weight-decay, or L2-Regularization, where we introduce a norm restriction on our model\u0026rsquo;s parameters $W$, generating the following optimization problem in the Ivanov form.\n$$\\textrm{minimize} \\underset{\\textrm{s.t. } \\left\\Vert W\\right\\Vert_2^2 \\le r}{R_n(h)}$$\nTypically, for means of analysis, we resort to the Tikhonov form despite not being entirely interchangeable as we switch from a hard to a soft penalization term.\n$$\\textrm{minimize } R_n(h) + \\lambda\\left\\Vert W\\right\\Vert_2^2$$\nRegularization can come in different shapes and sizes. Besides, sometimes, regularization may come implicitly. This roughly refers to the learner’s preference to implicitly choosing certain structured solutions as if some explicit regularization term appeared in its objective function. For instance, it is known that introducing noise to the observations may induce the same effect as weight decay (Bishop (1994)). Another equivalent example is early stopping, i.e. stopping training once the validation loss starts to increase (Goodfellow (2016)).\nHowever, despite all these techniques, even \u0026ldquo;bare-boned\u0026rdquo; deep networks seem to achieve fairly good generalization. Our intuition tells us that there must be an implicit control factor on the hypothesis space when training neural nets.\nComplexity control via gradient descent There are many competing intuitions on how deep neural nets are implicitly regularized. Generally, most of them point to the same factor: the training procedure, or more specifically, the gradient-based training. I don\u0026rsquo;t intend to make this a comprehensive review, so instead, I\u0026rsquo;ll provide an analysis on one of these, which is the work of Poggio et al. 2020 called Complexity Control by gradient descent in deep networks.\nGradient descent (GD) and its variations are widely used when training neural networks. In short, this technique update all parameters in the direction of the largest decrease on the loss function at each iteration, hence the name.\n$$W_{t+1} = -\\gamma(t)\\nabla_{W_t} R_n$$\nIn the analysis of Poggio (2020), we use deep networks with ReLU (Rectified Linear Unit) activation functions: $\\sigma(z) = \\frac{\\partial\\sigma}{\\partial z}z$. Also, instead of adding biases to each layer, we\u0026rsquo;ll only add them in the input layer as just another feature. Finally, we\u0026rsquo;ll be studying classification tasks using the exponential loss function that takes values $\\{-1;1\\}$:\n$$\\ell(x, y) := e^{-yf(x)}$$\nWe use this loss function to simplify our analysis of the gradient dynamics. Nevertheless, these results may be also extended to other exponential loss functions, such as the cross-entropy, most typically employed in classification tasks.\nOne of the advantages of this setting is to make use of the homogeneity property granted by ReLUs. This property enables us to detach the effects of scale and direction of the parameters. With that in mind, state our problem as follows: let $f(W,x)$ be a neural network composed by N layers $W_k$ In a given iteration, we can represent this network by the normalized version of the weights at layer $k$, that is: $$W_k = \\rho_k V_k,$$ where $\\rho_k$ is the norm of the weights at layer $k$ and $\\left\\Vert V_k \\right\\Vert$ is the unitary version of $W_k$. Using the homogeneity property, we can rewrite $f$ as being: $$f = \\tilde{f}(V,x)\\prod_{i=1}^N\\rho_i,$$ where $\\tilde{f}$ is the normalized version of the network.\nOur goal here is to study how the gradient behaves throughout iterations, and we\u0026rsquo;ll do so through the lens of a dynamical system in the continuous space. That is, instead of study discrete evolutions, we\u0026rsquo;ll focus on the rate $\\dot{W}$.\n$$\\dot{W} = \\frac{dW}{dt} = -\\gamma(t)\\nabla_W(R_n(f)).$$\nFor the rest of the work, we neglect the effect of the learning rate $\\gamma(t)$, thus obtaining the following rate for each layer:\n$$\\begin{aligned} \\dot{W}_k \u0026amp;= -\\frac{\\partial R_n}{\\partial W_k} \\\\\n\u0026amp;= \\sum_{i=1}^{N}\\left [\\frac{\\partial f(x_i)}{\\partial W_k}\\right]y_ie^{-y_if(x_i)} \\\\\n\\end{aligned}$$\nNext, we\u0026rsquo;ll break down the dynamics of $\\dot{W}$ in its scalar and normalized component $\\dot{\\rho}$ and $\\dot{V}$.\n$$\\dot{\\rho}_k = \\frac{\\partial \\left\\Vert W_k\\right\\Vert_2}{\\partial t} = \\frac{\\partial \\left\\Vert W_k\\right\\Vert_2}{\\partial W_k}\\frac{\\partial W_k}{\\partial t}= V_k^\\top\\dot{W}_k.$$ and $$\\dot{V}_k = \\frac{\\partial V_k}{dt} = \\frac{\\partial V_k}{\\partial W_k}\\frac{\\partial W_k}{\\partial t} = -\\frac{S}{\\rho_k} \\dot{W}_k,$$\nwhere\n$$S_k = I - V_kV_k^\\top.$$\nIn the following sections, we impose and relax some constraints and observe the effect on $\\dot{V}$ and $\\dot{\\rho}$ under three different scenarios.\n Case 1: constraining $\\rho$ and $V$ In the first scenario, we consider $\\rho$ to remain fixed throughout iterations and consistent with the definition, we\u0026rsquo;ll impose a restriction on $V_k$, such that $\\left\\Vert V_k \\right\\Vert_2^2 = 1$. There are several ways to impose this restriction, but for consistency, we\u0026rsquo;ll use Lagrangian multipliers. Our objective function then becomes:\n$$\\mathcal{L} = R_n(\\rho \\tilde{f}) + \\sum_k\\lambda_j \\left(\\left\\Vert V_j \\right\\Vert_2^2-1\\right).$$\nSince $R_n = \\sum_i e^{-y_i \\rho\\tilde{f}(x_i)}$ and $\\dot{W}_k = \\frac{\\partial \\mathcal{L}}{\\partial W_k}$, we get:\n$$ \\dot{W}_k = \\sum_i e^{-y_i \\rho\\tilde{f}(x_i)}y_i\\rho\\frac{\\partial \\tilde{f}}{\\partial W_k} + 2\\lambda_k\\frac{\\partial \\left\\Vert V_k \\right\\Vert_2^2}{\\partial W_k} $$\nSince by design, $\\frac{\\partial \\mathcal{L}}{\\partial \\rho} = 0$, we focus on $\\dot{V}$:\n$$ \\begin{align} \\dot{V}_k \u0026amp;= \\sum_i e^{-y_i \\rho\\tilde{f}(x_i)}y_i\\rho\\frac{\\partial \\tilde{f}}{\\partial V_k} - 2 \\lambda_kV_k \u0026amp;\\left(\\cdot\\, V^\\top_k\\right)\\\\\n\\cancelto{0}{\\dot{V}_kV_k^\\top} \u0026amp;= \\sum_i e^{-y_i \\rho\\tilde{f}(x_i)}y_i\\rho\\overbrace{\\frac{\\partial \\tilde{f}}{\\partial V_k}V^\\top_k}^{\\tilde{f}(x_i)} - 2 \\lambda_k\\cancelto{1}{V_kV^\\top_k} \u0026amp;\\\\\n\\end{align} $$\nProof: $V_kV^\\top_k = 1$ since $\\left\\Vert V_k \\right\\Vert_2^2 = 1$ and $\\dot{V}_kV_k = 0$ due to $\\frac{\\partial \\left\\Vert V_k \\right\\Vert_2^2}{\\partial t} = 0$. Setting $\\lambda_k = \\frac{1}{2}\\sum_ie^{y_i\\rho\\tilde{f}(x_i)}y_i\\rho\\tilde{f}(x_i)$ we have: $$ \\begin{align} \\dot{V}_k \u0026= \\rho\\sum_ie^{-y_i\\rho\\tilde{f}(x_i)}y_i\\left(\\frac{\\partial \\tilde{f}}{\\partial V_k} - \\tilde{f}(x_i)V_k\\right)\\\\\\\\ \u0026= \\rho\\sum_ie^{-y_i\\rho\\tilde{f}(x_i)}y_iS_k\\frac{\\partial \\tilde{f}}{\\partial V_k}. \\end{align} $$ As so happens with over parametrized models, we expect to reach a point of total class separability, where $sgn(f(x_i)) = sgn(y_i) ,\\forall (x_i, y_i) \\in \\mathcal{D}$. And this is the moment where we want to study. For a large enough $\\rho$, since we have perfect separation in the training set, the greatest values will yield the smallest loss contributions (due to the negative exponential), thus, they will vanish first. Hence, it is reasonable to assume that we reach a point where all but a few data points $x_*$ will vanish, converging to a stationary point in $V_k$ where:\n$$\\frac{\\partial\\tilde{f}(x_*)}{\\partial V_k} = \\tilde{f}(x_*)V_k$$\nWhat is the meaning of this stationary point? We will discuss this after the second and third case.\n Case 2: constraining $V_k$ In this case, everything remains the same, except that we let $\\rho$ vary. Consequently, we have the following rate $\\dot{\\rho}=\\frac{\\partial \\mathcal{L}}{\\partial \\rho_k}$: $$ \\begin{align} \\dot{\\rho}_k \u0026amp;= \\sum_{i=1}^{N}\\left [V_k^\\top\\frac{\\partial f(W, x_i)}{\\partial W_k}\\right]y_ie^{-y_if(x_i)}\\\\\n\u0026amp;= \\sum_{i=1}^{N}\\left [\\frac{\\rho}{\\rho_k}V_k^\\top\\frac{\\partial \\tilde{f}(V, x_i)}{\\partial V_k}\\right]y_ie^{-y_if(x_i)}\\\\\n\u0026amp;= \\frac{\\rho}{\\rho_k}\\sum_{i=1}^{N}f(V, x_i)y_ie^{-y_i\\rho\\tilde{f}(x_i)} \\end{align} $$\nHere we notice an interesting evolution. Again, under total separability, the rate $\\dot{\\rho}$ will be always non-negative. Since it will march towards infinity where $R_n(\\rho\\tilde{f})\\rightarrow 0$ we expect to have the same behavior we saw in the first scenario.\nCase 3: unconstrained gradient descent Finally, we come to the typical training setting where we have no explicit restrictions over any parameter. Further expanding the gradient dynamics we saw in the definition, we have:\n$$ \\begin{align} \\dot{\\rho}_k \u0026amp;= V_k^\\top\\dot{W}_k\\\\\n\u0026amp;= \\sum_{i=1}^{N}V_k^\\top\\left [\\frac{\\partial f(W, x_i)}{\\partial V_k}\\right]y_ie^{-y_if(x_i)}\\\\\n\u0026amp;= \\frac{\\rho}{\\rho_k}\\sum_{i=1}^{N}\\tilde{f}(x_i)y_ie^{-y_i\\rho\\tilde{f}(x_i)}\\\\\n\\end{align} $$\nand, $$ \\begin{align} \\dot{V}_k \u0026amp;= \\frac{S_k}{\\rho_k}\\dot{W}_k\\\\\n\u0026amp;= \\sum_{i=1}^{N}\\frac{S_k}{\\rho_k}\\left [\\frac{\\partial f(W, x_i)}{\\partial V_k}\\right]y_ie^{-y_if(x_i)}\\\\\n\u0026amp;= \\frac{\\rho}{\\rho_k^2}\\sum_{i=1}^{N}y_ie^{-y_i\\rho\\tilde{f}(x_i)}S_k\\frac{\\partial \\tilde{f}(x_i)}{\\partial V_k}\\\\\n\\end{align} $$\nHere we see that the gradients $\\dot{\\rho}_k$ and $\\dot{V}_k$ have identical and near-identical rates to the other scenarios, respectively, differing only by a factor of $\\frac{1}{\\rho_k^2}$.\nFor the scalar factor $\\rho$ it is quite common to see methods that have the same march to infinity once separability has been reached. Also, under the same conditions of full separability, we can see that $V_k$ has the same stationary point in the unconstrained case, thus presenting a very plausible explanation on how gradient-based methods perform complexity control, by imposing an implicit restriction on $\\left\\Vert V\\right\\Vert$.\nSo, in the end, gradient descent doesn\u0026rsquo;t really care so much about the size or norm of the weights, rather its direction. But what is the meaning of this direction?\nInterpretation Under the mentioned assumptions, these dynamics all converge to the same stationary point, one that maximizes the margin. The margin is the minimum distance between a training example of a class and the decision boundary of the classifier. Along this boundary the classifier impartial to the competing classes, so by maximizing the minimal distance from this region, we are essentially promoting a greater dichotomy of classes.\nAs some intuition on why this happens in our case, think of the dynamics of $\\dot{V}_k$ mentioned in Case 1. As $\\rho$ increases, all the points where there\u0026rsquo;s more agreement between $y_i$ and $f(x_i)$ start to vanish. What we are left are the points $x^*$ where the classifier is most uncertain, i.e. closest to the decision boundary, much like the support vectors of an SVM. A guideline to a formal proof (as in Banburski et al. 2019) is to show that given two candidate solutions that fully separate our data, the one that minimizes the empirical risk has a bigger margin.\nExperiments To validate the dynamics here presented, we propose the following experiment. We use a noiseless dataset for binary classification to train models with similar architectures but different parametrizations. All of these are fully-connected networks with ReLU functions and no biases, all initialized with the same parameters. We only introduce a bias term in the first layer, as an additional input, following the theoretical setting presented. As a result of the layout, we can exploit the homogeneity property in the parametrization procedure.\nIn this step, we break down the weights into two parameters: scalars $\\rho$ and normalized weights $V$. To enforce normalization throughout training, we either introduce a penalization term on the $\\left\\Vert V\\right\\Vert_2$, as with Lagrangian multipliers, or we forcibly normalize $V$ after each gradient update 2. And so we arrive at the following results.\n2 More details on the experiment can be found in this repository.  Evolution of $\\rho$ and $V$ during training. On the left the progression $\\prod_k \\rho_k$ for all model variations, and on the right, the distance of $V$ wrt to the final solution of the baseline model. The colored band indicates the region where all networks achieve full-separability on the training set.  Despite displaying distinct dynamics, these all share important characteristics. For one, although in different rates, all $\\rho$ increase once full separability is reached. What\u0026rsquo;s more, in the Lagrangian setting, there\u0026rsquo;s a strong relationship between the regularization factor $\\lambda$ and the growth of $\\rho$.\nAs for the normalized weights $V$, it is unlikely for all networks to yield the same solution considering their different training dynamics and the redundancy in the network. Nevertheless, once separability is achieved, these become closer to the baseline solution.\n Evolution of the margin throughout training  Finally, considering the margin $\\min f(x_i)y_i ,\\forall (x_i,y_i)\\in\\mathcal{D}$, its dynamics seem to agree with theory, as all margins increase after separability. Although there is a slight decrease in these for the highest regularization settings, that may be explained by an excessive penalization on $\\left\\Vert V \\right\\Vert$ and oscillations of the support vector.\nOutlook We\u0026rsquo;ve seen a lot of technical details regarding the dynamics of the parameters during training. But, how does this relates to the initial topic of generalization bounds? The general understanding is expecting that this preference for a particular solution poses a limitation on the class complexity thus explaining the generalization phenomena of deep neural networks. In fact, there is some empirical evidence that may favor the view generalization under normalized networks.\n Testing loss on the CIFAR10 dataset for different initializations. The figure on the left shows the original networks while the one of the right shows the same networks normalized at each layer. Refer to Banburski et al. 2019 for more details.   Furthermore, the concept of margin is already widely explored in the context of complexity measures (Antos et al. 2002, Bartlett et al. 2017) . Particularly, one of its main instruments is the estimate named Rademacher complexity. One example of such classical generalization bounds is:\n$$R(h) \\le R_n(h) + c_1\\mathbb{R}_n(\\mathcal{H}) + c_2\\sqrt{\\frac{ln(\\frac{1}{\\delta})}{2n}},$$\nwhere $\\mathbb{R}_n$ is the empirical estimate of the Rademacher complexity over the hypothesis class $\\mathcal{H}$. However, making use of the homogeneity property $h = \\rho\\tilde{h}$ we arrive at:\n$$R(\\rho\\tilde{h}) \\le R_n(\\rho\\tilde{h}) + \\rho\\mathbb{R}_n(\\widetilde{\\mathcal{H}}) + c_2\\sqrt{\\frac{ln(\\frac{1}{\\delta})}{2n}},$$\nthe expectation is to decrease the class complexity while also controlling the generalization bound via $\\rho$. However, as we already know, $\\rho$ naturally increases throughout training, so even with techniques that promise to bound $\\rho$ may not be enough to explain generalization in the current settings.\nAs of now, even though there\u0026rsquo;s no unified or undisputed explanation on the generalization of deep networks. Still, this is one of the most popular and prolific topics in theoretical machine learning. With this standing challenge come many competing explanations but also new directions and ingenious analyses, all of which could be used in demystifying deep networks as well as the new endeavors about to come.\nReferences Shalev-Shwartz, S., \u0026 Ben-David, S. (2014). Understanding machine learning: From theory to algorithms. Cambridge university press. Neyshabur, B., Li, Z., Bhojanapalli, S., LeCun, Y., \u0026 Srebro, N. (2018). Towards understanding the role of over-parametrization in generalization of neural networks. arXiv preprint. Bishop, C. M. (1995). Training with noise is equivalent to Tikhonov regularization. Neural computation Goodfellow, I., Bengio, Y., Courville, A., \u0026 Bengio, Y. (2016). Deep learning. Cambridge: MIT press. Poggio, T., Liao, Q., \u0026 Banburski, A. (2020). Complexity control by gradient descent in deep networks. Nature communications. Banburski, A., Liao, Q., Miranda, B., Rosasco, L., Hidary, J., \u0026 Poggio, T. (2019). Theory III: Dynamics and Generalization in Deep Networks - a simple solution. arXiv preprint. Antos, A., Kégl, B., Linder, T., \u0026 Lugosi, G. (2002). Data-dependent margin-based generalization bounds for classification. Journal of Machine Learning Research. Bartlett, P. L., Foster, D. J., \u0026 Telgarsky, M. J. (2017). Spectrally-normalized margin bounds for neural networks. In Advances in neural information processing systems.   body { text-align: justify}  ","date":1607749200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607749200,"objectID":"9f4e999bbcc0f0add9560610ff91f5cf","permalink":"/post/complexity/","publishdate":"2020-12-12T00:00:00-05:00","relpermalink":"/post/complexity/","section":"post","summary":"By this point in time, there\u0026rsquo;s no need to emphasize how important deep learning has become. Yet, despite paving the way for some of the boldest and most complicated projects of this decade, much of deep learning\u0026rsquo;s theoretical guarantees remain unexplained.","tags":null,"title":"A glimpse over capacity control in deep networks","type":"post"},{"authors":null,"categories":null,"content":"This work discusses the incapability of certain activation functions to work on deep neural networks under the prevailing initialization technique. As the number of layers increase, a problem known as saturation arises.\nSaturation is seen in bounded activation functions, as the weights in certain layers of network push all the activation values (i.e. the output of the activation functions) towards its bounded extremities. Since all the outputs in a certain layer are located in plateaus, it is challenging to distinguish observations, thus learning hardly takes place.\n The saturation regions of the sigmoid (left) and its derivative (right)  Saturation in activation functions such as the sigmoid and the hyperbolic tangent also present another predicament in gradient-based learning. Since the aforementioned functions have symmetric derivatives around zero, as the observations migrate to the functions' extremes, their corresponding values in the derivative of activation function tend to zero. This in turn makes it harder for the system to steer towards optimal solutions as the parameter updates get smaller.\nTraditionally, the weights of neural networks were initialized according to a technique popularized by Lecun (1998), which consists of sampling the weights from a bounded uniform distribution,\n$$W_{ij} \\sim U\\left[\\frac{-1}{\\sqrt{n}}, \\frac{1}{\\sqrt{n}}\\right]$$\nparametrized by $n$, the number of incoming connections to the linked node.\nHowever, in settings with a great number of layers, saturation often took place. In 2006, a research direction brought forward by Hinton et. al (2006), allowed for the training of deep neural networks using unsupervised pretraining, which yielded starting points that prevented this issue.\nGlorot and Bengio claim that good starting points as the ones generated by pretraining could be emulated by common initialization techniques. This was laid on top of an assumption: to maintain the variance of activation values and their gradients. By creating an initialization technique with these properties, the authors would precisely tackle saturation, as very small variances are seen in saturated activation values and their gradients. With these assumptions, the authors reach the following initialization technique:\n$$W_{ij} \\sim U\\left[\\frac{-6}{\\sqrt{n_j + n_{j+1}}}, \\frac{6}{\\sqrt{n_j + n_{j+1}}}\\right]$$\nwhere the $n$ are the number of incoming ($\\,j\\,$) and outgoing ($\\,j+1\\,$) connections. This initialization technique is often referred as the Xavier initialization. In the following sections, we are going to analyze the performance of this and other initialization techniques under several configurations.\nExperiments Here we replicate the experiments of the following paper\n Glorot, X. and Bengio, Y. (2010). Understanding the difficulty of training deep feedforward neural networks. Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, pages 249-256.\n The following results were obtained by training 5-hidden layer networks with 1000 neurons each, trained over the CIFAR-10 dataset. The density estimations are activation values of 300 observations of the test set. The implementation can be found here. Here are some of the results:\n Initialization Activation function    Sigmoid Hyperbolic tangent   LeCun (1998)     Glorot and Bengio (2010)     First off, the different distributions between activation functions can be explained by their different scale, as the sigmoid has a (0,1) range and the tanh (-1,1). As we can observe, regardless of the method of initialization employed, the activation values of the fifth hidden layer in the sigmoid spike around zero. These results are in accordance with the ones shown in the paper.\nThe authors indicate that this abrupt shift can perhaps be explained by an attempt of the system to suppress the meaningless information fed by the previous layers, thus basically relying on its bias to make the classification. With this result, the authors show the unsuitability of the sigmoid in deep networks.\nHowever, the results of the hyperbolic tangent function have shown to deviate from the original findings. Despite similar results in the baseline initialization (LeCun (1998)), these drastically differ in face of the initialization method proposed by the authors. The technique that allegedly inhibits saturation actually made it occur slightly faster.\nActivation values of the hyperbolic tangent during initialization using Lecun (1998) (top) and the proposed technique (below). Source: Glorot and Bengio (2010)  Furthermore, contrasting with the original work, the replicated activation values using the CIFAR-10 dataset have not shown any qualitative differences between the then go-to and the proposed initialization.\n Activation values across two different initialization techniques: Lecun (1998) on the left and Glorot and Bengio (2010) on the right  Additional experiments Unsupervised pretraining Besides the aforementioned techniques, the same analysis was performed on a network trained using unsupervised pretraining. Different from the Restricted Boltzmann Machine proposed by Hinton, we use a Denoising Autoencoder (Vincent et al. (2008)) to train each layer of the network for 100 epochs before the gradient-based stage.\n Activation values using unsupervised pretraining: sigmoid on the left and tanh on the right.  Here we notice that the early stages of training slowly push the activation values towards the extremities. Despite not having a great effect on the sigmoid, which after the process demonstrated a similar behavior, the unsupervised pretraining caused a great change in the hyperbolic tangent setting. Instead of continuing to spike around the extremities, the last layer of the network has settled in zero, a quite unstable point for the activation function. We conjecture that this behavior can be explained as the authors have done for the sigmoid: The high peak in the last layer on zero is an attempt to suppress useless information from previous layers.\nDynamic variance comparison In order to verify if the assumptions made by authors would hold during training, we compared the variance of the activation values across three different cases using the hyperbolic tangent:\n Variance of activation values across three different initialization techniques. From left to right: LeCun (1998), Glorot and Bengio (2010) and Vincent et al. (2008)  Here we noticed that the first and second results do not vary greatly. This by itself does not mean that the assumptions do not hold. However, when analyzing the behavior of the curves, we notice the abrupt growth in the early stages of training, which is later bounded by the function range. Despite \u0026ldquo;stabilizing\u0026rdquo; in later stages of training, this more likely has to do with the restrictions of the activation function, rather than the well-behaved, expected, performance.\nFurthermore, a similar result can be seen in the pretraining stage, only slower. The surges seen can be justified by the sampling procedure adopted during the pretraining phase, executed every 5 epochs, instead of every epoch as in gradient-based training. As mentioned in the last section and different from the other settings, here we can also notice the null-gradient in the last layer of the network.\nConclusion Finally, in qualitative terms, no configuration was immune to saturation and the discrepancies here showed may have potential reasons. The main dataset used by the authors was a syntactic Shapeset-3x2, while most of our analyses were performed by using the CIFAR-10 dataset. However, note that some of these analyses were also extended to the MNIST dataset, as suggested by the authors. Yet, they have yielded similar results like the ones here shown.\nMore details on the historical perspective of saturation, as well as an overview of unsupervised pretraining, can be seen here\n P.S. The activation plots slightly extrapolate the precise output limits. That doesn\u0026rsquo;t invalidate the conclusions but in influences the shape of the estimated distributions.\n References Glorot, X. and Bengio, Y. (2010). Understanding the diffculty of training deep feedforward neural networks. Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics , pages 249-256. LeCun, Y. A., Bottou, L., Orr, G. B., and Müller, K. R. (1998). Efficient BackProp. Neural Networks: Tricks of the Trade, 9-48. Hinton, G. E., Osindero S., and Teh Y. (2006). A Fast Learning Algorithm for Deep Belief Nets. Neural Computation 18:7, 1527-1554 Vincent, P., Larochelle, H., Bengio, Y., \u0026 Manzagol, P. A. (2008). Extracting and composing robust features with denoising autoencoders. In Proceedings of the 25th international conference on Machine learning (pp. 1096-1103).   body { text-align: justify}  ","date":1603947600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603947600,"objectID":"35f224bfa0c30c2147bcb955072f909d","permalink":"/post/saturation/","publishdate":"2020-10-29T00:00:00-05:00","relpermalink":"/post/saturation/","section":"post","summary":"This work discusses the incapability of certain activation functions to work on deep neural networks under the prevailing initialization technique. As the number of layers increase, a problem known as saturation arises.","tags":null,"title":"Investigation saturation","type":"post"},{"authors":null,"categories":null,"content":"How close can we get to a goal that can’t be precisely described? This idea may seem odd at first, but many of the concepts we daily think of cannot be defined accurately. Being like so, how can one ensure to have actually achieved such a goal?\nConsider an individual looking to get healthier. At first thought, his conception of well-being is likely a poorly defined notion instead of a thorough combination of bodily measurements, such as blood pressure or respiratory capacity. Even if it was, he would come to realize that health cannot be completely outlined by these, as they overlook other aspects of health, such as mental sanity.\nThe broader a concept seems, the harder it is to come up with a comprehensive definition for it, let alone ways to measure it. Still, this all may come across as far too overcritical, since we rarely mull over such problems in our daily lives. In practice, we avoid these dilemmas projecting our goal onto a concrete achievement or feature that could be indicative of this same objective. In our example, perhaps getting healthier may come as a consequence of eating vegetables every day, getting 8 hours of sleep, or even running that half-marathon at the end of the year.\nIn the scientific community, however, precise definitions matter. Furthermore, if we venture ourselves in this task collectively, how can we ensure that our notions and expectations converge? In these vague circumstances, reaching common ground is not only pragmatic but crucial. With a layer of optimism, the hope is to perpetually refine these agreed-upon goals and definitions under the scientific method, even though we may not be yet qualified to properly define this.\nAs in many other fields, there seems to be a taxonomical gray area within artificial intelligence. This and some of its associated terms have lost their original meaning due to the rapid wear and tear in light of the breakthroughs in deep learning over the past decade, and the inflated expectations that have followed. Professor Michael Jordan goes as far as arguing that these recent advancements are not considered part of artificial intelligence anymore. Jordan claims that current machine learning and deep learning are more focused on an engineering aspect instead of pursuing the original goal of AI.\nThe field of artificial intelligence has evolved from isolated research streams, including the early blueprints where current neural network models originated from (Buchanan (2005)). The term Artificial Intelligence (AI) was famously coined by John McCarthy for the Dartmouth convention, a two-month workshop dedicated to discussing crucial questions on this then-emerging-yet-to-be-defined field (McCarthy et. al (1955)). The name was chosen by McCarthy as a way to accommodate and conciliate these research angles around the theme at the time.\nAccording to McCarthy, Artificial Intelligence is the science and engineering of making intelligent machines, especially intelligent computer programs. By investigating and precisely describing the features of intelligence, McCarthy believed that these could be then replicated by machines. This perspective, however, leads us to a tricky debate: what constitutes intelligence?\nIntelligence Intelligence is such a far-reaching concept that its definition remains unsettled. In 1921, in an attempt to technically outline it, the Journal of Educational Psychology asked fourteen experts on the field to present their definitions of intelligence. As expected, there were fourteen distinct definitions. As one theorist on intelligence, R. Sternberg, points out: “there seem to be almost as many definitions of intelligence as there were experts asked to define it” (Legg and Hunter (2006)).\n Having learned or ability to learn to adjust oneself to the environment (S. S. Colvin)\nThe capacity to learn or to profit by experience (W. F. Dearborn)\n Some definitions extracted from the Journal of Educational Psychology (Pfeifer and Scheier (1999))\nIntelligence is a descriptive term, hence it assigns a certain set of characteristics to a group or individual. So being, it is fairly unlikely to accommodate multiple angles into a single definition. To some, intelligence is in the collective behavior demonstrated by colonies of bees and ants; to others in particular aspects of human intelligence, such as creativity, the ability to speak, or even on how some intelligent features could be artificially recreated.\nNevertheless, this ambivalence didn’t restrain researchers from claiming progress in AI. For decades, the game of chess was considered the pinnacle of the human intellect. Building a machine that could defeat world-class contenders was in the scope of engineers and scientists as to prove that intelligent machines could be built. In 1997, IBM organized a historic rematch between the world chess champion Garry Kasparov and the supercomputer named Deep Blue over a six-game match. In the previous year, Kasparov defeated Deep Blue, winning four out of six games. However, in the following encounter, the system had an unprecedented victory, landing two wins and three draws. This was a symbolic event, showcasing several advances that enabled a system to beat the greatest chess player of all time.\nKasparov against Deep Blue, 1997\nHowever, a large part of the academic community remained skeptical, portraying the victory as the result of a dumb system with amazing processing speed in contrast with Kasparov’s far greater intellect. This was partially true. The system developed by IBM had several chess strategies stored, built upon consulting chess masters multiple times. This allowed the system to cut corners when searching for the right move. Yet, this search was only made possible by the Deep Blue’s remarkable capacity to explore up to 200 million positions per second (Press (2018)).\nAs many critics claim that such a system was unintelligent, they wrongfully make of this concept a dichotomy: either you have intelligence or not. But intelligence is more of a spectrum than a predicate. This becomes more noticeable when we introduce a reference: are rats intelligent? Of course not as much as humans, but definitely more than ants. Despite using it qualitatively, when we say a person is intelligent, we normally mean that this individual has an above-average level of intelligence.\nAs we can observe, there’s little hope of an agreement over an all-inclusive terminology. Nevertheless, almost all definitions of intelligence seem to share a common factor: to exploit the ruling conditions and explore better alternatives. These two processes can be perceived as the ability to actively adapt, either to a ruling or emerging pattern, while preserving a certain structure, what Ashby (1960) refers to as maintaining homeostasis.\nMany of the modern techniques we use today within artificial intelligence, and in particular machine learning, make use of algorithms that modify parameters and adapt to the desired task. But even decades before the Dartmouth convention, some fields already attempted to understand and emulate intelligence using adaptable systems. One field, in particular, named connectionism consisted of representing mental phenomena throughout emerging patterns in the brain. This is the cornerstone in which modern neural networks are based on.\nTheories of mind and artificial intelligence The research streams around artificial intelligence generally involved much more than attempting to engineer intelligent systems. These encompassed not only computer science but cognitive psychology, neuroscience, and philosophy.\nFor centuries, thinkers were interested in understanding the relationship between mind and body. Humans are remarkably aware of the manifestations in our minds. With constructs such as beliefs, desires, actions, emotions, and so forth, we are capable of not describing our mental states, but to attribute these to other individuals. Do these mental states have physical manifestations as well? This question is famously known as the Mind-body problem and the theories around this topic are gathered in the field of philosophy of mind.\nDespite modern evidence suggesting the connection between brain and mind, this question remained fairly open up until the last centuries. With modern scientific advancements, this problem began to involve much more than philosophy. From these interactions, the field cognitive science emerged, focused on investigating the mind and its processes.\nCognitive science had a crucial role in the development of AI, from which two major branches unfolded: connectionism and symbolicism. Despite not being mutually exclusive, these two approaches practically formed a feud in artificial intelligence. Nevertheless, both seem to be rooted in the same theory of functionalism.\nIn philosophy of mind, one of the predominant theories of the last century titled identity of thought interpreted each mental state as a manifestation of a specific event in the brain. Hence, these biological activities were seen as the connection between the mind and the physical domain. This view implied that either a mental state is unique, or, for this to be experienced by others, these must have an identical organic structure.\nAs this view seemed too tight and unrealistic, the functionalist theory offered an alternative: instead of tying each mental state to a biological counterpart, it proposed identifying these by their corresponding functions (Piccini (2004)). If a mental state such as fear could be defined as producing concern for harm and the desire to either fight or flight an enemy. It does not matter if this is achieved through biological elements, as long as it produces the desired effect (Carneades.org (2016)).\nWhat implications of functionalism brought to AI? This perspective entails the following: intelligent behavior does not need to be carried out in the exact structure as we have in our brains. As long as the same results are attained, we can achieve the level of intelligence desired using a surrogate model. Hence, in theory, intelligence can be achieved at the algorithmic level using machines.\nSymbolicism Although functionalism implies that intelligence may be attained apart from biological structures, a clear proxy to this task is understanding how we humans think. Thought can be understood in many forms, fluctuating in a biological to philosophical scale. In the most abstract state, we see thought as the interaction between concepts. And according to the language of thought hypothesis, these relationships follow a syntax, just like language.\nIn symbolicism, this syntax is described according to laws of thought, a set of axioms dating back to Aristotle. Many centuries later, this syntax was revisited by Leibniz, who was fixated into formalizing these ideas in mathematical terms. By doing so, he believed that we could evaluate the trueness in statements in the same way we perform calculations, by verifying if these were mathematically sound. The building blocks laid by Leibniz are known today as the algebra of concepts (Zalta (2000)).\nThe formulation of symbols in Symbolicism can be traced back to the Leibnizian notion of a concept. A concept is an abstract body that encompasses all the properties necessarily implied by an object. For instance, the concept of an apple encodes the characteristic of being a fruit that grows on trees.\n(apple (isa fruit), (color red); (color green), (origin tree) );  Symbolic representation of an apple\nApples have attributes and relationships, some distinct and others unchangeable: apples can be red or green, but all of them are fruits. These characteristics are best expressed by a configuration proposed by Kant called schemas, which allows us to classify different symbols and make inferences based on their features. For instance, we can logically differentiate beets from apples based on their different schemas: despite possibly having the same color, apples are fruits that grow on trees as beets are vegetables that grow in soil.\nThe ideas of Leibniz were later rediscovered by George Boole in the 19th century and were a centerpiece for computer logic (Peckhaus (2018)). The computational models that unfolded that century placed Symbolicism, also known as classical AI or GOFAI (Good Old-Fashioned Artificial Intelligence), as the ruling paradigm artificial intelligence for more than 40 years. According to the physical symbol system hypothesis formulated by Newell and Simon, symbolic AI provided sufficient conditions for achieving a “general intelligent system”1.\n1 For this and other contributions, the authors have received a Turing award in 1975. However, this hypothesis is widely contested, for several reasons, not to mention the vagueness in the definition of intelligence. The authors even later recognized that it may not hold. For a more thorough investigation, see Nilsson (2007)\nReasoning is an important feature of intelligence and thus has always been of great interest in the AI community. Symbolicism has shared great success in these tasks. For instance, one well-celebrated type of implementation called Expert System assembles knowledge of a particular subject, making specialized logical inferences. Upon analyzing an incoming patient with fever, a medical expert system could determine that the patient has some sort of infection, and, based on additional symptoms and logical rules, assert by what type of disease the patient might be afflicted.\nAs in medicine and other risky applications, symbolic systems offered a great advantage. Not only they could automate some of the jobs, but since readable symbols were used throughout the whole solving procedure, these could be easily inspected by human experts for a second opinion. But what would happen if our input was not symbolic? This was where things started to break down.\nInteracting with data extracted from the real world and not some enclosed model was a great challenge for symbolic systems. Let us consider the task of identifying a position in space using cameras. This task was part of a project named Shakey, a collective endeavor conducted by 13 researchers at Stanford, aimed to combine several modules in artificial intelligence into a single robot. The input consisted of a large collection of pixels for a particular instant in time. Based on certain mapped patterns across an enclosed space, Shakey ought to recognize its location within the environment.\nExample of the process flow from input to the high-level representation. (Stanford (1972))\nOnce the camera captures a region of interest, it conducts a series of preprocessing states to extract the relevant features (in this case the lines of the image). Then, this intermediate representation is compared to each pattern in the system’s internal collection that maps a particular landmark, like a corner of a room as the figure demonstrates (Rosen and Nilsson (1967)).\nAlthough having a good performance in controlled environments, such as in Shakey, this technique is unsuited for practical applications. Real-world settings are not only prone to noisy recordings but, many of these require immediate feedback. In contrast, the techniques used to transform the high dimensional input into symbolic representations were not only computationally demanding but difficult to parallelize, as showcased by Shakey’s “excruciating slow operation” (Copeland (2020)). What’s more, these inference systems were not robust enough against noisy data. To make matters worse, the deal-breaker for symbolic AI was the inability to learn based on observations. During the late past century, learning became a central piece of intelligence, as a self-correcting mechanism. And without any human intervention, these systems could not improve their actions.\nSymbolicism is still widely used today. So much so, that in search problems, where multiple configurations must be evaluated (e.g. in chess), the symbolic paradigm offers a clear advantage over sub-symbolic representations. On the other hand, in recently popularized applications, such as natural language processing and computer vision, distributed representations make connectionism much more suited for handling multi-dimensional inputs and computing in parallel.\nPerhaps the motive of dispute between connectionists and symbolists is rooted in placing points of view as opposite sides of a spectrum. In symbolic AI, we have rich, symbolic concepts that are naturally coherent to humans. Alternatively, connectionism makes use of small fragments of information, that only when combined compose an intelligible concept. Symbolicism typically works with binary data and hard, logical constraints as connectionism deals with continuous numbers and soft, probabilistic constraints.\nHowever, as we hover over details, these differences become blurrier. Not only there are systems on both sides that don’t fully comply with these definitions. But at the implementation level, symbols and schemas can also be seen as distributed representations, constituted of streams of binary values. At the same time, the probabilistic operations within connectionism are a mere product of pseudo-random number generators, a product of logical, deterministic devices. Even the first conception of a connectionist model on a computer, the McCulloch-Pitts neuron - more alike symbolic systems - was used to solve propositional logic.\nDespite these imprecise distinctions, the connectionism paradigm received a lot of pushback during the second half of the twentieth century. But as a consequence of all symbolic shortcomings, the paradigm of sub-symbolic artificial intelligence swooped into popularity, bolstered by the wave of recent successes in connectionism.\nConnectionism Connectionist models are built on interconnected units, called neurons, which can be assembled in various arrangements, composing the so-called neural networks. Depending on the incoming stimuli, these neurons dispatch values or not, thus creating a representation based on the overall firing pattern.\nToy illustration of a distributed representation of apples, beets and their corresponding features, showcased by the connections between each node\nFor instance, the representation of apples could indicate the activation of the neurons tree (grows on trees), green or red (color), and fruit. This pattern of discharges is analytically achieved by a set of parameters uniting the nodes, as well as non-linear operations called activation functions, responsible for firing each unit.\nUnlike symbolic AI, this paradigm approaches intelligence from a biological aspect. Prototypical models are often linked to organic counterparts, which, at least to a certain extent, served as inspiration for contemporary architectures. Connectionist modeling was a research stream in computer science only claimed during the 1960s, by Frank Rosenblatt, in association with the cognitive science theory that carries the same name (Copeland (2020)).\nThe early influences of connectionism and date back to the 1850s, with Herbert Spencer’s Principles of Psychology. In this, Spencer alludes to how intelligence can be seen as the successive association of psychological states, which are driven by the connections between such states. In the following decades, William James outlined the mechanisms that indicated how these states could associate, an idea which would be later formalized by Donald Hebb in the 1950s.\nDespite today’s historical remarks, the works of Spencer and James were seen as mere speculation at the time. Considered the father of the field, Edward Thorndike, a student of James, was the first to proclaim himself as a “connectionist”. Thorndike, expanded the hypothesis of James to the biological sphere, suggesting that learning was somehow encoded in the connections between neurons. He also suggested these associations could take place upon external reinforcements. This idea was based on the preceding school of behaviorism, more specifically by the work of Pavlov. In this, he demonstrates the processes of excitation and inhibition, outlining the formation of conditioned reflexes, an adaptation of the brain upon a certain pattern, in the eyes of Thorndike (Walker (1992)).\nIllustration of the classical conditioning experiment conducted by Pavlov in 1897.\nResearchers in computer science later took inspiration from these ideas to develop computational neural network models and their adaptation mechanisms. However, connectionism in computer science has then strayed from its homonym. This paradigm can now be more seen as a functionalist perspective as it neglects the biological correspondence in search of a generalized view of intelligence.\nIn terms of accomplishment, connectionism in computer science has witnessed many successes in the last decade. Reviving the contested feat of Deep Blue, researchers at DeepMind have developed a system named Alpha Zero, that not only beat its predecessor, but it did so by its own experience, only playing with itself. Since the victory over Kasparov, researchers raised the stakes in search of more challenging feats. In 2015, the DeepMind’s AlphaGo defeated Lee Sedol, considered one of the greatest players at the game of Go, an immensely more complex challenge than chess (DeepMind (2020)).\n Despite some push-back against the distancing between the ongoing advancement of neural networks and cognitive science, these fields have been more popular than ever. Not only, neural networks are being used to strengthen cognitive hypothesis2, but these have sprawled and engaged other well-established disciplines on solving standing challenges.\nDue to the viewpoint of intelligence centered around learning, the most sought feature in applications is the ability to generalize, i.e. to transfer experience beyond known instances to novel situations. In this perspective, the circumstances and reasons why deep learning models can do so remain unsettled, despite some advancement brought forward by the current statistical learning theory stream 3.\n2 See Banino et. al (2018)\n3 See Poggio et. al (2020)\nOur current capabilities remain limited, and a question that lingers is how far are we from understanding intelligence, or even from reaching it computationally on a human level? Most likely very far, despite the widespread optimism surrounding deep learning, there are several visible limitations to the current settings. However, a good indication of headway is to not only achieve tangible milestones but also to continuously uncover misconceptions. As biological and philosophical novelties, the ongoing engineering experimentation can also help to shed light on these. And as we validate conceptions from multiple angles, the closer we move towards a comprehensive understanding of intelligence.\nReferences Buchanan, B. G. (2005). A (Very) Brief History of Artificial Intelligence. AI Magazine, 26(4), 53. doi:10.1609/aimag.v26i4.1848 McCarthy, J., Minsky, M. L., Rochester, N. and Shannon, C.E. (1955). A proposal for the Dartmouth summer research project on Artificial Intelligence. Retrieved April 13, 2020, from http://www-formal.stanford.edu/jmc/history/dartmouth/dartmouth.html Legg, S. and Hunter, M. (2006). A Collection of Definitions of Intelligence Pfeifer, R., \u0026 Scheier, C. (1999). Understanding intelligence. MIT Press. Press, G. (2018). The Brute Force Of IBM Deep Blue And Google DeepMind. Forbes Media. Retrieved April 13, 2020, from https://www.forbes.com/sites/gilpress/2018/02/07/the-brute-force-of-deep-blue-and-deep-learning/ Ashby, W. R. (1960). Design for a brain: The origin of adaptive behavior. (2d ed). Wiley; Sarıhan, I. (2017). Chapter 2 - Philosophical Puzzles Evade Empirical Evidence: Some Thoughts and Clarifications Regarding the Relation Between Brain Sciences and Philosophy of Mind. In J. Leefmann \u0026 E. Hildt (Eds.), The Human Sciences after the Decade of the Brain (pp. 14–23). Academic Press. doi:10.1016/B978-0-12-804205-2.00002-1 Piccinini, G. (2004). The First Computational Theory of Mind and Brain: A Close Look at Mcculloch and Pitts’s “Logical Calculus of Ideas Immanent in Nervous Activity.” Synthese, 141(2), 175–215. doi:10.1023/B:SYNT.0000043018.52445.3e Carneades.org. (2016). What is Functionalism? (Philosophy of Mind). Retrieved April 13, 2020, from https://www.youtube.com/watch?v=a5AwaFsp5Os Zalta, E. N. (2000). A (Leibnizian) Theory of Concepts. Logical Analysis and History of Philosophy, 3, 137–183. Peckhaus, V. (2018). Leibniz’s Influence on 19th Century Logic. The Stanford Encyclopedia of Philosophy Newell, Allen; Simon, H. A. (1976), \"Computer Science as Empirical Inquiry: Symbols and Search\", Communications of the ACM, 19 (3): 113–126, doi:10.1145/360018.360022 Nilsson, N. J. (2007). The Physical Symbol System Hypothesis: Status and Prospects. In M. Lungarella, F. Iida, J. Bongard, \u0026 R. Pfeifer (Eds.), 50 Years of Artificial Intelligence: Essays Dedicated to the 50th Anniversary of Artificial Intelligence (pp. 9–17). Springer. doi:10.1007/978-3-540-77296-5_2 Stanford University Libraries (2016). Shakey: Experiments in Robot Planning and Learning (1972). Retrieved April 13, 2020, from https://www.youtube.com/watch?v=GmU7SimFkpU Rosen, C. A. and Nilsson, N. J. (1967). Application of Intelligent Automata to Reconnaissance. Third Interim Report. Stanford Research Institute. Copeland, B.J. (2020). Artificial Intelligence | Definition, Examples, and Applications. Encyclopedia Britannica. Retrieved April 13, 2020, from https://www.britannica.com/technology/artificial-intelligence Walker, S. F. (1992) A brief history of connectionism and its psychological implications. In Clark, A. and Lutz, R. (eds) Connectionism in Context. Berlin: Springer-Verlag. 123-144 DeepMind (2020). AlphaGo: The story so far. Retrieved April 13, 2020, from https://deepmind.com/research/case-studies/alphago-the-story-so-far Banino, A., Barry, C., Uria, B., Blundell, C., Lillicrap, T., Mirowski, P., Pritzel, A., Chadwick, M. J., Degris, T., Modayil, J., Wayne, G., Soyer, H., Viola, F., Zhang, B., Goroshin, R., Rabinowitz, N., Pascanu, R., Beattie, C., Petersen, S., Kumaran, D. (2018). Vector-based navigation using grid-like representations in artificial agents. Nature, 557(7705), 429–433. doi:10.1038/s41586-018-0102-6 Poggio, T., Liao, Q., \u0026 Banburski, A. (2020). Complexity control by gradient descent in deep networks. Nature Communications, 11(1), 1–5. doi:10.1038/s41467-020-14663-9   h1 { text-align: left; } body { text-align: justify; }  ","date":1586667600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586667600,"objectID":"b7a59b2d1f654647e15924f717bae112","permalink":"/post/exploring_vagueness/","publishdate":"2020-04-12T00:00:00-05:00","relpermalink":"/post/exploring_vagueness/","section":"post","summary":"How close can we get to a goal that can’t be precisely described? This idea may seem odd at first, but many of the concepts we daily think of cannot be defined accurately.","tags":[""],"title":"Exploring vagueness: the underlying theses in artificial intelligence","type":"post"},{"authors":null,"categories":null,"content":"In the early development of neural networks, the initialization of parameters was performed by sampling a uniform distribution bounded to a predetermined interval. In this process, the weights and biases produced solutions that would generally lie far from an optimum set of parameters, requiring many training iterations to readjust. Shallow networks did not suffer too much from this due to the smaller number of parameters, hence presenting an easier system to optimize. However, as the depth of networks increased, so did the difficulty to train such models using this initialization procedure.\nGlorot and Bengio (2010) promoted a study to understand why random initialization performed so poorly in deep networks. In this investigation, the authors considered one common initialization heuristic, introduced by LeCun et al. (1998), that defines the biases at 0 and the weights via sampling according to the following uniform distribution:\n$$W_{ij}\\sim U\\left[-\\frac{1}{\\sqrt{n}},\\frac{1}{\\sqrt{n}}\\right]$$\nwhere $n$ is the number of inputs to the unit. The authors then verified that in deep configurations of 4 to 5 layers, activation values on the last layers got stuck in plateaus situated at the extremes of the activation function, a case otherwise known as saturation.\nSaturation regions on the sigmoid function. In the case observed by Glorot and Bengio (2010), the saturation occurred in the 5th and last layer of the network, with activation values converging to zero.\nOne hypothesis that explains saturation on sigmoid-equipped deep networks is that the random initialization does not provide useful information to the last layer of the network, that starts to suppress the previous contributions and rely more on its biases, which, in turn, are trained faster than its weights.\nMean (lines) and standard deviation (vertical bars) of sigmoid activation values across layers in a neural network using random initialization. The saturation is detectable in the last layer, where the activation values reach virtually zero. Source: Glorot and Bengio (2010)\nGradually but rapidly, the error gradient tends to push the activations towards zero in an attempt to suppress the influence of the previous layers. Eventually, the saturation may be overcome but the overall result would be of poor generalization.\nUnsupervised pre-training Layer saturation was one of the biggest technical hurdles that limited the progress of deep learning in the dawn of the millennium. However, in 2006, inspired by a well-established procedure, Hinton et al. (2006) developed a novel approach to initialize the parameters of a Deep Belief Network — a class of neural networks — that overcame the saturation issue and surpassed state of the art performance in deep architectures. These results not only re-sparked but drastically expanded researchers’ interest in this field.\nA Deep Belief Network (DBN) can be seen as a stack of smaller unsupervised learning algorithms named Restricted Boltzmann Machines. This configuration can then be bundled with a classical multi-layer perceptron for supervised learning tasks\nThis initialization procedure encompassed an unprecedented process: an unsupervised greedy layer-wise pre-training step1. Prior to the conventional supervised training, each layer is trained with its anterior neighboring layer identically to a Restricted Boltzmann Machine, using an unsupervised learning algorithm named Contrastive Divergence. This process starts with the input and first layer, and progressively advances one layer at a time until it sweeps all layers.\nA Boltzmann Machine is an unsupervised generative algorithm that learns the data representation by associating the patterns identified in the inputs to probabilistic configurations within its parameters. A Restricted Boltzmann Machine is a variation of such a model that reproduces a similar behavior but with significantly fewer connections.\n1 Despite the imprecision, unsupervised pre-training is here used interchangeably A Boltzmann Machine (left) and a Restricted Boltzmann Machine (right)\nMany other unsupervised pre-training algorithms were developed concomitantly or immediately after, such as autoencoders (Bengio et al. (2007)), denoising autoencoders (Vincent et al. (2008)), contractive autoencoders (Rifai et al. (2011)), among others.\nComparison of performance between networks running without pre-training (left) and with pre-training (right) Source: Erhan et al. (2010, pg. 636)\nWhy does this unsupervised learning methods help training deep architectures? Much of the explanation remains uncertain. Nonetheless, Erhan et al. (2010) provide some clarifications through considerable experimentation. The claims of the authors reside on two possible but not mutually exclusive reasons: optimization and regularization.\nDeep neural networks are composed of many parameters whose values are used to compute an approximation of a function. Due to its substantial nonlinear nature, this approximation yields a non-convex function that poses a challenge on searching the best combination of weights and biases.2\nA convex (left) and non-convex (right) function. Note that, contrarily to the convex function, the non-convex function possesses multiple local optima. Source: [Zadeh (2016)](#References)\n2 Many discussions today in academia evolve around the particular shape of loss landscape in deep neural networks, since many of the local minima appear to have equally good qualities, suggesting that the critical solutions reached during training are actually saddle points. This discussion will be reserved to further studies. Gradient-based methods employed in training eventually converge to their pre-selected basin of attraction, a region of the function space, such that any point in it eventually is iterated into the attractor (roughly speaking, a valley in the loss function). Unsupervised pre-training may work towards optimization by favoring a basin of attraction that might yield a lower training error. Thus, since the gradients are very prompt to abrupt changes, backpropagation is only used at a local search level, from an already favorable starting point (Hinton (2012, lecture 14b)).\nAs for regularization, one may commonly associate it with explicit techniques, such as the L1 or L2 norm:\n$$C=-\\frac{1}{n}\\sum_{j}\\left[y_j\\ln a_j^{L}+\\left(1-y_j\\right)\\ln\\left(1-a_j^{L}\\right)\\right]+\\frac{\\lambda}{2n}\\sum_iw_i{{}^2}$$\nBy adding the L2 regularization factor in the cross-entropy cost function, presented in the equation above, one can penalize overly complex models, that would result in poor generalization, i.e. high testing error. However, the regularization employed by pre-training is implicit. In attempt to model how such technique would work explicitly, Erhan et al. (2009) defines a regularization term such as:\n$$regularizer=-log\\thinspace P\\left(\\theta\\right)$$\nThe function $P\\left(\\theta\\right)$ describes the probability that the weights of the neural network are initialized as $\\theta$. So, if a configuration shows to be highly improbable, the regularizer term will hurt the cost function strongly. Furthermore, if the probability of landing such set of parameters is high, the regularizer will then reward the cost function. This probability is governed by the initialization methods employed. Considering two instances with and without pre-training, we obtain:\n$$P_{{\\text{pre-training}}}(\\theta) = \\sum_k\\frac{1_{\\theta \\in R_k}\\pi_k}{v_k}$$\nand\n$$P_{{\\text{no pre-training}}}(\\theta) = \\sum_k\\frac{1_{\\theta \\in R_k}r_k}{v_k}$$\nwhere $R_k$ is the basin of attraction that contains $\\theta$, and $1_{\\theta \\in R_k}$ is the identifier function – unitary for all $\\theta$ in $R_k$, otherwise null. Additionally, $\\pi_k$ and $r_k$ are the probabilities of landing in the basin of attraction $R_k$, which has a volume $v_k$. Since the basins of attraction are disjunct sets, the probability density function of the set of parameters located in $R_k$ is uniform, calculated by taking the probability of landing in the k-th basin ($\\pi_k$ or $r_k$) and dividing by its volume.\nPre-training the parameters of the network conditions the network initialization to land on regions of better generalization. This is hypothetically achieved by increasing the $\\pi_k$\u0026rsquo;s where the network parameters represent meaningful variations of the input, contributing to predict the output. For this reason, pre-training also reduces the variance upon parameter initialization.\n2-D visualization of parameters' trajectory of 100 neural networks with and without the unsupervised pre-training step. The color gradient from dark-blue to cyan symbolizes the progression of iterations. Source: Erhan et al. (2010, pg. 541)\nThe visualization of the parameters’ trajectory may demonstrate the effects of optimization and regularization. As mentioned through the former, it may select a basin of attraction with lower training errors. Conversely, regularization may bound the parameter interval to a range that yields good generalization. Also, it is crucial to notice that both training and testing errors collected in the experiments of Erhan et al. (2010) support these hypotheses, but do favor the latter.\nFurthermore, once established within the solution space, the parameters do not drastically change during the gradient-based adjustment process. This process is also denominated fine-tuning, as it only modifies the features slightly to get the category boundaries, rather than discovering new relationships (Hinton (2012, lecture 14b)).\nVisualization of filters of a Deep Belief Network used to recognize digits form the MNIST data-set after the different training processes; from left to right: units from the first, second and third layers, respectively.\nSource:Erhan et al. (2010, pg. 638-639)\nBut how can one conceptually understand the effects of unsupervised learning? Apart from the regularization and optimization hypothesis, the layer-wise pre-training resembles the underlying distribution of the input. Ideally, this representation, by combining the different features and mapping their inner relationships, can unveil, and more importantly, disentangle causal elements that influence the output. If those inputs can be transformed into uncorrelated features, it is possible to solve for a particular parameter disregarding its influence over the others.\nAs mentioned in Goodfellow et al. (2016, pg. 541), this hypothesis justify approaches in which one first seeks a good representation for $p(x)$ before training with the output. If the output is closely related to factors captured by the input, an initialization that captures the distribution of x is useful for predicting the desired output distribution $p\\left(y|x\\right)$.\nFurthermore, unsupervised pretraining can be related with the recent work of Schwartz-Ziv and Tishby (2017) on studying neural networks from an information theory perspective. Essentially, the authors claim that the learning process of a neural network model is based on the maximizing the mutual information between the inputs and the outputs. Mutual information can be defined as:\n$$I(X,Y) = H(X)-H(X|Y)$$\nwhere $H$ is the entropy of the variable $X$:\n$$H(X) = \\mathbb{E}[-\\log\\thinspace(P(X)]$$\nEntropy essentially measures the amount of information, i.e. the degree of uncertainty one has over a random variable. Moreover, when provided another random variable Y, we can measure the conditional entropy of X given Y:\n$$H(Y|X)=-\\sum _{x\\in {\\mathcal {X}},y\\in {\\mathcal {Y}}}p(x,y)\\log {\\frac {p(x,y)}{p(x)}} $$\nThus, mutual information is a statistical measurement between two random variables that indicate how much knowing one of these variables reduces uncertainty about the other. This, in turn, may have a connection with training neural networks as one begins the training stage knowing to little about the input and output, hence, having low mutual information stored in the neural network layers.\nMutual information measurements on 100-layer neural networks. Layer ordering ranges from green (initial layers) to orange (final layers). Source:Schwartz-Ziv and Tishby (2017)\nHowever, as the training phase starts, the layers slowing move towards higher mutual information over $X$ and then towards $Y$. What happens in principle is that in the beginning, the network layers learn different representations over the input space, which in turn carry a lot of information over the input, but also some information about the output. As the training phase continues, the network layers, particularly the deeper ones, then discard some of the irrelevant information of $X$ which is not predictive of $Y$.\nSnapshot of mutual information measurements along layers in different trainig epohcs. On the left, the neural network is at epoch zero, on the center at epoch 400, and the on right at epoch 9000. Source:Schwartz-Ziv and Tishby (2017)\nThe pretraining procedure can be related to the theory, particularly in the first part. By using an unsupervised learning algorithm, we may find a combination of parameters for each layer along with the network that shares higher mutual information with the input variable $X$ and consequently some with the output variable $Y$. As the gradient-based learning process continues, some of this information gets refined, as previously mentioned.\nHowever, despite the aforementioned advantages, unsupervised pre-training presents noteworthy drawbacks, such as establishing two separate learning stages (unsupervised and supervised). As a consequence, there is a long delay between adjusting hyperparameters on the first stage utilizing feedback from the second. Additionally, although pre-training being considered a valuable regularizer, its strength adjustment is troublesome, requiring a somewhat unclear modification of far too many hyperparameters — contrasting with explicit regularization techniques that can be adjusted by a single one.\nFor the reasons mentioned above, unsupervised pre-training is not so popularly used today, as other techniques discovered yielded the same benefits but much more efficiently. These may be explained in a follow-up post, where we delve into initialization methods that not only tackle saturation but also a different obstacle: vanishing gradients.\nReferences Bengio, Y. (2009). Learning Deep Architectures for AI. Foundations and Trends in Machine Learning. 1-127. Bengio, Y., Lamblin, P., Popovici, D., and Larochelle, H. (2007). Greedy Layer-Wise Training of Deep Networks. In Advances in Neural Information Processing Systems 19, 153-160. Erhan, D., Bengio, Y., Courville, A., Manzagol, P.A., Vincent, P., and Bengio, S. (2010). Why Does Unsupervised Pre-training Help Deep Learning? Journal of Machine Learning Research, 11-36. Erhan, D., Manzagol, P.A., Bengio, Y., Bengio, S., and Vincent, P. (2009). The Difficulty of Training Deep Architectures and the Effect of Unsupervised Pre-Training. Artificial Intelligence and Statistics, 153-160. Glorot, X. and Bengio, Y. (2010). Understanding the diffculty of training deep feedforward neural networks. Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics , pages 249-256. Goodfellow, I., Bengio, Y., and Courville, A. (2016). Deep Learning. MIT Press. Hinton, G. (2012). Neural Networks for Machine Learning. Coursera Online Course. Hinton, G. E., Osindero, S., and Teh, Y.-W. (2006). A Fast Learning Algorithm for Deep Belief Nets. Neural Computation , 1527-1554. LeCun, Y. A., Bottou, L., Orr, G. B., and Müller, K.-R. (1998). Efficient BackProp. Neural Networks: Tricks of the Trade, 9-48. Rifai, S., Vincent, P., Muller, X., Glorot, X., and Bengio, Y. (2011). Contractive Auto-encoders: Explicit Invariance During Feature Extraction. Proceedings of the 28th International Conference on Machine Learning, 833-840. Vincent, P., Larochelle, H., Bengio, Y., and Manzagol, P.-A. (2008). Extracting and Composing Robust Features with Denoising Autoencoders. Proceedings of the 25th International Conference, 1096-1103. Zadeh, R. (2016). The hard thing about deep learning. O'Reilly Media. Schwartz-Ziv, R. and Tishby, N. (2017). Opening the black box of Deep Neural Networks via Information. Featured on: Why \u0026 When Deep Learning Works: Looking Inside Deep Learning. arXiv:1703.00810v3   h1 { text-align: left; } body { text-align: justify; }  ","date":1557378000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557378000,"objectID":"ce1023914d78fa1081a2443f70dd6539","permalink":"/post/pretraining/","publishdate":"2019-05-09T00:00:00-05:00","relpermalink":"/post/pretraining/","section":"post","summary":"In the early development of neural networks, the initialization of parameters was performed by sampling a uniform distribution bounded to a predetermined interval. In this process, the weights and biases produced solutions that would generally lie far from an optimum set of parameters, requiring many training iterations to readjust.","tags":[""],"title":"Unsupervised pretraining","type":"post"},{"authors":null,"categories":null,"content":"In the last decade, there\u0026rsquo;s been a widespread interest in artificial intelligence, particularly machine learning, and even more specifically deep learning. Curiously, this topic has not only resurged within academia but rather it has also been commonly showcased on non-technical mediums.\nWhy these topics became so popular now? Certainly, recent feats such as AlphaGo or Waymo may raise awareness and hint to potential implications in our social organization, especially in the labor sector. Still, but what underlying technical advances have enabled these achievements - and what makes deep learning a paradigm so relevant within the past few years?\nIn the aforementioned fields, one common task is to reason upon prior knowledge to derive plausible conclusions, a task known as inference. On this matter, strategies within artificial intelligence like expert systems had shared some success by encapsulating an expert\u0026rsquo;s rationale into knowledge bases. These databases were then used by an inference engine, typically using logical rules, to reason out conclusions.\nHowever, as tasks grow in complexity, performance becomes hindered by computational limitations of reaching a decision using an inference engine in a large database. Not just that, but the lack of satisfactory techniques for reasoning under uncertainty was also an important obstacle that hampered the further development of expert systems (Heathfield (1999)).\nA different paradigm, brought forward by machine learning, is to derive the relationships needed for inference using observations. More importantly, these models must be able to generalize this behavior for unseen instances of the given problem. Oftentimes, these relationships within data can be quite convoluted and in recent decades, a class of learning algorithms came to attention by the ability to derive these mappings: local kernel methods.\nThis family of methods is a fairly recent grouping of techniques that have been developed independently for several years. They are called so due to the transformation performed so-called kernel function onto the input data, and some examples of models are Support Vector Machines, Gaussian processes, among other techniques. A local kernel method can be defined as:\n$$f(x) = b + \\sum^n_{i=1} \\alpha_iK_D\\left(x, x_i\\right)$$\nwhere $b$ is the bias term, $D$ is the set of observations of cardinality $n$, $α_i$ is scalar chosen by the learning algorithm upon $D$, $x_i$\u0026rsquo;s are the training input observations and $K_D$ is the kernel function. These methods are more effective than linear models by being able to generate more favorable representations of feature space by using kernels (usually non-linear), achieving fairly positive results in several applications.\n Transformation of features to generate a linearly-separable decision space. On the left the original dataset with two classes. On the right a transformation using PCA with a Gaussian kernel.\n One typical kernel function is the Gaussian kernel:\n$$K_\\sigma(u, v) = e^{-\\frac{\\left\\Vert u-v \\right\\Vert^2}{2\\sigma^{2}}}$$\nwhere $\\sigma^2$ is the equivalent of the variance in a gaussian probability density function. Note that the function is symmetric and as $u$ and $v$ become very distant from each other, the function approaches zero. This means that for the estimation of a query point $u$, the neighboring values of $u$ have much more influence than those further away from it.\nAs postulated by David Wolpert in his no free lunch theorem, no model performs better than random guess without any kind of assumption. Local kernel methods are no exception: they implicitly or explicitly partition the input space, obtaining good generalization for a new query point by exploiting the training examples in its neighborhood. This comes at the price of the so-called smoothness prior, meaning that these models expect a target function with little variation of results among neighboring observations.\n We may insert additional priors through the choice of the kernel as well as the feature representation. This is why for these types of models it might be beneficial to generate alternative representations of the input data. Rather than using their raw representation, these modifications could more closely conform to the model smoothness prior, yielding better results. Although not targeted at handcrafting representations, Bengio et al. (2014) provides a thorough analysis of good characteristics for representations of features.\n However, for particular types of problems the smoothness assumption may not hold, posing a predicament to this family of methods. As an illustration, consider the task of recognizing a character from a $\\texttt{20×20}$ pixel grid. If we consider the number of variations of a simple horizontal translation, we notice that a particular observation of the letter $O$ has much more similarity with the letter $U$ in the exact position and orientation that the same letter $O$ shifted horizontally, for instance.\nDemonstrations of the amount of variation among observations inter and intra-class.\nThis indicates that the target function may not be as smooth as expected. In this highly non-linear function, local kernel methods require a very large number of training examples to cover all the desired variations within a class. Hence, the number of necessary templates can grow exponentially with the intrinsic dimension of a class manifold (Bengio and Lecun (2007)).\nThe set of observations associated with the same class forms a manifold or a set of disjoint manifolds, i.e. regions of lower dimension than the original space of images. When this manifold is smooth, it can be approximated locally by linear patches, tangent to the manifold. However, as the manifold becomes irregular and highly dimensional, patches become smaller and exponentially many patches, as well as data, are required to obtain a good generalization. Source: Bengio (2009)\nNote that this does not confront the ability of local kernel methods to approximate functions, rather their efficiency on approximating a function in regards to data. By contrast, neural network models are capable of dealing with intra-class variations more effortlessly. While each kernel function is activated in a small area of the input space, feature detectors of neural networks are capable of detecting certain patterns (edges or curvatures) almost independently from its orientation. The combination of these feature detectors is what makes neural network representations so efficient.\nAs an illustration, consider the task of representing a number from 0-7. The representation generated by local methods can be seen as a one-hot encoding, signaling that each representation is mutually exclusive, for instance, $\\texttt{0000001}$ signals $1$ and $\\texttt{0000010}$ signals $2$ but the representation $\\texttt{0000011}$ does not convey any valid information. Conversely, a representation of a neural network can be seen as a binary representation, much more efficient: $\\texttt{001}$ is equal to $1$, $\\texttt{010}$ to 2 and $\\texttt{011}$ to $3$.\nIn terms of classification, local methods produce mutually exclusive partitions onto the feature space which it oftentimes makes it difficult to generalize for unseen instances of the problem, if the target function is not smooth. On the other hand, neural networks generate several partitions which are compositional, presenting a more efficient decision surface that can be more robust to unseen query points.\nSketch of a partition of the feature space produced by local methods (left) and neural networks (right). Note that the sub-partitions of the right figure can be combined to infer unseen instances of the problem, represented by the symbol $?$. These sub-partitions can be seen as potential explanatory factors. As an illustration, these can be the color, shape, and texture in the task of detecting particular types of fruits. Conversely, the partitions on the left symbolize specific combinations of these factors that can indicate a certain fruit. Source: Bengio (2009)\nThe set of tasks illustrated so far are relatively simple in comparison to those that we see today, as in multi-class object detection. In previous decades, these simpler tasks were carried out relying on a shallow neural network architecture, i.e. using only a small number of layers. However, as the complexity of applications increase, this approach tends to fail.\nComplex image classification tasks, for instance, pose a challenge to these methods namely because it becomes overwhelmingly difficult to combine a larger number of low-level features (pixels) to determine a contrasting abstract outcome (object). Alternatively, deep learning is able to better accomplish this task (Bradley (2010, pg. 22)), as experimental outlines in He et al. (2015) and Krizhevsky et al. (2012) indicate breakthrough performances upon increase in network depth.\nResults of the 2012 ImageNet classification competition. Only the first out of 5 top models was a deep neural network, which achieved unprecedented performance. Source: ImageNet\nHierarchical learning, deep structured learning, or simply deep learning can be defined as a class of machine learning techniques that exploit many layers of non-linear units (Deng and Yu (2013, pg. 199)). This better capability is attained by the substantial coupling of non-linear operations, deriving complex feature hierarchies from low-level inputs.\nCompositional representation of features throughout layers: from pixels to gradients and edges. Source: Goodfellow et al. (2016, pg. 6)\nAs illustrated, different representations are synthesized along the layers and as representations become more related to the task in hand, the better the model can perform. However, how many layers are necessary to well approximate a function? In reality, the absolute number of layers is not the important factor, rather how many of these are necessary to effectively represent the target function, denominated as the compact representation. Moreover, if a configuration is smaller than the compact representation, that can generate a negative effect on performance.\nIt turns out that a configuration with one layer less than the compact representation may need an exponential number of neurons to achieve the same performance. This can be illustrated by the awarded work of Håstad (1986). By analogy, if we consider neural networks as simple logic circuits (which they are able to emulate), we can observe that a logic architecture limited in-depth presents an exponential number of components in comparison with a deep counterpart. Consider the calculation of the parity function, defined as:\n$$f:\\{0,1\\}^{n}\\rightarrow\\{0,1\\},\\thinspace f(x)=\\left(\\overset{|x]}{\\underset{i=1}{\\sum}}x_{i}\\right) mod\\space 2$$\nThe number of logical components for a depth-limited architecture of 2 layers and $N$ inputs is of an order of $O(2^{N})$. On the other hand, unbounded architectures can produce less complex systems, such as the daisy-chain structure of complexity $O(N\\thinspace log\\thinspace N)$.\nDistinct architectures to compute the parity function. A Disjunctive normal form structure (left) with a complexity of $2^{N-1}$ and a balanced tree structure (right) with 5 layers and complexity of $O(N\\thinspace log\\thinspace N)$.\nConversely, what could happen when freely adding more layers? In principle, besides the additional computational cost, this would not imply any counterpoints to approximating the target function, since the spare layers could simply replicate the second-to-last representation output. However, as in the shallow case, the model may carry too many parameters and insufficient training examples. This, in turn, would have statistical drawbacks: instead of approximating the desired mapping, the model starts to memorize the training data, causing overfitting, i.e. poor generalization1.\nDespite the remarked capacity, deep architectures were not widely researched in the past as today. The reasons for this may encompass the former contemptuous view of researchers on this topic, or an insufficient computational power especially compared to contemporary standards. This, however, will not be covered in this series. The challenges of employing such architectures were also imposed by the difficulty in training neural networks with more than two layers, ultimately delivering poor generalization (Bengio et al. (2007))2.\n\n1 This is a generally accepted consequence of very deep models that do not have enough training observations. However, recent research by Nakkiran et al. points to a completely different direction [return]. \n2 Bengio (2009, pg. 24) appropriately points out that Convolutional Neural Networks with up to seven layers did not suffer as other types of models. This may be due to what the author denominates as a Topographic Structure, suggesting that each neuron in a layer is associated with a small portion of the input. From this, two favorable aspects are hypothesized: the gradient is propagated less diffusely and this hierarchical local connectivity structure may be more suited for image recognition tasks, which is the most common use for such architecture [return]. References Heathfield, H. (1999). The rise and “fall” of expert systems in medicine. Expert Systems. 183–188. Wolpert, D. (1996), The Lack of A Priori Distinctions between Learning Algorithms. Neural Computation, 1341-1390. Bengio, Y., Courville, A. and Vincent, P. (2014). Representation Learning: A Review and New Perspectives Bengio, Y. and LeCun, Y. (2007). Scaling Learning Algorithms towards AI. 41. Bengio, Y. (2009). Learning Deep Architectures for AI. Foundations and Trends in Machine Learning. 1-127. Bengio, Y., Lamblin, P., Popovici, D., and Larochelle, H. (2007). Greedy Layer-Wise Training of Deep Networks. In Advances in Neural Information Processing Systems 19, 153-160. Bradley, D. M. (2010). Learning In Modular Systems. PhD Thesis, Carnegie Mellon University. Olah, C. (2014). Neural Networks, Manifolds, and Topology. Colah's blog Deng, L. and Yu, D. (2013). Deep Learning: Methods and Applications. Foundations and Trends in Signal Processing, 7-197. Krizhevsky, A., Sutskever, I., and Hinton, G. E. (2012). ImageNet Classification with Deep Convolutional Neural Networks. Advances in Neural Information Processing Systems 25, 1097-1105. He, K., Zhang, X., Ren, S., and Sun, J. (2015). Deep Residual Learning for Image Recognition. Proceedings of the IEEE conference on computer vision and pattern recognition. Goodfellow, I., Bengio, Y., and Courville, A. (2016). Deep Learning. MIT Press.   h1 { text-align: left; } body { text-align: justify; }  ","date":1557378000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557378000,"objectID":"bdb6372f1100db1f7b7f3d2a32cef414","permalink":"/post/deep_learning/","publishdate":"2019-05-09T00:00:00-05:00","relpermalink":"/post/deep_learning/","section":"post","summary":"In the last decade, there\u0026rsquo;s been a widespread interest in artificial intelligence, particularly machine learning, and even more specifically deep learning. Curiously, this topic has not only resurged within academia but rather it has also been commonly showcased on non-technical mediums.","tags":[""],"title":"Why deep learning became relevant","type":"post"},{"authors":[""],"categories":null,"content":" Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":-1,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-1,"objectID":"2fd7006329ece9a00a1dd4e6ff979ec3","permalink":"/projects/lth/","publishdate":"1969-12-31T20:59:59-03:00","relpermalink":"/projects/lth/","section":"projects","summary":"Series of experiments replicating Frankle and Carbin's Lottery Ticket Hypothesis","tags":["Source Themes"],"title":"Testing the Lottery Ticket Hypothesis","type":"projects"},{"authors":[""],"categories":null,"content":" Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":-2,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-2,"objectID":"fed0c9c05988ef91ee2d9b27ae4915d3","permalink":"/projects/landscapeviz/","publishdate":"1969-12-31T20:59:58-03:00","relpermalink":"/projects/landscapeviz/","section":"projects","summary":"Visualizing the loss landscape of fully connected neural networks using tensorflow","tags":["Source Themes"],"title":"Landscapeviz","type":"projects"},{"authors":[""],"categories":null,"content":" Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":-3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-3,"objectID":"9ffae0391f44d5100ca140a95a65f56a","permalink":"/projects/psopt/","publishdate":"1969-12-31T20:59:57-03:00","relpermalink":"/projects/psopt/","section":"projects","summary":"A python package for discrete optimization using particle swarms","tags":["Source Themes"],"title":"PSOpt - Particle swarm optimizer","type":"projects"},{"authors":[""],"categories":null,"content":" Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":-4,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-4,"objectID":"f65c54d0ffe35d14f6cd84554a50c809","permalink":"/projects/sonify/","publishdate":"1969-12-31T20:59:56-03:00","relpermalink":"/projects/sonify/","section":"projects","summary":"Live detection and sonification of EEG artifacts","tags":["Source Themes"],"title":"EEG Sonify","type":"projects"},{"authors":[""],"categories":null,"content":" Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":-5,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-5,"objectID":"83b5f64f1df254e6591371fc835e7b23","permalink":"/projects/sac/","publishdate":"1969-12-31T20:59:55-03:00","relpermalink":"/projects/sac/","section":"projects","summary":"Implementation and evaluation of the Soft-Actor critic algorithm using MXNet and OpenAI Gym","tags":["Source Themes"],"title":"Soft Actor-Critic in MXNet","type":"projects"},{"authors":[""],"categories":null,"content":" Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":-6,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-6,"objectID":"89c2175564a409937e342f1eddc587c7","permalink":"/projects/snowman_slide/","publishdate":"1969-12-31T20:59:54-03:00","relpermalink":"/projects/snowman_slide/","section":"projects","summary":"A game of the endless running genre made in ThreeJS","tags":["Source Themes"],"title":"Snowman slide","type":"projects"}]